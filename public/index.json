[{"categories":null,"contents":" Hello World A sample go program is show here.\npackage main import \u0026#34;fmt\u0026#34; func main() { message := greetMe(\u0026#34;world\u0026#34;) fmt.Println(message) } func greetMe(name string) string { return \u0026#34;Hello, \u0026#34; + name + \u0026#34;!\u0026#34; } Run the program as below:\n$ go run hello.go Variables Normal Declaration:\nvar msg string msg = \u0026#34;Hello\u0026#34; Shortcut:\nmsg := \u0026#34;Hello\u0026#34; Constants const Phi = 1.618 ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/notes/go/basic/introduction/","summary":"\u003c!-- A Sample Program --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eHello World\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cp\u003eA sample go program is show here.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003epackage\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emain\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;fmt\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efunc\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emain\u003c/span\u003e() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003emessage\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egreetMe\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;world\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003efmt\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003ePrintln\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003emessage\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efunc\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egreetMe\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ename\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e) \u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Hello, \u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ename\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;!\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRun the program as below:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ go run hello.go\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!-- Declaring Variables --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eVariables\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cp\u003e\u003cstrong\u003eNormal Declaration:\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003evar\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emsg\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003emsg\u003c/span\u003e = \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Hello\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cp\u003e\u003cstrong\u003eShortcut:\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003emsg\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Hello\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!-- Declaring Constants --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eConstants\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003econst\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ePhi\u003c/span\u003e = \u003cspan style=\"color:#ae81ff\"\u003e1.618\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e","tags":null,"title":"Introduction"},{"categories":null,"contents":" Strings str := \u0026#34;Hello\u0026#34; Multiline string\nstr := `Multiline string` Numbers Typical types\nnum := 3 // int num := 3. // float64 num := 3 + 4i // complex128 num := byte(\u0026#39;a\u0026#39;) // byte (alias for uint8) Other Types\nvar u uint = 7 // uint (unsigned) var p float32 = 22.7 // 32-bit float Arrays // var numbers [5]int numbers := [...]int{0, 0, 0, 0, 0} Pointers func main () { b := *getPointer() fmt.Println(\u0026#34;Value is\u0026#34;, b) func getPointer () (myPointer *int) { a := 234 return \u0026amp;a a := new(int) *a = 234 Pointers point to a memory location of a variable. Go is fully garbage-collected.\nType Conversion i := 2 f := float64(i) u := uint(i) Slice slice := []int{2, 3, 4} slice := []byte(\u0026#34;Hello\u0026#34;) ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/notes/go/basic/types/","summary":"\u003c!-- String Type --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eStrings\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003estr\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Hello\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eMultiline string\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003estr\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e`Multiline\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003estring`\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!-- Number Types --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eNumbers\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cp\u003eTypical types\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003enum\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e          \u003cspan style=\"color:#75715e\"\u003e// int\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003enum\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3.\u003c/span\u003e         \u003cspan style=\"color:#75715e\"\u003e// float64\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003enum\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e4i\u003c/span\u003e     \u003cspan style=\"color:#75715e\"\u003e// complex128\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003enum\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e byte(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;a\u0026#39;\u003c/span\u003e)  \u003cspan style=\"color:#75715e\"\u003e// byte (alias for uint8)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eOther Types\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003evar\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eu\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003euint\u003c/span\u003e = \u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e        \u003cspan style=\"color:#75715e\"\u003e// uint (unsigned)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003evar\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ep\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efloat32\u003c/span\u003e = \u003cspan style=\"color:#ae81ff\"\u003e22.7\u003c/span\u003e  \u003cspan style=\"color:#75715e\"\u003e// 32-bit float\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!----------- Arrays  ------\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eArrays\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// var numbers [5]int\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003enumbers\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e [\u003cspan style=\"color:#f92672\"\u003e...\u003c/span\u003e]\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e{\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!-- Pointers --\u003e\n\u003cdiv class=\"note-card medium-note\"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003ePointers\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efunc\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emain\u003c/span\u003e () {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003eb\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003egetPointer\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003efmt\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003ePrintln\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Value is\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eb\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efunc\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egetPointer\u003c/span\u003e () (\u003cspan style=\"color:#a6e22e\"\u003emyPointer\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003ea\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e234\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003ea\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ea\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e new(\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003ea\u003c/span\u003e = \u003cspan style=\"color:#ae81ff\"\u003e234\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePointers point to a memory location of a variable. Go is fully garbage-collected.\u003c/p\u003e","tags":null,"title":"Basic Types"},{"categories":null,"contents":" Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) Switch switch day { case \u0026#34;sunday\u0026#34;: // cases don\u0026#39;t \u0026#34;fall through\u0026#34; by default! fallthrough case \u0026#34;saturday\u0026#34;: rest() default: work() } Loop for count := 0; count \u0026lt;= 10; count++ { fmt.Println(\u0026#34;My counter is at\u0026#34;, count) } entry := []string{\u0026#34;Jack\u0026#34;,\u0026#34;John\u0026#34;,\u0026#34;Jones\u0026#34;} for i, val := range entry { fmt.Printf(\u0026#34;At position %d, the character %s is present\\n\u0026#34;, i, val) n := 0 x := 42 for n != x { n := guess() } ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/notes/go/basic/flow-control/","summary":"\u003c!-- Condition --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eCondition\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eday\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;sunday\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e||\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eday\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;saturday\u0026#34;\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003erest\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e} \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eday\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;monday\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eisTired\u003c/span\u003e() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003egroan\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e} \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003ework\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e_\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eerr\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edoThing\u003c/span\u003e(); \u003cspan style=\"color:#a6e22e\"\u003eerr\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e!=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enil\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003efmt\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003ePrintln\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Uh oh\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!-- Switch --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eSwitch\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eswitch\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eday\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003ecase\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;sunday\u0026#34;\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e// cases don\u0026#39;t \u0026#34;fall through\u0026#34; by default!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efallthrough\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003ecase\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;saturday\u0026#34;\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003erest\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003edefault\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003ework\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!-- Loop --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eLoop\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ecount\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e; \u003cspan style=\"color:#a6e22e\"\u003ecount\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026lt;=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e; \u003cspan style=\"color:#a6e22e\"\u003ecount\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e++\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003efmt\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003ePrintln\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;My counter is at\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003ecount\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003eentry\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e []\u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e{\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Jack\u0026#34;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;John\u0026#34;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Jones\u0026#34;\u003c/span\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ei\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eval\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003erange\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eentry\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003efmt\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003ePrintf\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;At position %d, the character %s is present\\n\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003ei\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eval\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003en\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ex\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e42\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003en\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e!=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ex\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003en\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eguess\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e","tags":null,"title":"Flow Control"},{"categories":null,"contents":" Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/notes/go/advanced/files/","summary":"\u003c!-- Condition --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eCondition\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eday\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;sunday\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e||\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eday\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;saturday\u0026#34;\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003erest\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e} \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eday\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;monday\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eisTired\u003c/span\u003e() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003egroan\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e} \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003ework\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e_\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eerr\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edoThing\u003c/span\u003e(); \u003cspan style=\"color:#a6e22e\"\u003eerr\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e!=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enil\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003efmt\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003ePrintln\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Uh oh\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e","tags":null,"title":"File Manipulation"},{"categories":null,"contents":" Variable NAME=\u0026#34;John\u0026#34; echo $NAME echo \u0026#34;$NAME\u0026#34; echo \u0026#34;${NAME} Condition if [[ -z \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is empty\u0026#34; elif [[ -n \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is not empty\u0026#34; fi ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/notes/bash/basic/","summary":"\u003c!-- Variable --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eVariable\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eNAME\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;John\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eecho $NAME\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eecho \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e$NAME\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eecho \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e${\u003c/span\u003eNAME\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!-- Condition --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eCondition\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e[[\u003c/span\u003e -z \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e$string\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e]]\u003c/span\u003e; \u003cspan style=\"color:#66d9ef\"\u003ethen\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  echo \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;String is empty\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eelif\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e[[\u003c/span\u003e -n \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e$string\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e]]\u003c/span\u003e; \u003cspan style=\"color:#66d9ef\"\u003ethen\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  echo \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;String is not empty\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efi\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e","tags":null,"title":"Bash Variables"},{"categories":["vr","metaquest","game","unity","escape"],"contents":"Discover our virtual reality (VR) escape game, crafted by Mélanie Brégou and Pauline Spinga. Inspired by fantastic universes and Unity Store\u0026rsquo;s Escape Room assets, we have developed original enigmas: from recreating melodies to assembling puzzles, all to unravel clues for escaping our magical-themed house.\nSet Up To begin our VR game development journey, we meticulously set up our project environment in Unity. Here\u0026rsquo;s a step-by-step guide on how we initiated the project:\nCreating the Project: We started by creating a new 3D project in Unity using the following specifications:\nUnity version: 2022.3.2f1 Template: 3D Configuring XR Plugin Management:\nNavigate to Edit \u0026gt; Project Settings \u0026gt; XR Plugin Management. Install XR Plugin Management if not already installed. Open XR Plugin Management and confirm to restart Unity. Upon restart, address any issues by: Editing the Interaction Profiles for the Oculus Touch Controller Profile. Clicking on \u0026ldquo;Fix\u0026rdquo; to resolve any Input System issues. Installing XR Interaction Toolkit:\nOpen the Window menu and select Package Manager. Choose the Unity Registry and locate XR Interaction Toolkit. Install XR Interaction Toolkit. Import the \u0026ldquo;Starter Assets\u0026rdquo; from the samples provided. Setting Up XR Origin:\nAdd the XR Origin asset to the project\u0026rsquo;s assets. Removing Default Camera:\nDelete the default Main Camera as it will be replaced with VR camera setups. Integrating Escape Room Assets:\nWe incorporated the \u0026ldquo;Escape Room\u0026rdquo; free asset from the Unity Asset Store. Customized the environment by removing unnecessary elements and puzzles to craft our unique gaming experience. Enigmas Piano Players start in a room with a wall. They must reproduce a specific melody of the radio on the piano to make the wall disappear. Puzzle Upon solving the piano enigma, players gain access to another room where puzzle pieces are hidden. These pieces, designed using Blender, provide a hint for the next action. Assembling the puzzle also reveals a key.\nIncendio The obtained key opens a drawer containing a wand. Utilizing a script from the escape room asset, players can interact with the drawer. With the wand and an image of the puzzle, players can deduce its fire-making ability. By approaching the fireplace with the wand, players trigger an animation of a parchment with the \u0026ldquo;Alohomora\u0026rdquo; inscription.\nAlohomora Referencing the Harry Potter universe, \u0026ldquo;Alohomora\u0026rdquo; is a spell that opens doors. Similar to \u0026ldquo;Incendio\u0026rdquo; players must approach the door with the wand to unlock it.\nStart and End Menu We implemented a simple menu to launch the game and included an end/restart button upon completing the game.\nPlayer Movements In our VR game developed in Unity, player movements are facilitated through a combination of continuous locomotion and teleportation mechanics.\nContinuous Movements Continuous movements allow players to navigate the virtual environment seamlessly. To implement this, we utilized the XR Interaction Toolkit along with the Starter Assets. Here\u0026rsquo;s a guide:\nXR Interaction Toolkit Setup: Navigate to Samples \u0026gt; XR Interaction Toolkit \u0026gt; 2.3.2 \u0026gt; Starter Assets. Add the XRI Default Left Controller component to the ActionBasedController object. Repeat the same process for the right hand controller. In our game we chose to have \u0026lsquo;snap turns\u0026rsquo; on the left joystick and continuous movement on the right joystick.\nXR Origin Configuration: In the Project Settings, access the Preset Manager. In the Find section, input \u0026ldquo;Right\u0026rdquo; and \u0026ldquo;Left\u0026rdquo;, then remove and re-add the XR Origin. Ensure all fields are properly filled.\nAnimated Hands: We enhanced the immersive experience by integrating animated hands. We dowloaded them via our Unity teacher.\nTeleportation Mechanism In addition to continuous movements, our game features teleportation for quick navigation and strategic positioning.\nTeleportation Areas: We designated specific areas within the virtual environment where players can teleport. Optimization Our primary goal was to maintain a performance of above 70FPS to provide a smooth experience and mitigate motion sickness. We achieved this by removing unnecessary decoration assets, baking lighting, and implementing occlusion culling.\nConclusion and Further Improvements We conducted several iterations to refine our VR escape game on MetaQuest, ensuring an immersive and enjoyable experience for players. Moving forward, we plan to continue improving the game by refining existing enigmas and adding speech recognition for spells.\n","date":"January 30, 2024","hero":"/posts/ar-vr/escape-vr/front.png","permalink":"http://localhost:1313/posts/ar-vr/escape-vr/","summary":"\u003cp\u003eDiscover our virtual reality (VR) escape game, crafted by Mélanie Brégou and Pauline Spinga.\nInspired by fantastic universes and Unity Store\u0026rsquo;s Escape Room assets, we have developed original enigmas: from recreating melodies to assembling puzzles, all to unravel clues for escaping our magical-themed house.\u003c/p\u003e\n\u003cdiv style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"\u003e\n      \u003ciframe allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"allowfullscreen\" loading=\"eager\" referrerpolicy=\"strict-origin-when-cross-origin\" src=\"https://www.youtube.com/embed/Up3RjfBAd6I?autoplay=0\u0026amp;controls=1\u0026amp;end=0\u0026amp;loop=0\u0026amp;mute=0\u0026amp;start=0\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" title=\"YouTube video\"\u003e\u003c/iframe\u003e\n    \u003c/div\u003e\n\n\u003ch2 id=\"set-up\"\u003eSet Up\u003c/h2\u003e\n\u003cp\u003eTo begin our VR game development journey, we meticulously set up our project environment in Unity. Here\u0026rsquo;s a step-by-step guide on how we initiated the project:\u003c/p\u003e","tags":null,"title":"Escape Game in VR"},{"categories":["ar","hololens","game","unity","MRTK"],"contents":"Introducing our AR game circuit developed using Unity3D and MRTK3 for HoloLens 2. This project was brought to life through a collaborative effort between myself and Pauline Spinga.\nHere are the key steps of the project:\nDesign In designing the circuit within a ski station environment, we sourced various models from free3D, turbosquid, and sketchfab websites. Additionally, we crafted a custom panel featuring our school logo. All elements were assembled in Blender, with additional snow textures for ambiance.\nHowever, during the application build for the HoloLens, we encountered performance issues due to the weight of the model. Consequently, we had to simplify the design elements.\nGame positionning To optimize user experience, we implemented a feature allowing players to adjust the position and scale of the circuit before gameplay. This adjustment can be made using hand gestures, facilitated by clicking on the \u0026ldquo;Game\u0026rdquo; button. Once satisfied with the positioning and scale, players can confirm their selection by pressing the \u0026ldquo;Done\u0026rdquo; button.\nGame Control Initially, we implemented a joystick Canva element for sleigh control. However, it was discovered that Canva elements are not compatible with the HoloLens headset. As a workaround, we replaced the joystick with four directional buttons (forward, backward, left, and right).\nLevels and Gameplay We created two levels, both measuring player performance by the circuit\u0026rsquo;s chrono time and the collection of flakes along the track. At the end of each race, the score and time are displayed. Upon completing the first level, players have the option to replay it or progress to the second level.\nIn the second level, snow piles are introduced. Colliding with one of these snow piles results in the loss of one collected flake.\nAt each level, players can choose between two different sleigh designs.\nHoloLens Deployment To deploy our application on the HoloLens headset we had the following build settings. In OpenXR Feature Groups we selected Microsoft HoloLens with Hand Tracking and Mixed Reality Features ticked. There were the following interaction profiles:\nMicrosoft Hand Interaction Profile Microsoft Motion Controller Profile Eye Gaze Interaction Profile Once we built in Active(ARM64) platform with Active(Release) configuration, we connect the headset to the computer with a wire. We click on .snl file and open it in visual studio. Then in Project \u0026gt; properties \u0026gt; configuration properties \u0026gt; Debugging change the Machine Name. The machine name is in update and safety \u0026gt; developer mode \u0026gt; activate all togles to make ethernet work. Then enter the IP adress in machine name.\nHoloLens Deployment To deploy our application on the HoloLens headset, we configured the following build settings. In the OpenXR Feature Groups, we selected Microsoft HoloLens with Hand Tracking and Mixed Reality Features enabled. The interaction profiles included:\nMicrosoft Hand Interaction Profile Microsoft Motion Controller Profile Eye Gaze Interaction Profile After building in the Active(ARM64) platform with the Active(Release) configuration, we connected the headset to the computer using a wire. Then, we opened the .snl file in Visual Studio. To ensure ethernet functionality, we activated all toggles in Update and Safety \u0026gt; Developer Mode (hololens). Next, we entered the IP address as the machine name in Project \u0026gt; Properties \u0026gt; Configuration Properties \u0026gt; Debugging.\nConclusion and Further Improvements Creating this game was not only enjoyable but also provided us with a great AR experience. However, there are several areas where we can further enhance the game:\nAutomatic Scene Positioning: Implementing an automatic scene positioning feature would simplify the user experience, eliminating the need for manual adjustments.\nImproved UI: Enhancing the user interface with better button placement and overall design aesthetics would contribute to a more intuitive experience.\nEnhanced Speed and Trajectory Control: Improving speed control and trajectory adjustments would allow for smoother and more responsive gameplay, enhancing immersion and enjoyment for players.\nHere is a short demo :\n","date":"January 30, 2024","hero":"/posts/ar-vr/sleigh-ar/front.png","permalink":"http://localhost:1313/posts/ar-vr/sleigh-ar/","summary":"\u003cp\u003eIntroducing our AR game circuit developed using Unity3D and MRTK3 for HoloLens 2. This project was brought to life through a collaborative effort between myself and Pauline Spinga.\u003c/p\u003e\n\u003cp\u003eHere are the key steps of the project:\u003c/p\u003e\n\u003ch3 id=\"design\"\u003eDesign\u003c/h3\u003e\n\u003cp\u003eIn designing the circuit within a ski station environment, we sourced various models from free3D, turbosquid, and sketchfab websites. Additionally, we crafted a custom panel featuring our school logo. All elements were assembled in Blender, with additional snow textures for ambiance.\u003c/p\u003e","tags":null,"title":"Snow sleigh game on Hololens2"},{"categories":["content","protection","blockchain"],"contents":"In collaboration with Kanita Loisy and Pauline Spinga, I delved into a project focusing on the application of blockchain technology in royalty management for the course \u0026ldquo;Digital Content Protection\u0026rdquo; . As the digitization of resources continues to proliferate, the need for robust protection mechanisms has become increasingly apparent. Our project aimed to explore how blockchain could revolutionize the management of intellectual property rights, particularly in the realm of visual content.\nUnderstanding Blockchain and Its Application in Royalty Management Definition and Objectives: Blockchain: A decentralized and secure network of nodes (computers) that generates cryptographically linked immutable data blocks, serving as a digital ledger. Smart Contracts: Pieces of computer code, automated and immutable, acting as engines of decentralized applications offering various services, such as decentralized finance and marketplaces. Tokens: Represent value and digital assets on the blockchain, including transactional tokens (e.g., Gas on the Ethereum network) and user-created tokens (e.g., Ethereum, Bitcoin). Solution Offered by Blockchains: Facilitates decentralized ownership and licensing models through tokenization, providing a secure and transparent way to transfer and manage content rights. Implementation Sharing Rights on an Artwork: Overview: Consideration of an asset (painting, picture) that can be created by one or multiple artists. Each artist owns cuts, and tokens are associated to represent the right of access to the asset.\nImplementation Details:\nUtilization of Ethereum blockchain, with NFTs used to represent non-divisible assets like pictures. Deployment of smart contracts in Solidity language, enabling the creation, transfer, and ownership of tokens representing rights to artistic assets. Tools such as Ganache, Truffle, and Web3.js utilized for development and interaction with the smart contract. Ganache: Ethereum blockchain simulator\nSolidity: Programming language for smart contracts\nTruffle: Ethereum development framework\nWeb3.js: Library for interacting with Ethereum node\nIllustration: Implementation Results Our project implementation resulted in significant insights into how blockchain can streamline royalty management. Here\u0026rsquo;s a breakdown of the process illustrated with examples:\nInitial Accounts and Token Distribution The initial setup depicts fictive accounts on Ganache for Kanita and Pauline (artists) as well as Bob and Mélanie (not artists). Two tokens are created and distributed (id : 123 and 456) for Kanita\u0026rsquo;s and Pauline\u0026rsquo;s work of art, with a 50/50 share between two artists.\nMélanie Buying Token 123 Mélanie purchases Token 123, resulting in changes in account balances. The artists receive 5 ETH each, while Mélanie\u0026rsquo;s account decreases by 10 ETH.\nMélanie Selling Token to Bob Mélanie sells Token 123 to Bob, resulting in royalties for the artists. The transaction details indicate amounts exchanged between parties.\nConclusion In conclusion, protecting visual content and managing royalties can be challenging. However, the integration of blockchain technology presents a promising solution to overcome these challenges. Blockchain offers transparent, tamper-proof, and traceable transactions. The introduction of non-fungible tokens (NFTs) enables the representation of unique assets and the management of intellectual property rights. Through our implementation on the Ethereum blockchain, we have explored the creation, transfer, and ownership of tokens representing rights to artistic assets. Additionally, we have addressed the sharing of rights on artwork and the distribution of royalties.\n","date":"November 10, 2023","hero":"/posts/dcp/blockchain-project/front.png","permalink":"http://localhost:1313/posts/dcp/blockchain-project/","summary":"\u003cp\u003eIn collaboration with Kanita Loisy and Pauline Spinga, I delved into a project focusing on the application of blockchain technology in royalty management for the course \u0026ldquo;Digital Content Protection\u0026rdquo; . As the digitization of resources continues to proliferate, the need for robust protection mechanisms has become increasingly apparent. Our project aimed to explore how blockchain could revolutionize the management of intellectual property rights, particularly in the realm of visual content.\u003c/p\u003e","tags":null,"title":"Blockchain Application in Royalty Management for Digital Content Protection"},{"categories":["ar","mobile","game","unity","ARFoundation"],"contents":"In the context of our AR course at Institut Polytechnique de Paris, we were asked to develop a mobile application with AR Foundation in Unity to place object on the ground by taping its desired place on the screen.\nYour browser does not support the video tag. Here are the different steps of the project :\nUnity settings In order to make the application work on my Iphone I tried several Unity versions but the one which worked was the 2022.3.10f1. Then create a new project with AR Mobile Core template\nPlace Item script Write and put the following script with a prefab in the XR origin.\nusing System.Collections.Generic; using UnityEngine; using UnityEngine.XR.ARFoundation; using UnityEngine.XR.ARSubsystems; using EnhancedTouch = UnityEngine.InputSystem.EnhancedTouch; [RequireComponent(typeof(ARRaycastManager), typeof(ARPlaneManager))] public class PlaceItem : MonoBehaviour { [SerializeField] private GameObject prefab; private ARRaycastManager arRaycastManager; private ARPlaneManager arPlaneManager; private List\u0026lt;ARRaycastHit\u0026gt; hits = new List\u0026lt;ARRaycastHit\u0026gt;(); private void Awake() { arRaycastManager = GetComponent\u0026lt;ARRaycastManager\u0026gt;(); arPlaneManager = GetComponent\u0026lt;ARPlaneManager\u0026gt;(); } private void OnEnable() { EnhancedTouch.TouchSimulation.Enable(); EnhancedTouch.EnhancedTouchSupport.Enable(); EnhancedTouch.Touch.onFingerDown += FingerDown; } private void OnDisable() { EnhancedTouch.TouchSimulation.Disable(); EnhancedTouch.EnhancedTouchSupport.Disable(); EnhancedTouch.Touch.onFingerDown -= FingerDown; } private void FingerDown(EnhancedTouch.Finger finger) { if (finger.index != 0) return; // Pas de multi-touch if (arRaycastManager.Raycast(finger.currentTouch.screenPosition, hits, TrackableType.PlaneWithinPolygon)) { foreach (ARRaycastHit hit in hits) { Pose pose = hit.pose; Instantiate(prefab, pose.position, pose.rotation); } } } } ","date":"November 10, 2023","hero":"/posts/ar-vr/tap-to-place/front.png","permalink":"http://localhost:1313/posts/ar-vr/tap-to-place/","summary":"\u003cp\u003eIn the context of our AR course at Institut Polytechnique de Paris, we were asked to develop a mobile application with AR Foundation in Unity to place object on the ground by taping its desired place on the screen.\u003c/p\u003e\n\u003cvideo width=\"640\" height=\"360\" controls\u003e\n  \u003csource src=\"tap-to-place.mov\" type=\"video/quicktime\"\u003e\n  Your browser does not support the video tag.\n\u003c/video\u003e\n\u003cp\u003eHere are the different steps of the project :\u003c/p\u003e\n\u003ch4 id=\"unity-settings\"\u003eUnity settings\u003c/h4\u003e\n\u003cp\u003eIn order to make the application work on my Iphone I tried several Unity versions but the one which worked was the 2022.3.10f1.\nThen create a new project with AR Mobile Core template\u003c/p\u003e","tags":null,"title":"Tap to Place App"},{"categories":null,"contents":"Introduction: In 1991, computer scientist Mark Weiser introduced the concept of \u0026ldquo;ubiquitous computing,\u0026rdquo; envisioning a world where technology seamlessly integrates into our daily lives. Today, we\u0026rsquo;ll explore whether we are already living in a world of ubiquitous computing, highlighting both what has been achieved and what remains on the horizon.\nWhat is \u0026ldquo;Ubiquitous Computing\u0026rdquo; ? Ubiquitous computing, often referred to as \u0026ldquo;pervasive computing,\u0026rdquo; is a paradigm where computing technology is seamlessly embedded in the environment, making it an invisible and integral part of our daily lives. It envisions a world where computers are so interwoven into our surroundings that they operate unnoticed but provide significant benefits.\nAspects proving ubiquitous computing is already here: Smartphones and Wearables: We carry powerful computers in our pockets and wear them on our wrists. Smartphones and wearables have become extensions of ourselves, offering instant access to information, communication, and a wide range of applications. Internet of Things (IoT): Our homes are filled with interconnected devices that communicate with each other, from thermostats that adjust temperatures based on our preferences to smart speakers that respond to voice commands. Voice Assistants: Virtual assistants like Siri, Alexa, and Google Assistant have made it easy to interact with technology using natural language, becoming an integral part of our daily routines. Aspects showing the path forward: Interoperability: For a more seamless experience, devices need to work together effortlessly. We need standardized communication protocols to ensure that different gadgets can collaborate effectively. Privacy and Security: Protecting our data and privacy in an interconnected world is an ongoing concern. Strengthening security measures and promoting user education is crucial. Energy Efficiency: Ubiquitous computing devices should be energy-efficient and rely on sustainable power sources to reduce their environmental impact. What about ubiquitous Mixed Reality Head-Mounted Displays (MR HMDs) In a future where Mixed Reality Head-Mounted Displays (MR HMDs) are as ubiquitous as smartphones, the applications available on the MR app store will significantly impact how we live, work, and interact with the digital and physical worlds.\nHere are some of the most downloaded applications that I envision and the reasons behind their popularity:\nHealth and Wellness Assistants: These apps will provide personalized health and wellness guidance, helping users monitor their physical activity, nutrition, and mental health. Virtual trainers and health advisors will guide users through workouts, track their progress, and offer immediate feedback for a healthier lifestyle.\nAugmented Learning and Education Apps: With MR HMDs, learning can become an immersive experience. These apps will offer interactive and educational content, taking students on virtual field trips, dissecting 3D models, and providing real-time language translation for global learning.\nMixed Reality Social Networking: Why: MR HMDs will bring people together in virtual environments, allowing them to socialize and collaborate in new and exciting ways. Users can meet in shared spaces, play games, attend virtual events, and work together, regardless of physical distances, making social networking more immersive and personal.\n","date":"October 31, 2023","hero":"/posts/hci/ubiquitous/ubi.png","permalink":"http://localhost:1313/posts/hci/ubiquitous/","summary":"\u003ch4 id=\"introduction\"\u003eIntroduction:\u003c/h4\u003e\n\u003cp\u003eIn 1991, computer scientist Mark Weiser introduced the concept of \u0026ldquo;ubiquitous computing,\u0026rdquo; envisioning a world where technology seamlessly integrates into our daily lives. Today, we\u0026rsquo;ll explore whether we are already living in a world of ubiquitous computing, highlighting both what has been achieved and what remains on the horizon.\u003c/p\u003e\n\u003ch4 id=\"what-is-ubiquitous-computing-\"\u003eWhat is \u0026ldquo;Ubiquitous Computing\u0026rdquo; ?\u003c/h4\u003e\n\u003cp\u003eUbiquitous computing, often referred to as \u0026ldquo;pervasive computing,\u0026rdquo; is a paradigm where computing technology is seamlessly embedded in the environment, making it an invisible and integral part of our daily lives. It envisions a world where computers are so interwoven into our surroundings that they operate unnoticed but provide significant benefits.\u003c/p\u003e","tags":null,"title":"Ubiquitous Computing: A Vision and Reality"},{"categories":null,"contents":"Mobile phones have come a long way in terms of design over the past decade. In this blog post, we\u0026rsquo;ll take a closer look at the evolution of mobile phone design and shape, with a specific focus on weight, thickness, and display size.\nWeight Matters: Mobile phone manufacturers have been on a mission to make our devices smaller and lighter. While the average weight of mobile phones has remained relatively consistent, it\u0026rsquo;s worth noting that modern high-end smartphones now pack more features into smaller, lighter packages. However, the quest for feature-rich phones has driven the trend towards slightly heavier devices. The finish of a phone, such as metal or tempered glass, can add to its weight.\nThin is In: Slimness has become a defining factor in mobile phone design. The trend towards thinner phones gained momentum with iconic models like the Motorola RAZR V3. High-end phones have increasingly embraced slim profiles, with a notable gap emerging between high-end and average devices. Nevertheless, there\u0026rsquo;s a limit to how thin a phone can be while remaining durable. Extremely slim phones may not withstand regular use, and there\u0026rsquo;s a consensus that most users don\u0026rsquo;t need them to be any slimmer. So, high-end smartphone thickness may find its ideal balance in the near future.\nBigger Displays: When it comes to display size, the mantra seems to be \u0026ldquo;bigger is better.\u0026rdquo; Advances in screen resolution have made larger screens more appealing. While some argue that smaller, high-resolution displays offer a sharper image, most users prefer larger screens for their everyday tasks. Phone screens have been steadily growing in size, with some crossing the 5-inch threshold. However, it\u0026rsquo;s uncertain whether phones will continue to grow beyond this point, as ergonomic considerations may limit further size increases.\n","date":"October 30, 2023","hero":"/posts/hci/current/evolution.png","permalink":"http://localhost:1313/posts/hci/current/","summary":"\u003cp\u003eMobile phones have come a long way in terms of design over the past decade. In this blog post, we\u0026rsquo;ll take a closer look at the evolution of mobile phone design and shape, with a specific focus on weight, thickness, and display size.\u003c/p\u003e\n\u003ch4 id=\"weight-matters\"\u003eWeight Matters:\u003c/h4\u003e\n\u003cp\u003eMobile phone manufacturers have been on a mission to make our devices smaller and lighter. While the average weight of mobile phones has remained relatively consistent, it\u0026rsquo;s worth noting that modern high-end smartphones now pack more features into smaller, lighter packages. However, the quest for feature-rich phones has driven the trend towards slightly heavier devices. The finish of a phone, such as metal or tempered glass, can add to its weight.\u003c/p\u003e","tags":null,"title":"The shape of smartphones"},{"categories":["protection"],"contents":" Name Course Date Mélanie Brégou Digital content protection 27/10/2023 Objective : Implement a spread spectrum watermarking method, with a focus on utilizing off-the-shelf random number generators, understanding the principles of uniform and Gaussian distributed generators, exploring correlation functions, and applying these concepts towards CDMA-based watermarking.\nTable of Contents Off-the-Shelf Random Number Generators Principles Uniform Distributed Generator Gaussian Distributed Generator Correlation Functions Graphical Representation and Properties Temporal Autocorrelation Function Cross-Correlation Function Towards CDMA Study the robustness against noise addition CDMA-Based Watermarking Off-the-Shelf Random Number Generators Principles import matplotlib.pyplot as plt import random import numpy as np We define the LCG (linear congruential generator) function : xn = (a * xn-1 + b) mod c,\nwhere a, b, c are positive integers c can be a prime number, a \u0026lt; c, b \u0026lt; c def LCG(x0,a,b,c): list = [] x = x0 for n in range(0,2*c): list.append((a*x + b) % c) x= (a*x + b) % c return list def Xn(x0,a,b,c,T): x1 = (a*x0 + b) % c return Xn(x1,a,b,c,T-1) if T\u0026gt;0 else x1 Let\u0026rsquo;s generate a random sequence following the LCG rule, for the following parameters: a = 3, b = 5, c = 19 and x0 = 3\nplt.plot(LCG(x0=3,a=3,b=5,c=19)) plt.title(\u0026#34;x0=3,a=3,b=5,c=19\u0026#34;) plt.show() This parameter combination leads to repetitive sequences, resulting in patterns in the plot rather than true randomness.\nfig, axes = plt.subplots(1, 4, figsize=(10, 5)) axes[0].plot(LCG(x0=7,a=3,b=5,c=19)) axes[0].set_title(f\u0026#34;x0=7,a=3,b=5,c=19\u0026#34;) axes[1].plot(LCG(x0=3,a=3,b=7,c=19)) axes[1].set_title(f\u0026#34;x0=3,a=3,b=7,c=19\u0026#34;) axes[2].plot(LCG(x0=3,a=7,b=5,c=19)) axes[2].set_title(f\u0026#34;x0=3,a=7,b=5,c=19\u0026#34;) axes[3].plot(LCG(x0=3,a=3,b=5,c=15)) axes[3].set_title(f\u0026#34;x0=3,a=3,b=5,c=15\u0026#34;) Text(0.5, 1.0, 'x0=3,a=3,b=5,c=15') Repeating the example for:\nc = 19, a = 3, b = 5, and x0 = 7 c = 19, a = 3, b = 7, and x0 = 3 c = 19, a = 7, b = 5, and x0 = 3 c = 15, a = 3, b = 5, and x0 = 3 The generated plots are still repetitive sequences.\nNow let\u0026rsquo;s repeat the example for the MLCG (Multiple LCG), described by xn = (a * xn-1 + b * xn-2 + c) mod d, where a, b, c, d are positive integers and d can be a prime number, a \u0026lt; d, b \u0026lt; d, c \u0026lt; d\ndef MLCG(x0,x1, a,b,c,d): list = [] xn = x0 xn1 = x1 for n in range(0,2*d): list.append((a*xn1 + b*xn + c) % d) xn_ = xn xn = xn1 xn1= (a*xn1 + b*xn_ + c) % d return list fig, axes = plt.subplots(1, 3, figsize=(10, 5)) axes[0].plot(MLCG(x0=7,x1=0,a=3,b=5,c=6,d=31)) axes[0].set_title(f\u0026#34;x0=7,x1=0,a=3,b=5,c=6,d=31\u0026#34;) axes[1].plot(MLCG(x0=3,x1=2,a=3,b=7,c=6,d=31)) axes[1].set_title(f\u0026#34;x0=3,x1=2,a=3,b=7,c=6,d=31\u0026#34;) axes[2].plot(MLCG(x0=3,x1=4,a=3,b=7,c=6,d=31)) axes[2].set_title(f\u0026#34;x0=3,x1=4,a=3,b=7,c=6,d=31\u0026#34;) Text(0.5, 1.0, 'x0=3,x1=4,a=3,b=7,c=6,d=31') The generated sequences do not seem repetitive and appear to be random.\nUniform Distributed Generators def random_vector(T): return [random.uniform(0,1) for t in range(T)] T=100 x = random_vector(T) plt.plot(x) plt.title(f\u0026#34;[0, 1] uniform generator with {T} components\u0026#34;) plt.show() print(\u0026#34;mean :\u0026#34;,np.mean(x)) print(\u0026#34;variance :\u0026#34;,np.var(x)) plt.hist(x, 15, density=True) plt.title(f\u0026#34;[0, 1] uniform generator with {T} components\u0026#34;) plt.show() mean : 0.523604935218904 variance : 0.08380462274270617 The mean is approximately the theoretical mean of 0.5, which aligns with the uniform distribution in the range [0, 1]. The variance is also approximately as expected for a uniform distribution in [0, 1]. The more T is large the more uniform the sequence is.\nprint(\u0026#34;x:\u0026#34;, x) a= 1 b = 6 y1 = [a*xi + b for xi in x] print(\u0026#34;y1:\u0026#34;,y1) a= 6 b = 0 y2 = [a*xi + b for xi in x] print(\u0026#34;y2:\u0026#34;,y2) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].plot(y1) axes[0].set_title(f\u0026#34;y = x + 6\u0026#34;) axes[1].plot(y2) axes[1].set_title(f\u0026#34;y = 6x\u0026#34;) print(\u0026#34;\\nmean y = x + 6:\u0026#34;,np.mean(y1)) print(\u0026#34;variance y = x + 6:\u0026#34;,np.var(y1)) print(\u0026#34;\\nmean y = 6x :\u0026#34;,np.mean(y2)) print(\u0026#34;variance y = 6x:\u0026#34;,np.var(y2)) x: [0.25837191659828673, 0.4470563921290477, 0.21382730728002808, 0.7086462073203836, 0.3705133398244411, 0.776530587035182, 0.6245249968295411, 0.6055792797378172, 0.20047566143197193, 0.7476917028832212, 0.007605557962887088, 0.6600414938252693, 0.7897935065863732, 0.8451555781774278, 0.46916100754285683, 0.29834856967908496, 0.15299829505332485, 0.7990704904514152, 0.7052788414120111, 0.43455085071476685, 0.41080882660183615, 0.8469856705920843, 0.17925364856258053, 0.597235407898144, 0.8894834971435803, 0.17677942597730867, 0.9017874629765549, 0.1391642452258447, 0.16908696917443422, 0.9687377617065794, 0.8558605812694794, 0.9672336228942946, 0.025345710253115716, 0.9368928428842539, 0.2841382877833384, 0.2676295593554707, 0.4483339344855558, 0.3112961811035514, 0.9842870866904126, 0.2429504222912039, 0.9534669885193416, 0.856102428315788, 0.11246021882927382, 0.6255693356389754, 0.0092892352608408, 0.3484323300025869, 0.5204459560633431, 0.4491732384806463, 0.7939177716387159, 0.6778219507997898, 0.25035108261802685, 0.6757293094523129, 0.3729853645172859, 0.17497848177504938, 0.7141314038388153, 0.9839656409272567, 0.6531845148593777, 0.4053919177857205, 0.9676795868041943, 0.3719787397880119, 0.3610795330975649, 0.4260508767361372, 0.27951603091395316, 0.04360423771126598, 0.785288125768064, 0.5039429430086181, 0.1382793843880049, 0.8461760019908358, 0.9147695729971356, 0.21423640538998656, 0.10539646167702355, 0.7953651908588297, 0.3264988721469524, 0.31319950539263697, 0.1657945694437255, 0.210725128993006, 0.6459855824939482, 0.610809529651901, 0.34482684476226844, 0.003863402465228094, 0.7105935752171066, 0.9348469122045779, 0.3340640165132659, 0.19298940842885515, 0.7449744847248124, 0.7317522404803241, 0.8992896601967665, 0.5586670660533961, 0.9212492669920598, 0.5922609334191495, 0.6794701791984561, 0.8624487424953742, 0.7555926452368393, 0.37518956356210553, 0.35120430225899457, 0.7273158434032417, 0.8574333485301392, 0.2783898505862623, 0.24347399882053722, 0.9383110604207417] y1: [6.2583719165982865, 6.447056392129047, 6.213827307280028, 6.708646207320384, 6.370513339824441, 6.776530587035182, 6.624524996829541, 6.6055792797378174, 6.200475661431972, 6.7476917028832215, 6.007605557962887, 6.660041493825269, 6.789793506586373, 6.845155578177428, 6.469161007542857, 6.298348569679085, 6.152998295053325, 6.799070490451415, 6.705278841412011, 6.434550850714767, 6.410808826601836, 6.846985670592084, 6.179253648562581, 6.597235407898144, 6.88948349714358, 6.176779425977308, 6.9017874629765545, 6.139164245225845, 6.169086969174434, 6.9687377617065795, 6.8558605812694795, 6.967233622894295, 6.025345710253116, 6.936892842884254, 6.284138287783338, 6.267629559355471, 6.4483339344855555, 6.311296181103551, 6.984287086690412, 6.242950422291204, 6.953466988519342, 6.856102428315788, 6.112460218829273, 6.625569335638976, 6.009289235260841, 6.348432330002587, 6.520445956063343, 6.449173238480646, 6.793917771638716, 6.67782195079979, 6.250351082618026, 6.675729309452313, 6.372985364517286, 6.1749784817750495, 6.714131403838815, 6.983965640927257, 6.653184514859378, 6.40539191778572, 6.967679586804194, 6.371978739788012, 6.3610795330975645, 6.426050876736137, 6.279516030913953, 6.043604237711266, 6.785288125768064, 6.503942943008618, 6.138279384388005, 6.846176001990836, 6.914769572997136, 6.214236405389986, 6.1053964616770235, 6.7953651908588295, 6.326498872146953, 6.313199505392637, 6.165794569443726, 6.210725128993006, 6.645985582493948, 6.610809529651901, 6.344826844762268, 6.003863402465228, 6.710593575217107, 6.934846912204578, 6.334064016513266, 6.192989408428855, 6.744974484724812, 6.731752240480324, 6.899289660196766, 6.558667066053396, 6.92124926699206, 6.5922609334191495, 6.679470179198456, 6.862448742495374, 6.755592645236839, 6.375189563562105, 6.351204302258995, 6.727315843403241, 6.857433348530139, 6.2783898505862625, 6.243473998820537, 6.938311060420742] y2: [1.5502314995897204, 2.682338352774286, 1.2829638436801685, 4.251877243922301, 2.2230800389466467, 4.659183522211092, 3.7471499809772464, 3.6334756784269033, 1.2028539685918316, 4.486150217299327, 0.04563334777732253, 3.960248962951616, 4.738761039518239, 5.070933469064567, 2.8149660452571412, 1.7900914180745098, 0.9179897703199491, 4.794422942708492, 4.231673048472067, 2.607305104288601, 2.464852959611017, 5.081914023552506, 1.0755218913754832, 3.5834124473888638, 5.336900982861482, 1.060676555863852, 5.410724777859329, 0.8349854713550682, 1.0145218150466053, 5.812426570239476, 5.135163487616876, 5.803401737365768, 0.1520742615186943, 5.621357057305524, 1.7048297267000303, 1.6057773561328244, 2.6900036069133346, 1.8677770866213084, 5.905722520142476, 1.4577025337472234, 5.72080193111605, 5.136614569894728, 0.6747613129756429, 3.753416013833853, 0.0557354115650448, 2.0905939800155213, 3.1226757363800584, 2.6950394308838774, 4.763506629832295, 4.066931704798739, 1.5021064957081611, 4.054375856713877, 2.2379121871037153, 1.0498708906502963, 4.284788423032892, 5.90379384556354, 3.919107089156266, 2.4323515067143227, 5.806077520825166, 2.2318724387280713, 2.1664771985853895, 2.5563052604168233, 1.677096185483719, 0.2616254262675959, 4.711728754608384, 3.0236576580517083, 0.8296763063280295, 5.0770560119450145, 5.488617437982813, 1.2854184323399194, 0.6323787700621413, 4.772191145152979, 1.9589932328817143, 1.8791970323558218, 0.994767416662353, 1.264350773958036, 3.8759134949636893, 3.664857177911406, 2.0689610685736106, 0.023180414791368564, 4.26356145130264, 5.609081473227468, 2.004384099079595, 1.1579364505731309, 4.469846908348875, 4.390513442881945, 5.395737961180599, 3.3520023963203767, 5.527495601952358, 3.5535656005148972, 4.076821075190736, 5.174692454972245, 4.533555871421036, 2.251137381372633, 2.1072258135539674, 4.36389506041945, 5.144600091180835, 1.6703391035175739, 1.4608439929232233, 5.62986636252445] mean y = x + 6: 6.523604935218902 variance y = x + 6: 0.08380462274270613 mean y = 6x : 3.141629611313425 variance y = 6x: 3.016966418737421 On the left :\nWe apply a linear transformation with \u0026lsquo;a = 1\u0026rsquo; and \u0026lsquo;b = 6\u0026rsquo; to the vector x. This transformation shifts all values in x by adding 6.\nThe plot shows that each value in x has been shifted.\nMean is the expected mean of x + 6. Variance remains approximately the same as the variance of x. On the right :\nWe set \u0026lsquo;a = 6\u0026rsquo; and \u0026lsquo;b = 0\u0026rsquo;, which effectively scale all values in x by a factor of 6.\nThe plot demonstrates that all values in x have been multiplied by 6, causing a uniform stretch of the distribution.\nMean is approximately 6 times the mean of x. Variance is 6^2 times the variance of x. Gaussian Distributed Generators T= 100 gaussian = np.random.normal(0,0.1,T) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].plot(gaussian) axes[0].set_title(\u0026#34;Gaussian Distribution Plot\u0026#34;) axes[1].hist(gaussian, 10, density=True) axes[1].set_title(\u0026#34;Histogram of Gaussian Distribution\u0026#34;) print(\u0026#34;mean :\u0026#34;,np.mean(gaussian)) print(\u0026#34;variance :\u0026#34;,np.var(gaussian)) mean : 0.010568025096744142 variance : 0.012131041665992786 In this Gaussian distribution, the mean is close to zero, and the variance reflects the spread of the data around this central point both indicative of the characteristics of a typical Gaussian distribution. With a larger T, the mean would likely converge even closer to zero.\nCorrelation Functions In the domain of watermarking, autocorrelation functions are essential tools for quantifying the inherent randomness within a watermark and its correlation with the host signal. These functions are crucial for the security and resilience of embedded watermarks.\nGraphical Representation and Properties Temporal Autocorrelation Function correlation = np.correlate(x,x,mode=\u0026#39;full\u0026#39;) plt.plot(correlation) plt.title(\u0026#34;Correlation\u0026#34;) plt.show() The central spike corresponds to the peak autocorrelation represents the self-similarity or self-correlation of the signal with itself when no time lag is introduced. This is a fundamental characteristic of any signal\u0026rsquo;s autocorrelation function.\nx_1000 = random_vector(1000) x_10000 = random_vector(10000) correlation_1000 = np.correlate(x_1000,x_1000,mode=\u0026#39;full\u0026#39;) correlation_10000 = np.correlate(x_10000,x_10000,mode=\u0026#39;full\u0026#39;) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].plot(correlation_1000) axes[0].set_title(\u0026#34;Normal autocorrelation (1000)\u0026#34;) axes[1].plot(correlation_10000) axes[1].set_title(\u0026#34;Normal autocorrelation (10000)\u0026#34;) Text(0.5, 1.0, 'Normal autocorrelation (10000)') As the sample size T increase, the central spike becomes narrower and taller, and the random fluctuations become less pronounced. This is because with a larger sample size, the estimate of the autocorrelation becomes more precise and approaches the theoretical expectations.\nThe random fluctuations away from the central spike suggest that the Gaussian signal is essentially uncorrelated with itself at different time lags.\nCross Correlation Function gaussian_1000 = np.random.normal(0,0.1,1000) a= 6 b = 16 z = [a*gauss+ b for gauss in gaussian_1000] t = [a*gauss**2+ b for gauss in gaussian_1000] correlation_y1 = np.correlate(z,z,mode=\u0026#39;full\u0026#39;) correlation_y2 = np.correlate(t,t,mode=\u0026#39;full\u0026#39;) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].plot(correlation_y1) axes[0].set_title(\u0026#34;Autocorrelation z = 6x + 16 \u0026#34;) axes[1].plot(correlation_y2) axes[1].set_title(\u0026#34;Autocorrelation t = 6x**2 + 16 \u0026#34;) Text(0.5, 1.0, 'Autocorrelation t = 6x**2 + 16 ') correlation_gauss_z = np.correlate(gaussian_1000,z,mode=\u0026#39;full\u0026#39;) correlation_gauss_t = np.correlate(gaussian_1000,t,mode=\u0026#39;full\u0026#39;) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].plot(correlation_gauss_z) axes[0].set_title(\u0026#34;Correlation gaussian and z\u0026#34;) axes[1].plot(correlation_gauss_t) axes[1].set_title(\u0026#34;Correlation gaussian and t\u0026#34;) Text(0.5, 1.0, 'Correlation gaussian and t') Towards CDMA Study the robustness against noise addition In this section, we explore the robustness of correlation functions in the context of Code Division Multiple Access (CDMA) as we examine their behavior in the presence of noise.\nT = 1000 alpha = 1 x_gauss_1 = np.random.normal(0,0.1,T) x_gauss_2 = np.random.normal(0,0.1,T) x_gauss_3 = np.random.normal(0,0.1,T) correlation = np.correlate(x_gauss_1,x_gauss_2,mode=\u0026#39;full\u0026#39;) plt.plot(correlation) plt.title(\u0026#34;Correlation\u0026#34;) plt.show() The sets of Gaussian signals are largely uncorrelated.\nr1= [x_gauss_1[i] + alpha* x_gauss_3[i] for i in range(T)] r2 = [x_gauss_1[i] + x_gauss_2[i] + alpha* x_gauss_3[i] for i in range(T)] correlation_gauss_r_x1 = np.correlate(x_gauss_1,r1,mode=\u0026#39;full\u0026#39;) correlation_gauss_r_x2 = np.correlate(x_gauss_2,r1,mode=\u0026#39;full\u0026#39;) correlation_gauss_r2_x1 = np.correlate(x_gauss_1,r2,mode=\u0026#39;full\u0026#39;) correlation_gauss_r2_x2 = np.correlate(x_gauss_2,r2,mode=\u0026#39;full\u0026#39;) fig, axes = plt.subplots(2, 2, figsize=(20, 10)) axes[0][0].plot(correlation_gauss_r_x1) axes[0][0].set_title(\u0026#34;Correlation gaussian1 and r = gaussian1 + alpha * gaussian 3 \u0026#34;) axes[0][1].plot(correlation_gauss_r_x2) axes[0][1].set_title(\u0026#34;Correlation gaussian2 and r\u0026#34;) axes[1][0].plot(correlation_gauss_r2_x1) axes[1][0].set_title(\u0026#34;Correlation gaussian1 and r2 = gaussian1 + gaussian2 + alpha * gaussian 3 \u0026#34;) axes[1][1].plot(correlation_gauss_r2_x2) axes[1][1].set_title(\u0026#34;Correlation gaussian2 and r2 \u0026#34;) fig.suptitle(\u0026#34;Correlation with alpha = 1 and T = 1000\u0026#34;, fontsize=30) Text(0.5, 0.98, 'Correlation with alpha = 1 and T = 1000') On the up right there is no correlation.\nT = 1000 alpha = 1 x_gauss_1 = np.random.normal(0,0.1,T) x_gauss_2 = np.random.normal(0,0.1,T) r= [x_gauss_1[i] + alpha* x_gauss_2[i] for i in range(T)] correlation_gauss_r = np.correlate(x_gauss_1,r,mode=\u0026#39;full\u0026#39;) plt.plot(correlation_gauss_r) plt.title(\u0026#34;Correlation gauss (1000) and r = gauss1 + gauss2 \u0026#34;) plt.show() fig, axes = plt.subplots(1, 3, figsize=(10, 5)) fig.suptitle(\u0026#34;r = gauss1 + alpha * gauss2 with 1000 components\u0026#34;, fontsize=14) alphas = [2,8,64] for i, alpha in enumerate(alphas): r= [x_gauss_1[i] + alpha* x_gauss_2[i] for i in range(T)] correlation_gauss_r = np.correlate(x_gauss_1,r,mode=\u0026#39;full\u0026#39;) axes[i].plot(correlation_gauss_r) axes[i].set_title(f\u0026#34;alpha = {alpha}\u0026#34;) x_gauss_1_100000 = np.random.normal(0, 0.1, 100000) x_gauss_2_100000 = np.random.normal(0, 0.1, 100000) T = 100000 r_gauss_100000 = [x_gauss_1_100000[j] + alpha * x_gauss_2_100000[j] for j in range(T)] fig2, axes2 = plt.subplots(1, 3, figsize=(10, 5)) fig2.suptitle(\u0026#34;r = gauss1 + alpha * gauss2 with 100000 components\u0026#34;, fontsize=14) for i, alpha in enumerate(alphas): correlation_gauss_r_100000 = np.correlate(x_gauss_1, r_gauss_100000, mode=\u0026#39;full\u0026#39;) axes2[i].plot(correlation_gauss_r_100000) axes2[i].set_title(f\u0026#34;{alpha}\u0026#34;) The higher the alpha and the number of samples (T), the noisier the autocorrelation.\nCDMA based watermarking import cv2 Firstly let\u0026rsquo;s compute the DCT transform.\nlena = cv2.imread(\u0026#34;../TP-HTI/lena.jpg\u0026#34;, 0) lena_f = np.float32(lena) dct_lena = cv2.dct(lena_f) idct_lena = cv2.idct(dct_lena) idct_lena = np.uint8(idct_lena) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(\u0026#34;Lena image\u0026#34;) axes[1].imshow(idct_lena, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(\u0026#34;Lena IDCT image\u0026#34;) for ax in axes: ax.axis(\u0026#39;off\u0026#39;) The image obtained after IDCT is identical.\nNow before applying IDCT, we generate a mark: a standard Gaussian noise sequence of 32x32 valus with m(1,1) = 0 and insert the mark according to the formula: niu_w = niu*(1+ alpha*m) where alpha is a scalar.\nalpha = 1 niu = dct_lena[:32,:32] m = np.random.normal(0, 1, (32, 32)) niu_w = niu*(1+ alpha*m) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].imshow(niu, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(\u0026#34;niu\u0026#34;) axes[1].imshow(niu_w, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(\u0026#34;niu_w\u0026#34;) Text(0.5, 1.0, 'niu_w') Then, we perform the IDCT transform on the watermarked image.\ndct_lena[:32,:32] = niu_w dct_lena[0,0] = niu[0,0] idct_lena = cv2.idct(dct_lena) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(\u0026#34;Lena image\u0026#34;) axes[1].imshow(idct_lena, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(\u0026#34;Lena IDCT watermarked image\u0026#34;) for ax in axes: ax.axis(\u0026#39;off\u0026#39;) With a value alpha=0.1 the PSNR is small. In order to have high image quality, let\u0026rsquo;s write a function so that PSNR(lena, lena_mark) \u0026gt; 30 dB.\ndef watermark_lena(alpha): lena = cv2.imread(\u0026#34;../TP-HTI/lena.jpg\u0026#34;, 0) lena_f = np.float32(lena) dct_lena = cv2.dct(lena_f) niu = dct_lena[:32,:32] m = np.random.normal(0, 1, (32, 32)) niu_w = niu*(1+ alpha*m) dct_lena[:32,:32] = niu_w dct_lena[0,0] = niu[0,0] idct_lena = cv2.idct(dct_lena) return idct_lena, m alpha = 1 idct_lena_w, m = watermark_lena(alpha) lena = np.float32(lena) psnr = cv2.PSNR(idct_lena, lena) while psnr \u0026lt; 30 : alpha = alpha/10 idct_lena_w,m = watermark_lena(alpha) psnr = cv2.PSNR(idct_lena_w, lena) print(\u0026#34;PSNR final\u0026#34;,psnr) print(\u0026#34;Alpha final\u0026#34;,alpha) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(\u0026#34;Lena image\u0026#34;) axes[1].imshow(idct_lena_w, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(\u0026#34;Lena IDCT watermarked image\u0026#34;) for ax in axes: ax.axis(\u0026#39;off\u0026#39;) PSNR final 46.59884513165156 Alpha final 0.01 The watermark is now imperceptible.\nDetection step def watermark_detection(lena,idct_lena_w, m, threshold=0.5): idct_lena_w_f = np.float32(idct_lena_w) dct_lena = cv2.dct(idct_lena_w_f) niu_w = dct_lena[:32,:32] niu = niu_w/(1+ alpha*m) dct_lena[:32,:32] = niu dct_lena[0,0] = niu[0,0] idct_lena = cv2.idct(dct_lena) h, w = lena.shape diff = cv2.subtract(lena, idct_lena) err = np.sum(diff**2) mse = err/(float(h*w)) if mse \u0026lt; threshold: return \u0026#34;Detected\u0026#34;, mse else: return \u0026#34;Not Detected\u0026#34;, mse print(watermark_detection(lena,idct_lena_w, m )) ('Detected', 1.0668644856437481e-09) The mean squared error between Lena and the image, from which we removed the watermark \u0026rsquo;m\u0026rsquo; from its DCT, is almost 0, which is as expected, since this image is the one to which we added the watermark.\nRobusteness to jpeg compression from PIL import Image from matplotlib import image Firstly, let\u0026rsquo;s save the watermarked in jpg with different quality factors (100, 99 and 75).\nidct_lena_w = Image.fromarray(idct_lena_w) idct_lena_w = idct_lena_w.convert(\u0026#34;L\u0026#34;) idct_lena_w.save(f\u0026#39;./results/lena_w{alpha}_100.jpg\u0026#39;, optimize=True, quality=100) idct_lena_w.save(f\u0026#39;./results/lena_w{alpha}_99.jpg\u0026#39;, optimize=True, quality=99) idct_lena_w.save(f\u0026#39;./results/lena_w{alpha}_75.jpg\u0026#39;, optimize=True, quality=75) lena_w_100 = image.imread(f\u0026#34;./results/lena_w{alpha}_100.jpg\u0026#34;) lena_w_99 = image.imread(f\u0026#34;./results/lena_w{alpha}_99.jpg\u0026#34;) lena_w_75 = image.imread(f\u0026#34;./results/lena_w{alpha}_75.jpg\u0026#34;) fig, axes = plt.subplots(1, 3, figsize=(10, 5)) axes[0].imshow(lena_w_100, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(f\u0026#34;Lena jpg image Q=100\u0026#34;) axes[1].imshow(lena_w_99, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(f\u0026#34;Lena jpg image Q=99\u0026#34;) axes[2].imshow(lena_w_75, cmap=\u0026#39;gray\u0026#39;) axes[2].set_title(f\u0026#34;Lena jpg image Q=75\u0026#34;) print(watermark_detection(lena,lena_w_100, m )) print(watermark_detection(lena,lena_w_99, m )) print(watermark_detection(lena,lena_w_75, m )) for ax in axes: ax.axis(\u0026#39;off\u0026#39;) ('Detected', 0.3582899868488312) ('Detected', 0.37200385332107544) ('Not Detected', 10.26805305480957) Even if the visual difference is imperceptible, the watermark may not be detected based on the threshold we select, particularly for lower quality images.\n","date":"October 27, 2023","hero":"/posts/dcp/watermarking-lab/front.png","permalink":"http://localhost:1313/posts/dcp/watermarking-lab/","summary":"\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eName\u003c/th\u003e\n          \u003cth\u003eCourse\u003c/th\u003e\n          \u003cth\u003eDate\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eMélanie Brégou\u003c/td\u003e\n          \u003ctd\u003eDigital content protection\u003c/td\u003e\n          \u003ctd\u003e27/10/2023\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eObjective :\u003c/strong\u003e Implement a spread spectrum watermarking method, with a focus on utilizing off-the-shelf random number generators, understanding the principles of uniform and Gaussian distributed generators, exploring correlation functions, and applying these concepts towards CDMA-based watermarking.\u003c/p\u003e\n\u003ch3 id=\"table-of-contents\"\u003eTable of Contents\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#off-the-shelf-random-number-generators\"\u003eOff-the-Shelf Random Number Generators\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#principles\"\u003ePrinciples\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#uniform-distributed-generators\"\u003eUniform Distributed Generator\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"###Gaussian-Distributed-Generators\"\u003eGaussian Distributed Generator\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#correlation-functions\"\u003eCorrelation Functions\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#graphical-representation-and-properties\"\u003eGraphical Representation and Properties\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#temporal-autocorrelation-function\"\u003eTemporal Autocorrelation Function\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#cross-correlation-function\"\u003eCross-Correlation Function\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#towards-cdma\"\u003eTowards CDMA\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#study-the-robustness-against-noise-addition\"\u003eStudy the robustness against noise addition\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#cdma-based-watermarking\"\u003eCDMA-Based Watermarking\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"off-the-shelf-random-number-generators\"\u003eOff-the-Shelf Random Number Generators\u003c/h2\u003e\n\u003ch3 id=\"principles\"\u003ePrinciples\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e matplotlib.pyplot \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e plt\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e random\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e numpy \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e np\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWe define the LCG (linear congruential generator) function : xn = (a * xn-1 + b) mod c,\u003c/p\u003e","tags":null,"title":"Spread Spectrum Watermarking Lab"},{"categories":null,"contents":"In today\u0026rsquo;s fast-paced world, technology has become an integral part of our daily lives, and it\u0026rsquo;s continually reshaping the way we go about our routines. Augmented Reality (AR) is one such technology that holds the potential to enhance human abilities and make our lives more efficient and enjoyable. In this blog article, I present an innovative project aimed at transforming the way we plan meals, manage kitchen inventory, and shop for groceries – all through the lens of AR.\nEnhanced Augmented Alimentation: AR Meal Planning, Inventory Management, and Shopping Assistance Concept: An integrated AR application that combines meal planning, kitchen inventory management, and shopping assistance to streamline the entire food preparation process.\nInteraction:\nEnhanced meal planning Meal Planning: Users input their dietary preferences and select meals they\u0026rsquo;d like to prepare for a set duration (e.g., a week). Inventory Scanning: Users scan the barcodes or visually identify food items in their kitchen, which the AR app recognizes and adds to their virtual kitchen inventory. Auto-Generated Shopping List: The app cross-references the chosen meals with the inventory and generates a shopping list for missing ingredients. Enhanced Shopping Assistance Grocery Store Navigation: When users go shopping, the AR app provides optimal in-store navigation to locate items, along with real-time stock updates.\nEnhanced Cooking Assistance Cooking Assistant: For each meal, the app offers step-by-step cooking instructions and timers using AR overlays.\nPrototype: Let\u0026rsquo;s break down the components of the prototype:\nLogging In / Signing Up: Users start by logging in or signing up. This step ensures that the app can personalize their meal planning and shopping experience. Meal Planning, Inventory Management and Grocery List Generation: Users specify what they\u0026rsquo;d like to eat for the week, along with the number of people. They use camera buttons to scan the food items they already have in their kitchen.The AI component processes the scanned items and generates a list of groceries needed for the planned meals.\nShopping Optimization: Users have the option to click on \u0026ldquo;Go shopping,\u0026rdquo; which connects the app to their AR headset. The app uses AR technology to provide an optimized path for shopping in their favorite grocery store.\nEvaluation I tried this prototype on my family and here are some feed back on the different functionnalities :\nThe login/sign-up process was straightforward and user-friendly. They didn\u0026rsquo;t encounter any issues during this stage. One suggestion is to consider adding a navigation bar at the bottom of the app. It would make it more convenient to switch between different sections of the app, especially when navigating through the AR headset. This could improve the overall user experience. ","date":"October 21, 2023","hero":"/posts/hci/futur/futur.png","permalink":"http://localhost:1313/posts/hci/futur/","summary":"\u003cp\u003eIn today\u0026rsquo;s fast-paced world, technology has become an integral part of our daily lives, and it\u0026rsquo;s continually reshaping the way we go about our routines. Augmented Reality (AR) is one such technology that holds the potential to enhance human abilities and make our lives more efficient and enjoyable. In this blog article, I present an innovative project aimed at transforming the way we plan meals, manage kitchen inventory, and shop for groceries – all through the lens of AR.\u003c/p\u003e","tags":null,"title":"Enhancing human habilities in the age of Augmented Reality"},{"categories":null,"contents":"The \u0026ldquo;FeetThrough: Electrotactile Foot Interface\u0026rdquo; is an interesting haptic input device that falls into the category of a sensory or haptic user interface. It aims to provide haptic feedback to the user\u0026rsquo;s feet, enhancing their sensory experience during foot-based activities, such as walking, stepping on virtual textures, or interacting with virtual environments. While it\u0026rsquo;s a promising concept with potential benefits, there are factors that have contributed to its limited success or challenges it may face in the future.\nChallenges and Potential Reasons for Limited Success:\nAcceptance and Adoption: The adoption of such a device is a significant challenge. Users may not be accustomed to receiving sensory input through their feet in the way they are through their hands, and it may take time for people to adapt to and accept this form of haptic feedback. Technical Limitations: The electrotactile technology may have technical limitations that hinder its widespread adoption. Issues such as power consumption, durability, and the complexity of the device can pose challenges to its practical use. Cost and Accessibility: Developing and manufacturing such devices could be costly, making them inaccessible to a broader audience. If the technology remains prohibitively expensive, it may not achieve widespread use. User Comfort and Safety: Ensuring user comfort and safety while using electrotactile stimulation is crucial. If users find the sensations uncomfortable or if there are any safety concerns (e.g., electrical shocks), it could deter adoption. ","date":"October 20, 2023","hero":"/posts/hci/failed_input/fail.png","permalink":"http://localhost:1313/posts/hci/failed_input/","summary":"\u003cp\u003eThe \u0026ldquo;\u003ca href=\"https://www.youtube.com/watch?v=rKwKWXOPFbs\" target=\"_blank\" rel=\"noopener\"\u003eFeetThrough: Electrotactile Foot Interface\u003c/a\u003e\u0026rdquo; is an interesting haptic input device that falls into the category of a sensory or haptic user interface. It aims to provide haptic feedback to the user\u0026rsquo;s feet, enhancing their sensory experience during foot-based activities, such as walking, stepping on virtual textures, or interacting with virtual environments. While it\u0026rsquo;s a promising concept with potential benefits, there are factors that have contributed to its limited success or challenges it may face in the future.\u003c/p\u003e","tags":null,"title":"Input Devices and Interaction Paradigms"},{"categories":["protection"],"contents":" Name Course Date Mélanie Brégou Digital content protection 13/10/2023 Objective : relate the transform representation and its visual impact\nTable of Contents Compute the 2D-DCT for the images Low pass filtering High pass filtering Illustrating how the spatial DCT frequencies are related the visual content Compute the 2D-DCT for the images The Two-Dimensional Discrete Cosine Transform is a mathematical operation applied in image processing to represent images in a frequency domain. This transformation is a pivotal tool for analyzing the spatial characteristics of images, allowing the separation of high-frequency components (edges or fine details) from low-frequency components (smooth transitions and gradual changes).\nimport cv2 import numpy as np import matplotlib.pyplot as plt import os Let\u0026rsquo;s calculate and save to results folder the DCT of Lena and baboon images.\nresults_dir = \u0026#34;./results\u0026#34; if not os.path.exists(results_dir): os.makedirs(results_dir) lena = cv2.imread(\u0026#34;../TP-HTI/lena.jpg\u0026#34;, 0) lena_f = np.float32(lena) dct_lena = cv2.dct(lena_f) cv2.imwrite(\u0026#34;./results/lena_dct.jpg\u0026#34;, dct_lena) baboon = cv2.imread(\u0026#34;../TP-HTI/baboon.jpg\u0026#34;, 0) baboon_f = np.float32(baboon) dct_baboon = cv2.dct(baboon_f) cv2.imwrite(\u0026#34;./results/baboon_dct.jpg\u0026#34;, dct_baboon) True Now let\u0026rsquo;s apply the Inverse Discrete Cosine Transform (IDCT) which is the reverse process of the DCT : It converts frequency domain representations back into the original spatial domain.\nidct_lena = cv2.idct(dct_lena) idct_lena = np.uint8(idct_lena) cv2.imwrite(\u0026#34;./results/lena_idct.jpg\u0026#34;, idct_lena) idct_baboon = cv2.idct(dct_baboon) idct_baboon = np.uint8(idct_baboon ) cv2.imwrite(\u0026#34;./results/baboon_idct.jpg\u0026#34;, idct_baboon ) True diff_lena = lena - dct_lena diff_baboon = baboon -dct_baboon print(f\u0026#34;The difference between lena and IDCT lena is {np.max(diff_lena)} for the maximal absolute and {np.min(diff_lena)} for the minimal absolute value. \u0026#34;) print(f\u0026#34;The difference between baboon and IDCT baboon is {np.max(diff_baboon)} for the maximal absolute and {np.min(diff_baboon)} for the minimal absolute value. \u0026#34;) The difference between lena and IDCT lena is 5380.279296875 for the maximal absolute and -63353.06640625 for the minimal absolute value. The difference between baboon and IDCT baboon is 4685.52001953125 for the maximal absolute and -65976.203125 for the minimal absolute value. image_dct_lena = cv2.imread(\u0026#34;./results/baboon_dct.jpg\u0026#34;,0) print(np.max(image_dct_lena)) plt.imshow(image_dct_lena, cmap=\u0026#39;gray\u0026#39;) 255 \u0026lt;matplotlib.image.AxesImage at 0x1792de1d0\u0026gt; fig, axes = plt.subplots(2, 3, figsize=(10, 5)) image_dct_lena = cv2.imread(\u0026#34;./results/lena_dct.jpg\u0026#34;,0) image_dct_baboon = cv2.imread(\u0026#34;./results/baboon_dct.jpg\u0026#34;,0) axes[0][0].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[0][0].set_title(\u0026#34;Lena image\u0026#34;) axes[0][1].imshow(image_dct_lena, cmap=\u0026#39;gray\u0026#39;) axes[0][1].set_title(\u0026#34;Lena DCT image\u0026#34;) axes[0][2].imshow(idct_lena, cmap=\u0026#39;gray\u0026#39;) axes[0][2].set_title(\u0026#34;Lena IDCT image\u0026#34;) axes[1][0].imshow(baboon, cmap=\u0026#39;gray\u0026#39;) axes[1][0].set_title(\u0026#34;Baboon image\u0026#34;) axes[1][1].imshow(image_dct_baboon, cmap=\u0026#39;gray\u0026#39;) axes[1][1].set_title(\u0026#34;Baboon DCT image\u0026#34;) axes[1][2].imshow(idct_baboon, cmap=\u0026#39;gray\u0026#39;) axes[1][2].set_title(\u0026#34;Baboon IDCT image\u0026#34;) for ax in axes: for row in ax : row.axis(\u0026#39;off\u0026#39;) The upper-left corner of the DCT images contains lower-frequency components, representing smoother variations in the original images, such as gradual changes in intensity. Conversely, the lower-right corner contains higher-frequency components, which capture fine details, edges, and rapid transitions. As the Baboon image has more texture, its DCT representation contains more white pixels outside the top left-hand corner.\nAfter applying IDCT, the images are reconstructed from their frequency components. However, the reconstructed images exhibit differences due to the inevitable loss of information during the DCT transformation and subsequent IDCT reconstruction. Notably, the maximal and minimal absolute differences with the original images reveal imperceptible deteriorations, highlighting the subtle information loss in the transformation process.\nLow pass filtering In this section, we examine the impact of low-pass filtering on our image reconstruction using a low pass filter before performing the IDCT. By varying the cutoff frequency of the filter, we can observe how different levels of filtering influence the reconstructed images.\ncf_list = [2**i for i in range (2, 9)] def low_pass_filter(img,fc): img_f = np.float32(img) img_dct = cv2.dct(img_f) img_dct_lp = np.zeros(img.shape) img_dct_lp[:fc,:fc] = img_dct[:fc,:fc] img_idct_lp = cv2.idct(img_dct_lp) img_idct_lp = np.uint8(img_idct_lp) return img_idct_lp fig, axes = plt.subplots(2, 4, figsize=(10, 5)) fig2,axes2 = plt.subplots(2, 4, figsize=(10, 5)) for i, fc in enumerate(cf_list) : lena_idct_lp = low_pass_filter(lena,fc) cv2.imwrite(f\u0026#34;./results/lena_idct_lp_fc_{fc}.jpg\u0026#34;, lena_idct_lp) axes[i // 4, (i % 4) ].imshow(lena_idct_lp, cmap=\u0026#39;gray\u0026#39;) axes[i // 4, (i % 4)].set_title(f\u0026#34;Low pass {fc}\u0026#34;) baboon_idct_lp = low_pass_filter(baboon,fc) cv2.imwrite(f\u0026#34;./results/baboon_idct_lp_fc_{fc}.jpg\u0026#34;, baboon_idct_lp) axes2[i // 4, (i % 4) ].imshow(baboon_idct_lp, cmap=\u0026#39;gray\u0026#39;) axes2[i // 4, (i % 4)].set_title(f\u0026#34;Low pass {fc}\u0026#34;) axes[1][3].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[1][3].set_title(f\u0026#34;Original\u0026#34;) axes2[1][3].imshow(baboon, cmap=\u0026#39;gray\u0026#39;) axes2[1][3].set_title(f\u0026#34;Original\u0026#34;) for row in axes: for ax in row: ax.axis(\u0026#39;off\u0026#39;) for row in axes2: for ax in row: ax.axis(\u0026#39;off\u0026#39;) A low-pass filter allows only lower-frequency components to pass through, smoothing the details of the image. Baboon image has more texture than Lena image and its general shape is less detectable than lena at really low fc.\nHigh pass filtering def high_pass_filter(img,fc): img_f = np.float32(img) img_dct = cv2.dct(img_f) img_dct_hp = np.zeros(img.shape) high_freq = img_dct[fc:,fc:] constant = img_dct[0][0] img_dct_hp[fc:,fc:] = high_freq img_dct_hp[0][0]= constant img_idct_hp = cv2.idct(img_dct_hp) img_idct_hp = np.uint8(img_idct_hp) return img_idct_hp fig, axes = plt.subplots(2, 4, figsize=(10, 5)) fig2,axes2 = plt.subplots(2, 4, figsize=(10, 5)) for i, fc in enumerate(cf_list) : lena_idct_hp = high_pass_filter(lena,fc) cv2.imwrite(f\u0026#34;./results/lena_idct_hp_fc_{fc}.jpg\u0026#34;, lena_idct_hp) axes[i // 4, (i % 4) ].imshow(lena_idct_hp, cmap=\u0026#39;gray\u0026#39;) axes[i // 4, (i % 4)].set_title(f\u0026#34;High pass {fc}\u0026#34;) baboon_idct_hp = high_pass_filter(baboon,fc) cv2.imwrite(f\u0026#34;./results/baboon_idct_hp_fc_{fc}.jpg\u0026#34;, baboon_idct_hp) axes2[i // 4, (i % 4) ].imshow(baboon_idct_hp, cmap=\u0026#39;gray\u0026#39;) axes2[i // 4, (i % 4)].set_title(f\u0026#34;High pass {fc}\u0026#34;) axes[1][3].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[1][3].set_title(f\u0026#34;Original\u0026#34;) axes2[1][3].imshow(baboon, cmap=\u0026#39;gray\u0026#39;) axes2[1][3].set_title(f\u0026#34;Original\u0026#34;) for row in axes: for ax in row: ax.axis(\u0026#39;off\u0026#39;) for row in axes2: for ax in row: ax.axis(\u0026#39;off\u0026#39;) The high pass filtering emphasizes the finer details and edge of the images such as the fur of the baboon. However, when the cutoff frequency (fc) is too high, it leads to significant information loss in the images and the filtered images may appear mostly gray.\nIllustrating how the spatial DCT frequencies are related the visual content def combined_image(image_lf, image_hf, fc): image_lf = np.float32(image_lf) image_hf = np.float32(image_hf) dct_lf = cv2.dct(image_lf) dct_hf = cv2.dct(image_hf) dct_lf[fc:, fc:] = dct_hf[fc:, fc:] return cv2.idct(dct_lf) lena_baboon_32 = combined_image(lena,baboon,32) plt.imshow(lena_baboon_32, cmap=\u0026#39;gray\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.show() fig, axes = plt.subplots(2, 4, figsize=(10, 5)) fig2,axes2 = plt.subplots(2, 4, figsize=(10, 5)) for i, fc in enumerate(cf_list) : lena_baboon = combined_image(lena,baboon,fc) cv2.imwrite(f\u0026#34;./results/lena_baboon_{fc}.jpg\u0026#34;, lena_baboon) axes[i // 4, (i % 4) ].imshow(lena_baboon, cmap=\u0026#39;gray\u0026#39;) axes[i // 4, (i % 4)].set_title(f\u0026#34;Lena Baboon {fc}\u0026#34;) baboon_lena = combined_image(baboon,lena,fc) cv2.imwrite(f\u0026#34;./results/baboon_lena_{fc}.jpg\u0026#34;, baboon_lena) axes2[i // 4, (i % 4) ].imshow(baboon_lena, cmap=\u0026#39;gray\u0026#39;) axes2[i // 4, (i % 4)].set_title(f\u0026#34;Baboon Lena {fc}\u0026#34;) axes[1][3].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[1][3].set_title(f\u0026#34;Lena\u0026#34;) axes2[1][3].imshow(baboon, cmap=\u0026#39;gray\u0026#39;) axes2[1][3].set_title(f\u0026#34;Baboon\u0026#34;) for row in axes: for ax in row: ax.axis(\u0026#39;off\u0026#39;) for row in axes2: for ax in row: ax.axis(\u0026#39;off\u0026#39;) Lena_baboon – obtained by considering the lower frequencies from Lena and the higher frequencies from Baboon\nBaboon_lena – obtained by considering the lower frequencies from Baboon and the higher frequencies from Lena\nTherefore images can be decomposed into various frequency components, with low-frequency components capturing large-scale features and high-frequency components representing fine details. Manipulating the DCT coefficients have a direct impact on the visual content of images, allowing us to create new visual compositions by combining frequency characteristics from the different images.\n","date":"October 13, 2023","hero":"/posts/dcp/filtering-lab/front.png","permalink":"http://localhost:1313/posts/dcp/filtering-lab/","summary":"\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eName\u003c/th\u003e\n          \u003cth\u003eCourse\u003c/th\u003e\n          \u003cth\u003eDate\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eMélanie Brégou\u003c/td\u003e\n          \u003ctd\u003eDigital content protection\u003c/td\u003e\n          \u003ctd\u003e13/10/2023\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eObjective :\u003c/strong\u003e relate the transform representation and its visual impact\u003c/p\u003e\n\u003ch3 id=\"table-of-contents\"\u003eTable of Contents\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#compute-the-2d-dct-for-the-images\"\u003eCompute the 2D-DCT for the images\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#low-pass-filtering\"\u003eLow pass filtering\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#high-pass-filtering\"\u003eHigh pass filtering\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#illustrating-how-the-spatial-dct-frequencies-are-related-the-visual-content\"\u003eIllustrating how the spatial DCT frequencies are related the visual content\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"compute-the-2d-dct-for-the-images\"\u003eCompute the 2D-DCT for the images\u003c/h3\u003e\n\u003cp\u003eThe Two-Dimensional Discrete Cosine Transform is a mathematical operation applied in image processing to represent images in a frequency domain. This transformation is a pivotal tool for analyzing the spatial characteristics of images, allowing the separation of high-frequency components (edges or fine details) from low-frequency components  (smooth transitions and gradual changes).\u003c/p\u003e","tags":null,"title":"2D Image filtering Lab"},{"categories":["unity"],"contents":"Introduction After a 12-hour class at the Institut Polytechnique de Paris, where we explored the fundamentals of Unity, Pauline Spinga and I embarked on a creative project to model and develop a music-themed platform game named \u0026ldquo;Virtuoso\u0026rdquo;. In this game, players step into the shoes of Vivaldi, a character who has lost his piano and guitar and must embark on a journey through the changing seasons to recover his instruments while collecting musical notes along the way.\n3D Modeling Our journey began with 3D modeling using Blender. We crafted various elements such as the ground, bridges, trees, houses, mushrooms, and even a snowman. For the musical notes, we sourced a model from an external website. And we included our Blender modelisation projects : my guitar and Pauline\u0026rsquo;s piano !\nPlayer Animation and Controls To bring our character to life, we incorporated animations and controls. We obtained these assets from Mixamo, which provided both the character\u0026rsquo;s appearance and animations.\nHealth system In \u0026ldquo;Virtuoso,\u0026rdquo; players confront a formidable adversary – the snowman, who relentlessly hurls snow bullets at them. To intensify the gaming experience, we introduced a health system that equips the player with three lives. Each time a player is hit by a snow bullet, they lose one of their lives. Moreover, if the player inadvertently falls into the water, it results in an immediate game over.\nMusic, Level Progression and User Interface When you first launch the game, you will find yourself on the starting page, where you can choose to begin or quit the game. The game rules are clearly displayed for your reference.\nVirtuoso is accompanied by Vivaldi\u0026rsquo;s music, and the user interface provides essential information, displaying the player\u0026rsquo;s remaining lives, the status of the piano and guitar (whether found or not), and the collected notes. Furthermore an exit button is available.\nThe game offers two distinct outcomes:\nWhen the player loses (due to falling into the water or running out of lives), a \u0026ldquo;Game Over\u0026rdquo; screen is displayed. On the other hand, when the player successfully recovers both instruments, they are greeted with a \u0026ldquo;Win\u0026rdquo; screen, and their level progresses, ranging from a novice musician to a virtuoso, based on the number of notes collected. In either case, you have the option to return to the starting page or restart the game.\n","date":"October 10, 2023","hero":"/posts/cg/unity-project/front.png","permalink":"http://localhost:1313/posts/cg/unity-project/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eAfter a 12-hour class at the Institut Polytechnique de Paris, where we explored the fundamentals of Unity, Pauline Spinga and I embarked on a creative project to model and develop a \u003cstrong\u003emusic-themed platform game named \u0026ldquo;Virtuoso\u0026rdquo;\u003c/strong\u003e. In this game, players step into the shoes of Vivaldi, a character who has lost his piano and guitar and must embark on a journey through the changing seasons to recover his instruments while collecting musical notes along the way.\u003c/p\u003e","tags":null,"title":"3D platform game Unity"},{"categories":null,"contents":"In this article, we will delve into the visionary concepts put forth by Ivan Sutherland in his seminal paper, \u0026ldquo;The Ultimate Display\u0026rdquo; written in 1965. He envisaged several possibilities, some of which have already materialized, while others remain on the horizon:\nAdvanced Graphics and Visualization: Sutherland anticipated the development of computer displays capable of rendering plot, curves or graphics. Today, modern graphics cards and software can produce highly detailed and realistic 2D and 3D graphics for applications such as gaming and scientific simulations.\nUser Interfaces: He explored various input methods, including light pens, styluses, joysticks, and voice commands, as tools for human-computer interaction. These methods are now commonplace, and we continue to witness innovations like touchscreens and gesture-based interfaces.\nKinesthetic Displays: Sutherland introduced the concept of kinesthetic displays, providing tactile feedback to users\nEye Tracking: He speculated about using eye movements to control computers, and today, eye-tracking technology is used by surgeons to consult information on a computer while operating on a patient, vehicles to obtain information about the driver’s attention or virtual reality headsets.\nWhat Could Become Reality in the Future: Haptic Feedback: Sutherland\u0026rsquo;s idea of kinesthetic displays may progress further. In the future, we could experience more advanced haptic feedback systems, providing even more realistic physical sensations in virtual environments.\nFully Immersive Environments: The concept of a room where computers control everything could evolve into even more immersive environments, pushing the boundaries between the physical and virtual realms. As technology advances, we may witness increasingly realistic and interactive simulated worlds.\n","date":"October 10, 2023","hero":"/posts/hci/ultimate_display/display.png","permalink":"http://localhost:1313/posts/hci/ultimate_display/","summary":"\u003cp\u003eIn this article, we will delve into the visionary concepts put forth by Ivan Sutherland in his seminal paper, \u0026ldquo;\u003ca href=\"http://worrydream.com/refs/Sutherland%20-%20The%20Ultimate%20Display.pdf\" target=\"_blank\" rel=\"noopener\"\u003eThe Ultimate Display\u003c/a\u003e\u0026rdquo; written in 1965. He envisaged several possibilities, some of which have already materialized, while others remain on the horizon:\u003c/p\u003e\n\u003ch5 id=\"advanced-graphics-and-visualization\"\u003eAdvanced Graphics and Visualization:\u003c/h5\u003e\n\u003cp\u003eSutherland anticipated the development of computer displays capable of rendering plot, curves or graphics. Today, modern graphics cards and software can produce highly detailed and realistic 2D and 3D graphics for applications such as gaming and scientific simulations.\u003c/p\u003e","tags":null,"title":"The Ultimate Display"},{"categories":null,"contents":"Exploring the Pioneering HCI Research of Dr. Elizabeth Churchill Brief introduction Dr. Elizabeth Churchill, currently a Director of User Experience at Google in Mountain View, California, has drawn on social, computer, engineering and data sciences to create innovative end-user applications and services for the past 20 years.\nAcademic Background She holds a PhD from the University of Cambridge, an honorary Doctor of Science (DSc.) from the University of Sussex, and in September will be awarded an honorary doctorate from the University of Stockholm. In 2016 she received a Citris-Banatao Institute Award Athena Award for Women in Technology for her Executive Leadership.\nCurrent Research Projects Dr. Elizabeth Churchill, a distinguished research scientist, is currently delving into the dynamic field of Human-Computer Interaction (HCI). Her primary research interests revolve around designing and evaluating technologies that enhance communication and connection among individuals.\nSpecifically, Dr. Churchill is focused on exploring novel HCI solutions for social contexts. Her work encompasses various facets of human interaction, including community-building, collaboration, communication enhancement, coordination, consensus-building, competition, compassion, and creativity. Her research not only seeks to design interactive applications but also to understand how people ingeniously integrate and adapt these technologies into their daily lives.\nMoreover, Dr. Churchill is intrigued by the evolving digital media landscape and \u0026ldquo;Internet ethnoscapes,\u0026rdquo; a term she borrows from Arjun Appadurai. This concept refers to the ever-shifting dynamics of individuals and groups in digital spaces, such as online communities and social media platforms.\nHer research approach takes a cross-cultural perspective, drawing from her experiences working in Japan, the United States, and the United Kingdom. She is keenly interested in how social technologies and social media are adopted and adapted within diverse cultural and social settings.\nDr. Churchill\u0026rsquo;s research is informed by a multidisciplinary approach, integrating insights from psychology, sociology, anthropology, cultural studies, urban studies, and film studies. Her professional interests span various areas within HCI, including interactive technology design, social media, augmented and virtual reality, and user experience (UX). In her quest to understand and improve the human-computer interaction landscape, she continues to innovate and contribute to the HCI field.\n","date":"October 9, 2023","hero":"/posts/hci/researcher/research.png","permalink":"http://localhost:1313/posts/hci/researcher/","summary":"\u003ch2 id=\"exploring-the-pioneering-hci-research-of-dr-elizabeth-churchill\"\u003eExploring the Pioneering HCI Research of Dr. Elizabeth Churchill\u003c/h2\u003e\n\u003ch4 id=\"brief-introduction\"\u003eBrief introduction\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"http://elizabethchurchill.com\" target=\"_blank\" rel=\"noopener\"\u003eDr. Elizabeth Churchill\u003c/a\u003e, currently a Director of User Experience at Google in Mountain View, California, has drawn on social, computer, engineering and data sciences to create innovative end-user applications and services for the past 20 years.\u003c/p\u003e\n\u003ch4 id=\"academic-background\"\u003eAcademic Background\u003c/h4\u003e\n\u003cp\u003eShe holds a PhD from the University of Cambridge, an honorary Doctor of Science (DSc.) from the University of Sussex, and in September will be awarded an honorary doctorate from the University of Stockholm. In 2016 she received a Citris-Banatao Institute Award Athena Award for Women in Technology for her Executive Leadership.\u003c/p\u003e","tags":null,"title":"Presentation of HCI researcher"},{"categories":["unity"],"contents":" Name Course Date Mélanie Brégou Digital content protection 06/10/2023 Objective : delve into image encryption and steganography, exploring classical ciphers and digital techniques.\nTable of Contents CAESAR cipher Simple substitution cipher Simple transposition cipher LSB (lowest significant bit) steganography Image compression in JPG Image compression in PNG LSB with 2 significant bit from matplotlib import image import matplotlib.pyplot as plt import numpy as np import random from PIL import Image Firstly, let\u0026rsquo;s display the images used for this lab.\nbaboon = image.imread(\u0026#34;../TP-HTI/baboon.jpg\u0026#34;) lena = image.imread(\u0026#34;../TP-HTI/lena.jpg\u0026#34;) fig, axes = plt.subplots(1, 2, figsize=(8, 4)) axes[0].imshow(baboon, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(\u0026#34;Baboon image\u0026#34;) axes[1].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(\u0026#34;Lena image\u0026#34;) for ax in axes: ax.axis(\u0026#39;off\u0026#39;) plt.show() CAESAR cypher applied to images CAESAR encryption applied to images involves shifting each pixel\u0026rsquo;s intensity by a fixed key value, with encryption adding the key and decryption subtracting it.\ndef encrypt(image,key): encrypted = np.zeros(image.shape) for i in range(image.shape[0]): for j in range(image.shape[1]): pixel = image[i][j] encrypted[i][j] = (pixel + key) % 256 return encrypted def decrypt(image,key): decrypted = np.zeros(image.shape) for i in range(image.shape[0]): for j in range(image.shape[1]): pixel = image[i][j] decrypted[i][j] = (pixel - key) % 256 return decrypted keys = [50,100,150,200] for key in keys : encrypt_baboon = encrypt(baboon, key) encrypt_lena = encrypt(lena,key) decrypt_baboon = decrypt(encrypt_baboon, key) decrypt_lena = decrypt(encrypt_lena,key) fig, axes = plt.subplots(2, 3, figsize=(10, 5)) axes[0][0].imshow(baboon, cmap=\u0026#39;gray\u0026#39;) axes[0][0].set_title(f\u0026#34;Baboon image\u0026#34;) axes[0][1].imshow(encrypt_baboon, cmap=\u0026#39;gray\u0026#39;) axes[0][1].set_title(f\u0026#34;Encrypted image with key ={key}\u0026#34;) axes[0][2].imshow(decrypt_baboon, cmap=\u0026#39;gray\u0026#39;) axes[0][2].set_title(f\u0026#34;Decrypted image with key ={key}\u0026#34;) axes[1][0].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[1][0].set_title(f\u0026#34;Lena image\u0026#34;) axes[1][1].imshow(encrypt_lena, cmap=\u0026#39;gray\u0026#39;) axes[1][1].set_title(f\u0026#34;Encrypted image with key ={key}\u0026#34;) axes[1][2].imshow(decrypt_lena, cmap=\u0026#39;gray\u0026#39;) axes[1][2].set_title(f\u0026#34;Decrypted image with key ={key}\u0026#34;) for row in axes: for ax in row: ax.axis(\u0026#39;off\u0026#39;) plt.show() The encrypted image remains recognizable, regardless of the key value. This suggests a significant limitation of the Caesar cipher for image encryption as it lacks the ability to effectively obscure the content and is highly vulnerable to attacks.\nSimple substitution cipher applied to images Substitution cipher involves changing each pixel\u0026rsquo;s value in an image by mapping it to a predefined shuffled list during encryption and back to the original value during decryption\nvalues = np.arange(255) random.shuffle(values) shuffled_values_list = values.tolist() def encrypt_substitution(image,shuffled_values_list): encrypted = np.zeros(image.shape) for i in range(image.shape[0]): for j in range(image.shape[1]): pixel = image[i][j] encrypted[i][j] = shuffled_values_list[pixel] return encrypted def decrypt_substitution(image,shuffled_values_list): decrypted = np.zeros(image.shape) for i in range(image.shape[0]): for j in range(image.shape[1]): pixel = image[i][j] decrypted[i][j] = shuffled_values_list.index(pixel) return decrypted encrypt_substitution_lena = encrypt_substitution(lena,shuffled_values_list) decrypt_substitution_lena = decrypt_substitution(encrypt_substitution_lena,shuffled_values_list) fig, axes = plt.subplots(1, 3, figsize=(10, 5)) axes[0].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(f\u0026#34;Lena image\u0026#34;) axes[1].imshow(encrypt_substitution_lena, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(f\u0026#34;Encrypted image\u0026#34;) axes[2].imshow(decrypt_substitution_lena, cmap=\u0026#39;gray\u0026#39;) axes[2].set_title(f\u0026#34;Decrypted image\u0026#34;) for ax in axes: ax.axis(\u0026#39;off\u0026#39;) The encrypted image is less visible than Caesar encryption but the general shape is still recognizable.\nSimple transposition cipher applied to images In simple transposition cipher, pixel positions are shuffled using random row and column permutations. This process includes encrypting the original image by rearranging pixel positions and then decrypting it to recover the original image.\nrows = np.arange(512) columns = np.arange(512) random.shuffle(rows) random.shuffle(columns) shuffled_rows_list = rows.tolist() shuffled_columns_list = columns.tolist() def encrypt_transposition(image,shuffled_rows_list,shuffled_columns_list): encrypted = np.zeros(image.shape) for i in range(image.shape[0]): for j in range(image.shape[1]): key_i = shuffled_rows_list[i] key_j = shuffled_columns_list[j] encrypted[i][j] = image[key_i][key_j] return encrypted def decrypt_transposition(image,shuffled_rows_list,shuffled_columns_list): decrypted = np.zeros(image.shape) for i in range(image.shape[0]): for j in range(image.shape[1]): original_i = shuffled_rows_list.index(i) original_j = shuffled_columns_list.index(j) decrypted[i][j] = image[original_i][original_j] return decrypted encrypt_transposition_lena = encrypt_transposition(lena,shuffled_rows_list,shuffled_columns_list) decrypt_transposition_lena = decrypt_transposition(encrypt_transposition_lena,shuffled_rows_list,shuffled_columns_list) fig, axes = plt.subplots(1, 3, figsize=(10, 5)) axes[0].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(f\u0026#34;Lena image\u0026#34;) axes[1].imshow(encrypt_transposition_lena, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(f\u0026#34;Encrypted image\u0026#34;) axes[2].imshow(decrypt_transposition_lena, cmap=\u0026#39;gray\u0026#39;) axes[2].set_title(f\u0026#34;Decrypted image\u0026#34;) for ax in axes: ax.axis(\u0026#39;off\u0026#39;) Lena is now well hidden in the encrypted image !\nLSB (lowest significant bit) steganography In LSB (Least Significant Bit) steganography, the process involves hiding information in an image by manipulating the least significant bits of the pixel values.\nlena_lsb_00 = lena[:256,:256] - lena[:256,:256]%2 lena_lsb_01 = lena[:256,256:] - lena[:256,256:]%2 +1 lena_lsb_10 = lena[256:,:256] - lena[256:,:256]%2 +1 lena_lsb_11 = lena[256:,256:] - lena[256:,256:]%2 fig, axes = plt.subplots(2, 2, figsize=(5, 5)) axes[0][0].imshow(lena_lsb_00, cmap=\u0026#39;gray\u0026#39;) axes[0][1].imshow(lena_lsb_01, cmap=\u0026#39;gray\u0026#39;) axes[1][0].imshow(lena_lsb_10, cmap=\u0026#39;gray\u0026#39;) axes[1][1].imshow(lena_lsb_11, cmap=\u0026#39;gray\u0026#39;) for row in axes: for ax in row: ax.axis(\u0026#39;off\u0026#39;) lena_lsb = np.zeros((512,512)) lena_lsb[:256,:256]= lena_lsb_00 lena_lsb[:256,256:] = lena_lsb_01 lena_lsb[256:,:256] = lena_lsb_10 lena_lsb[256:,256:] = lena_lsb_11 fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(f\u0026#34;Lena image\u0026#34;) axes[1].imshow(lena_lsb, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(f\u0026#34;Lena LSB image\u0026#34;) for ax in axes : ax.axis(\u0026#39;off\u0026#39;) lena_decoded = lena_lsb %2 plt.imshow(lena_decoded, cmap=\u0026#39;gray\u0026#39;) \u0026lt;matplotlib.image.AxesImage at 0x12ace1a50\u0026gt; By creating four versions of the image \u0026ldquo;lena\u0026rdquo; (lena_lsb_00, lena_lsb_01, lena_lsb_10, lena_lsb_11) the least significant bit in different regions is altered. These regions hide binary information without significantly altering the global visual appearance and the hidden information can be decoded.\nLet\u0026rsquo;s examine image compression in JPG and PNG and its consequences on the quality of the transmitted information.\nImage compression in JPG Image compression\nimg_lena_lsb = Image.fromarray(lena_lsb) img_lena_lsb = img_lena_lsb.convert(\u0026#34;L\u0026#34;) img_lena_lsb.save(\u0026#39;./lena_lsb_100.jpg\u0026#39;, optimize=True, quality=100) img_lena_lsb.save(\u0026#39;./lena_lsb_99.jpg\u0026#39;, optimize=True, quality=99) img_lena_lsb.save(\u0026#39;./lena_lsb_75.jpg\u0026#39;, optimize=True, quality=75) Compressed image and decoded information display\nlena_lsb_100 = image.imread(\u0026#34;./lena_lsb_100.jpg\u0026#34;) lena_lsb_99 = image.imread(\u0026#34;./lena_lsb_99.jpg\u0026#34;) lena_lsb_75 = image.imread(\u0026#34;./lena_lsb_75.jpg\u0026#34;) fig, axes = plt.subplots(2, 3, figsize=(10, 5)) axes[0][0].imshow(lena_lsb_100, cmap=\u0026#39;gray\u0026#39;) axes[0][0].set_title(f\u0026#34;Lena jpg image Q=100\u0026#34;) axes[0][1].imshow(lena_lsb_99, cmap=\u0026#39;gray\u0026#39;) axes[0][1].set_title(f\u0026#34;Lena jpg image Q=99\u0026#34;) axes[0][2].imshow(lena_lsb_75, cmap=\u0026#39;gray\u0026#39;) axes[0][2].set_title(f\u0026#34;Lena jpg image Q=75\u0026#34;) lena_decoded_100 = lena_lsb_100 %2 lena_decoded_99 = lena_lsb_99%2 lena_decoded_75 = lena_lsb_75%2 axes[1][0].imshow(lena_decoded_100, cmap=\u0026#39;gray\u0026#39;) axes[1][1].imshow(lena_decoded_99, cmap=\u0026#39;gray\u0026#39;) axes[1][2].imshow(lena_decoded_75, cmap=\u0026#39;gray\u0026#39;) for row in axes: for ax in row: ax.axis(\u0026#39;off\u0026#39;) The compression has no visual effet on the Lena image but deteriorates the transmitted information.\nImage compression in PNG Image compression\nimg_lena_lsb = Image.fromarray(lena_lsb) img_lena_lsb = img_lena_lsb.convert(\u0026#34;L\u0026#34;) img_lena_lsb.save(\u0026#39;./lena_lsb_9.png\u0026#39;, optimize=True, quality= 9) img_lena_lsb.save(\u0026#39;./lena_lsb_7.png\u0026#39;, optimize=True, quality=7) img_lena_lsb.save(\u0026#39;./lena_lsb_5.png\u0026#39;, optimize=True, quality=5) Compressed image and decoded information display\nlena_lsb_100_png = image.imread(\u0026#34;./lena_lsb_9.png\u0026#34;) lena_lsb_99_png = image.imread(\u0026#34;./lena_lsb_7.png\u0026#34;) lena_lsb_75_png = image.imread(\u0026#34;./lena_lsb_5.png\u0026#34;) fig, axes = plt.subplots(2, 3, figsize=(10, 5)) axes[0][0].imshow(lena_lsb_100_png, cmap=\u0026#39;gray\u0026#39;) axes[0][0].set_title(f\u0026#34;Lena png image Q=9\u0026#34;) axes[0][1].imshow(lena_lsb_99_png, cmap=\u0026#39;gray\u0026#39;) axes[0][1].set_title(f\u0026#34;Lena png image Q=7\u0026#34;) axes[0][2].imshow(lena_lsb_75_png, cmap=\u0026#39;gray\u0026#39;) axes[0][2].set_title(f\u0026#34;Lena png image Q=5\u0026#34;) lena_decoded_100_png = (lena_lsb_100_png * 255).astype(np.uint8) % 2 lena_decoded_99_png = (lena_lsb_99_png * 255).astype(np.uint8) % 2 lena_decoded_75_png = (lena_lsb_75_png * 255).astype(np.uint8) % 2 axes[1][0].imshow(lena_decoded_100_png, cmap=\u0026#39;gray\u0026#39;) axes[1][1].imshow(lena_decoded_99_png, cmap=\u0026#39;gray\u0026#39;) axes[1][2].imshow(lena_decoded_75_png, cmap=\u0026#39;gray\u0026#39;) for row in axes: for ax in row: ax.axis(\u0026#39;off\u0026#39;) The PNG compression has no effect on the Lena image and does not impact the transmitted information !\nLSB with 2 significant bit In LSB with 2 significant bits steganography, information is hidden within the image by altering the two least significant bits of pixel values.\nlena_lsb2_00 = lena[:256,:256] - lena[:256,:256]%4 lena_lsb2_01 = lena[:256,256:] - lena[:256,256:]%4 +1 lena_lsb2_10 = lena[256:,:256] - lena[256:,:256]%4 +2 lena_lsb2_11 = lena[256:,256:] - lena[256:,256:]%4 +3 fig, axes = plt.subplots(2, 2, figsize=(5, 5)) axes[0][0].imshow(lena_lsb2_00, cmap=\u0026#39;gray\u0026#39;) axes[0][1].imshow(lena_lsb2_01, cmap=\u0026#39;gray\u0026#39;) axes[1][0].imshow(lena_lsb2_10, cmap=\u0026#39;gray\u0026#39;) axes[1][1].imshow(lena_lsb2_11, cmap=\u0026#39;gray\u0026#39;) for row in axes: for ax in row: ax.axis(\u0026#39;off\u0026#39;) lena_lsb2 = np.zeros((512,512)) lena_lsb2[:256,:256]= lena_lsb2_00 lena_lsb2[:256,256:] = lena_lsb2_01 lena_lsb2[256:,:256] = lena_lsb2_10 lena_lsb2[256:,256:] = lena_lsb2_11 fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(f\u0026#34;Lena image\u0026#34;) axes[1].imshow(lena_lsb2, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(f\u0026#34;Lena 2bit-LSB image\u0026#34;) for ax in axes: ax.axis(\u0026#39;off\u0026#39;) The information added has no visual effect on Lena \u0026hellip;\nlena2_decoded = lena_lsb2 %4 plt.imshow(lena2_decoded, cmap=\u0026#39;gray\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) (-0.5, 511.5, 511.5, -0.5) \u0026hellip; but can be effectively decrypted !\nConclusion CAESAR cipher doesn\u0026rsquo;t provide privacy of the image content as well as simple substitution. However, simple transposition ensure privacy for the image content. Furthermore, information can be concealed in images with LSB technique (single or two significant bit). But the use of JPG compression may impact the quality of the decoded information.\n","date":"October 6, 2023","hero":"/posts/dcp/encryption-lab/front.png","permalink":"http://localhost:1313/posts/dcp/encryption-lab/","summary":"\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eName\u003c/th\u003e\n          \u003cth\u003eCourse\u003c/th\u003e\n          \u003cth\u003eDate\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eMélanie Brégou\u003c/td\u003e\n          \u003ctd\u003eDigital content protection\u003c/td\u003e\n          \u003ctd\u003e06/10/2023\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eObjective :\u003c/strong\u003e delve into image encryption and steganography, exploring classical ciphers and digital techniques.\u003c/p\u003e\n\u003ch3 id=\"table-of-contents\"\u003eTable of Contents\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#caesar-cypher-applied-to-images\"\u003eCAESAR cipher\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#simple-substitution-cipher-applied-to-images\"\u003eSimple substitution cipher\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#simple-transposition-cipher-applied-to-images\"\u003eSimple transposition cipher\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#lsb-lowest-significant-bit-steganography\"\u003eLSB (lowest significant bit) steganography\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#image-compression-in-jpg\"\u003eImage compression in JPG\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#image-compression-in-png\"\u003eImage compression in PNG\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#lsb-with-2-significant-bit\"\u003eLSB with 2 significant bit\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e matplotlib \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e image\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e matplotlib.pyplot \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e plt \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e numpy \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e np\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e random\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e PIL \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e Image\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eFirstly, let\u0026rsquo;s display the images used for this lab.\u003c/p\u003e","tags":null,"title":"Illustrations on image encryption and steganography Lab"},{"categories":null,"contents":"After a 6-hour lecture introducing Human-Computer Interaction (HCI), our first homework was to find real-life objects that exemplify the concepts in HCI.\nAffordances Definition Before we delve into real-life examples, let\u0026rsquo;s define what an affordance in HCI. Affordance refers to the inherent usability or functionality of an object, interface, or system. It\u0026rsquo;s a concept that suggests what actions can be performed with an object and how those actions can be intuitively understood by users.\nGood Affordance Example: The Guitar Stand One excellent example of a good affordance is a simple guitar stand. Indeed, the guitar stand provides a clear and intuitive affordance for users to place their guitars securely.\nThe mapping of where to place the guitar on the stand is straightforward. The shape of the stand corresponds to the shape of the guitar, making it easy to understand how to position the instrument. When you place a guitar on the stand, you feel a sense of stability and support. This feedback reinforces the understanding that the stand is designed for this purpose. Bad Affordance Example: The Complex Guitar Amplifier On the other hand, let\u0026rsquo;s consider a guitar amplifier with an abundance of buttons and controls as an example of a bad affordance.\nWhile the amplifier\u0026rsquo;s purpose is clear—to amplify the guitar\u0026rsquo;s sound—the multitude of buttons, knobs, and switches can overwhelm users, making it challenging to determine their functions at a glance.\nThe mapping between the controls and their effects on the sound may not be immediately evident. Users may need to consult a manual or experiment extensively to understand how to achieve their desired sound.\nHow to Improve the Guitar Amplifier\u0026rsquo;s Affordance To enhance the affordance of the guitar amplifier, several improvements can be made:\nSimplify Controls: Reduce the number of controls and group related functions together. Use labels and icons to make the purpose of each control more apparent. Visual Feedback: Incorporate more LED indicators or display screens to provide real-time visual feedback on the settings\u0026rsquo; impact on sound. Gestalt Laws Definition The Gestalt principles, developed by German psychologists, explain how humans interpret their complex surroundings. These principles help us understand why we perceive certain visual and auditory phenomena and make sense of seemingly chaotic information. There are six fundamental Gestalt principles:\nSimilarity: Similar things tend to appear grouped together, both visually and auditorily. We naturally group objects based on their similarity, such as color or sound. Prägnanz: This principle, also called the law of simplicity, suggests that our brains simplify complex objects into recognizable forms. For example, we see the Olympic logo as overlapping circles, not complex lines. Proximity: Objects physically close to each other are seen as more related. This leads to the grouping of nearby elements. Continuity: Points connected by smooth lines are perceived as related. Elements along a line or curve are seen as more connected. Closure: We perceive elements as belonging together if they complete some entity, even with missing information. Our brains fill in gaps to create meaningful patterns. Common Region: Elements within the same closed region are seen as related. This principle explains why elements within a defined boundary appear connected. Examples Here are examples of website which do not respect these laws.\nUserinyerface The game Userinyerface is a prime example of a digital interface that intentionally disregards several Gestalt Laws, creating a frustrating and confusing user experience. If you try the game you will observe that it introduces complexity and confusion in its design, challenging the notion of simplicity (Prägnanz), visual elements do not follow smooth paths or logical sequences, further adding to the disorienting experience (Continuity), consistency in color, shape, and styling is intentionally disrupted to defy user expectations (Similarity).\nThe game therefore highlights the importance of following Gestalt principles in real-world design to create user-friendly and intuitive interfaces.\nMoodle website Another example, though less critical than the game, is the login experience on the Moodle website where courses are accessed. The website design disregards Gestalt principles, leading to a potentially frustrating user experience.\nProblem: The \u0026ldquo;Connexion\u0026rdquo; (Login) button is tiny and has light text and color, making it less prominent (Prägnanz). After clicking it, users are directed to a page where they need to choose their school, introducing an unexpected step that disrupts the Law of Closure.\nCorrection: To enhance the user experience and align the design with Gestalt principles, consider implementing the following changes:\nButton Visibility: Increase the visibility of the \u0026ldquo;Connexion\u0026rdquo; button by using a contrasting color, a larger font size, or bold text to make it more prominent. This makes it easier for users to identify and initiate the login process. Streamline Login: Aim to simplify the login process. Direct users to the login form immediately after clicking the \u0026ldquo;Connexion\u0026rdquo; button. If school selection is necessary, integrate it into the login form as a dropdown menu or a clear, concise choice, rather than leading users to a separate page. Dark designs \u0026ldquo;A dark pattern is a type of user interface that appears to have been carefully crafted to trick users into doing things that are not in their interest and is usually at their expense.\u0026rdquo; Harry Brignull, a London based UX designer.\nForced Continuity When you have to start your free trial by adding your card details, or you need to enter your email to continue using a website or an app. Choosing to skip these self-interest gimmicks is not an option in forced continuity.\nTo improve the design :\nTransparent Cart Explicit consent Confirmation Roach Motel The Roach Motel design pattern makes it easy for users to sign up for a service or subscription but incredibly difficult for them to cancel or unsubscribe.\nTo redesign for an ethical use :\nClear Cancelation Process Confirmation and Feedback Transparent Pricing ","date":"September 30, 2023","hero":"/posts/hci/affordance/vox_front.jpg","permalink":"http://localhost:1313/posts/hci/affordance/","summary":"\u003cp\u003eAfter a 6-hour lecture introducing Human-Computer Interaction (HCI), our first homework was to find real-life objects that exemplify the concepts in HCI.\u003c/p\u003e\n\u003ch3 id=\"affordances\"\u003eAffordances\u003c/h3\u003e\n\u003ch5 id=\"definition\"\u003eDefinition\u003c/h5\u003e\n\u003cp\u003eBefore we delve into real-life examples, let\u0026rsquo;s define what an affordance in HCI. Affordance refers to the inherent \u003cstrong\u003eusability or functionality of an object, interface, or system\u003c/strong\u003e. It\u0026rsquo;s a concept that suggests what actions can be performed with an object and how those actions can be intuitively understood by users.\u003c/p\u003e","tags":null,"title":"Affordance in daily life"},{"categories":null,"contents":"Introduction In this article, I will walk you through the process of developing a rollerball game in Unity, where the player controls a ball on a plane, collecting stars while avoiding red enemies. My game includes both static and shooter enemies, a health system with three lives, and a Game Over screen that allows players to restart or exit the game. Let\u0026rsquo;s dive into the development process step by step.\nGame development Create the Ground, Walls, and Ball The initial step in crafting my rollerball game was setting up the game environment. I meticulously created a ground plane, enclosed it with walls to establish the play area, and strategically positioned the controllable ball at the center.\nAnimate the Ball with a Script Then I created a script to handle the ball\u0026rsquo;s animation, allowing it to roll smoothly on the ground plane and respond to user input.\nCollectibles Counter The goal of the game is to collect as many stars as possible. To do so a counter is displayed in the user interface\nHealth system I implemented a three-lives health system, adding depth to the game\u0026rsquo;s challenge. When a player lost all three lives, a \u0026ldquo;Game Over\u0026rdquo; screen would appear, offering the choice to either restart or exit the game. Unity\u0026rsquo;s user-friendly UI system streamlined the process of creating this pivotal feature.\nShooting ennemy I designed two distinct types of enemies: static and shooter. The static enemy remained stationary, and contact with it led to a life loss. The shooter enemy, on the other hand, utilized Unity\u0026rsquo;s AI Navigation system to pursue the player and shoot projectiles, adding an extra layer of challenge to the game.\nDemonstration on Computer Here is a little demo of the game :\nExtension to mobile game The game currently works on a computer, but the next step is to make it mobile-friendly. I tested it on an iPhone 14 Pro Max Emulator in Xcode. Here are the necessary steps:\nAdd the necessary modules: Unity Hub \u0026gt; Installs \u0026gt; Add Modules and add iOS build support.\nAdjust the game for the emulator by changing the setting to Player Settings \u0026gt; Target SDK: Simulator SDK.\nOpen Xcode and launch the RollerBall app on the emulator.\nExtension to VR game I also built a version for a VR Hololens headset, following my professor\u0026rsquo;s tutorial:\nInstall the OpenXR Plugin and XR Interaction Toolkit from the Package Manager (Unity Registry).\nGo to Edit \u0026gt; Project Settings \u0026gt; XR Plug-in Management and enable OpenXR.\nIn the hierarchy, create an XR Origin by right-clicking: XR \u0026gt; XR Origin (VR).\nAdd Character Controller and Character Controller Driver to the XR Origin. Ensure that the Origin Base GO is set to XR Origin.\nDelete the default main camera from the Hierarchy.\nRight-click in the Hierarchy, go to XR \u0026gt; Locomotion System (Action based), and untick Teleportation and Snap Turn Provider. In the Add component, add Continuous Move and Continuous Turn Provider (action-based). Drag and drop the XR rig and Locomotion System. Make sure Input Action Manager is set in the XR Origin.\nSet the hand XR controller parameters correctly.\nCreate an empty Prefab (HandController) in the Prefabs folder, creating a square and a cylinder to form a remote controller shape. Add it to the model prefab of the hand controller. Add a sphere collider to be able to grab objects. Add XR Direct Interactor and XR Grab Interactable to the PlayBoard.\nCreate an empty game object called AttachPoint and set it as the AttachTransform parameter.\nNow you can immerse yourself in the game and use your hands as controllers.\nThe next step, if time allows, is to create a room and make the playboard smaller to fit inside the room.\n","date":"September 30, 2023","hero":"/posts/hci/simple-unity-project/rollerball.png","permalink":"http://localhost:1313/posts/hci/simple-unity-project/","summary":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eIn this article, I will walk you through the process of developing a rollerball game in Unity, where the player controls a ball on a plane, collecting stars while avoiding red enemies. My game includes both static and shooter enemies, a health system with three lives, and a Game Over screen that allows players to restart or exit the game. Let\u0026rsquo;s dive into the development process step by step.\u003c/p\u003e","tags":null,"title":"Developing a Rollerball Game in Unity"},{"categories":null,"contents":"I installed Unity on Mac M1 sucessfully and in this article I\u0026rsquo;ll provide a brief step-by-step guide on how to install Unity based on Léa Saunier\u0026rsquo;s tutorial.\nStep 1: Download Unity Hub Begin by downloading Unity Hub from https://unity.com/download. Unity Hub acts as a central management tool for your Unity projects and installations.\nStep 2: Connect Your Account/License Open Unity Hub and log in with your Unity account or license to access Unity\u0026rsquo;s features and manage your licenses.\nStep 3: Install Unity 2022.3.2f1 Install the recommended Unity version, 2022.3.2f1, by clicking this link: unityhub://2022.3.2f1/d74737c6db50. Unity Hub will handle the installation process for you.\nStep 4: Install Required Modules Customize your Unity installation by installing modules for specific platforms like iOS, Windows, or Android. Unity Hub simplifies the process by allowing you to select the modules you need.\nStep 5: Install a Code Editor It\u0026rsquo;s highly recommended to install Visual Studio alongside Unity. This option will provide a robust code editor and seamlessly integrate with Unity. Unity will prompt you to install the Unity plugin for Visual Studio to enhance your workflow.\nStep 6 : Enjoy ! ","date":"September 30, 2023","hero":"/posts/hci/unity-install/unityinstall.png","permalink":"http://localhost:1313/posts/hci/unity-install/","summary":"\u003cp\u003eI installed Unity on Mac M1 sucessfully and in this article I\u0026rsquo;ll provide a brief step-by-step guide on how to install Unity based on Léa Saunier\u0026rsquo;s tutorial.\u003c/p\u003e\n\u003ch5 id=\"step-1-download-unity-hub\"\u003eStep 1: Download Unity Hub\u003c/h5\u003e\n\u003cp\u003eBegin by downloading Unity Hub from \u003ca href=\"https://unity.com/download\" target=\"_blank\" rel=\"noopener\"\u003ehttps://unity.com/download\u003c/a\u003e. Unity Hub acts as a central management tool for your Unity projects and installations.\u003c/p\u003e\n\u003ch5 id=\"step-2-connect-your-accountlicense\"\u003eStep 2: Connect Your Account/License\u003c/h5\u003e\n\u003cp\u003eOpen Unity Hub and log in with your Unity account or license to access Unity\u0026rsquo;s features and manage your licenses.\u003c/p\u003e","tags":null,"title":"Unity Installation"},{"categories":["blender"],"contents":"After a 9-hour introductory class to Blender, I took on the challenge of creating a 3D model of my Taylor 510ce guitar. In this article, I\u0026rsquo;ll walk you through the steps I followed to bring this project to life.\nGathering Reference image To start, I began by capturing reference photos of my guitar from both side and front angles. I then removed the background of these images using Photoshop and put them as background images on x and y axis in Blender.\nSetting Up Background Images With my reference images prepared, I imported them into Blender and positioned them as background images on both the X and Y axes. These images served as blueprints, helping me maintain the accurate proportions and details of the guitar throughout the modeling process.\nModeling the Guitar The core of this project was modeling the guitar\u0026rsquo;s intricate shape. I began by using Blender\u0026rsquo;s Knife tool to outline the guitar\u0026rsquo;s body on a plane. Afterward, I extruded and adjusted vertices to match the contours and dimensions of the reference images. Moving on to the neck and headstock, I created these components using cube primitives. I then proceeded to model other components such as the head, bridge, strings, and smaller details like the rosette, pickguard, and tuners.\nApplying Textures Once satisfied with the 3D structure, I shifted my attention to applying textures to the model. I used the reference pictures to ensure the textures were authentically replicated.\nCreating the Scene To showcase the 3D guitar effectively, I created a virtual room environment within Blender. Adjusting the lighting and spotlights helped illuminate the guitar in a way that highlighted its details.\nCrafting the Animation Finally, to provide a multifaceted view of the guitar, I created a simple camera animation.\n","date":"September 28, 2023","hero":"/posts/cg/blender-project/taylor_guitar.jpg","permalink":"http://localhost:1313/posts/cg/blender-project/","summary":"\u003cp\u003eAfter a 9-hour introductory class to Blender, I took on the challenge of creating a 3D model of my Taylor 510ce guitar. In this article, I\u0026rsquo;ll walk you through the steps I followed to bring this project to life.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/posts/cg/blender-project/taylor_guitar.png\" alt=\"Rendered image\"\u003e\u003c/p\u003e\n\u003ch4 id=\"gathering-reference-image\"\u003eGathering Reference image\u003c/h4\u003e\n\u003cp\u003eTo start, I began by capturing reference photos of my guitar from both side and front angles.\n\u003cdiv class=\"row\"\u003e\n    \n       \u003cdiv class=\"col col-sm-12 col-lg-6\"\u003e\n            \u003cimg src=\"/posts/cg/blender-project/face_guitar.png\" alt=\"face\" height=\"500\"\u003e\n\n        \u003c/div\u003e\n    \n       \u003cdiv class=\"col col-sm-12 col-lg-6\"\u003e\n            \u003cimg src=\"/posts/cg/blender-project/side_guitar.png\" alt=\"side\" height=\"500\"\u003e\n\n        \u003c/div\u003e\n    \n\u003c/div\u003e\n\u003c/p\u003e\n\u003cp\u003eI then removed the background of these images using Photoshop and put them as background images on x and y axis in Blender.\u003c/p\u003e","tags":["blender"],"title":"Blender project : My guitar in 3D !"},{"categories":null,"contents":"As part of my Human-Computer Interface class, I was tasked with creating a blog to showcase my work during my master\u0026rsquo;s program. I opted to build my website using Hugo, a fast and modern static site generator. In this guide, I will walk you through the steps to set up your own portfolio website using Hugo and deploy it on GitHub Pages.\nHere are the different steps to set up this portfolio :\nInstallation on MacOs Open a terminal and execute the following command to install Hugo using Homebrew: brew install hugo Visit the Hugo Themes website and choose the Toha theme. Then, access the Github repository. Fork the repository and rename it to [your GitHub username].github.io Clone the repository to your local machine: git clone https://github.com/[your GitHub username]/[your GitHub username].github.io.git Website personalization Once you have cloned the repository, open the project in Visual Studio Code or your preferred text editor. Modify the config.yaml file and set baseURL = https://[your github username].github.io. Choose the languages you want to use for the website Customize the template to suit your needs. You can use YAML files in the /data directory for the portfolio page and create blog articles in the /content/posts . Visualize your website locally with the following command hugo server -D Deployment in Github Pages Ensure that your repository name is [your GitHub username].github.io Create a gh-pages branch typing the following command : git checkout -b gh-pages Push the gh-pages branch to Github : git push origin gh-pages Check if the template provides a workflow in .github/workflows/deploy-site.yaml for automated deployments using GitHub Actions. Go back to the main branch and push all your changes Your portfolio website is now accessible at https://[your github username].github.io ","date":"September 23, 2023","hero":"/posts/hci/blog-creation/blog.png","permalink":"http://localhost:1313/posts/hci/blog-creation/","summary":"\u003cp\u003eAs part of my Human-Computer Interface class, I was tasked with creating a blog to showcase my work during my master\u0026rsquo;s program. I opted to build my website using Hugo, a fast and modern static site generator. In this guide, I will walk you through the steps to set up your own portfolio website using Hugo and deploy it on GitHub Pages.\u003c/p\u003e\n\u003cp\u003eHere are the different steps to set up this portfolio :\u003c/p\u003e","tags":null,"title":"Creating a Portfolio Website with Hugo and Deploying on GitHub Pages"},{"categories":["blog"],"contents":"Welcome to the blog section of my portfolio website, where I present some of my work, projects, and experiments in:\nDeep Leaning AR/VR Computer Graphics Digital Content Protection ","date":"September 22, 2023","hero":"/posts/introduction/space.jpg","permalink":"http://localhost:1313/posts/introduction/","summary":"\u003cp\u003eWelcome to the blog section of my portfolio website, where I present some of my work, \u003ca href=\"https://melaniebrg.github.io/#projects\" target=\"_blank\" rel=\"noopener\"\u003eprojects\u003c/a\u003e, and experiments in:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDeep Leaning\u003c/li\u003e\n\u003cli\u003eAR/VR\u003c/li\u003e\n\u003cli\u003eComputer Graphics\u003c/li\u003e\n\u003cli\u003eDigital Content Protection\u003c/li\u003e\n\u003c/ul\u003e","tags":["Introduction","Blog"],"title":"Introduction"},{"categories":["project","deep learning","segmentation"],"contents":"For this research project, I collaborated with Clarisse Nouet and Clara Stavun under the supervision of Catalin Fetita, Noureddine Khiati and Antoine Didier from ARTEMIS departement.\nIntroduction Context and Problem Statement Infiltrative Lung Pathologies Infiltrative lung diseases manifest as a progressive disintegration of lung tissues, resulting in impaired functioning. They can obstruct up to 40%, as depicted in the following images, representing lungs affected by IIP (fibrosis), Covid-19, and ARDS (consolidations).\nPatient Monitoring and Segmentation These patients require regular monitoring to track the progression of the disease and assess the effectiveness of treatments. The first step in this monitoring is lung segmentation. This method involves identifying and precisely delineating the lungs from medical scans, thereby obtaining quantitative measurements and specifying areas for study. This initial step will facilitate a comprehensive study of the disease, such as the proportion of diseased lung tissues or a study of tissue texture.\nProblem Statement Automated segmentation is relatively straightforward for healthy lungs. However, when a patient has infiltrative lung diseases, certain areas of the lungs appear clearer on CT scans, making differentiation between the lung and its contour difficult, especially when lung peripheries are affected. Therefore, our project aims to address the following problem: How to automate the segmentation of lungs affected by pathologies?\nState of the Art Segmentation on Healthy Lungs Today, there are numerous models of deep neural networks capable of segmenting lungs in CT scans. The most commonly used neural network architectures for lung segmentation are U-Net, cascaded convolutional networks, and residual networks. These models are trained on large annotated CT scan databases, enabling them to learn to identify lung contours and structures with increased accuracy.\nImage preprocessing techniques such as intensity normalization, noise removal, and histogram matching are often used to improve the performance of lung segmentation models. Additionally, data augmentation techniques such as rotation, cropping, and scaling are used to increase the variability of training data and enhance model robustness.\nPerformance evaluation of lung segmentation models is typically done using measures such as Dice coefficient, Jaccard index, and sensitivity. Recent work has also focused on segmenting sub-structures within the lungs, such as blood vessels and nodules, to improve the accuracy and clinical relevance of results.\nSegmentation on Diseased Lungs There are few studies that use deep learning to address the segmentation problem of lungs with pathology. Other methods are used, such as in studies [1] and [2], which deal with automated segmentation on lungs with pathologies using graph search algorithms. Pu et al [1] developed an algorithm to correct borders to include parts of the lungs that were not detected. Hua et al [2] use a graph search algorithm to segment lung tissues. The algorithm utilizes image intensity and image gradient to define a cost function. This cost function is then used to guide graph search to achieve accurate segmentation of lung tissues. However, these methods are limited, and an approach with deep learning could make automatic segmentation more effective.\nGoals The project aims to develop a robust lung segmentation method using convolutional neural networks (CNN), initially for healthy lungs then for those with infiltrative pulmonary pathologies.\nTools We carry out our project on the GPU of the ARTEMIS department with the framework Python\u0026rsquo;s PyTorch. We also use Wandb which allows us to easily visualize online the curves of the evolution of the different parameters of the model.\n1. Lung segmentation without pathology 1.1 Objective Firstly our goal was to segment healthy lungs with a convolutional neural network (CNN). This network should be able to segment the right and left lung from the CT scan of a healthy patient :\nOnce a first CNN has been built and the first results have been obtained, we optimize the model in order to be able to make a good segmentation of the lungs presenting pathologies.\n1.2 U-Net Model 1.2.1 U-Net architecture For our lung segmentation, we chose to use a U-Net architecture due to its common usage in medical segmentation problems. The U-Net model architecture is specifically designed to address the issue of semantic segmentation, where the objective is to classify each pixel of the image into different classes. The model gets its name from its \u0026ldquo;U\u0026rdquo; shape, which consists of a combination of convolutional layers and deconvolutional layers, also known as transpose layers.\nThe uniqueness of the U-Net model lies in its encoder-decoder architecture, where information is progressively downscaled by convolutional layers in the encoder to extract features, and then restored to their original size by deconvolutional layers in the decoder to perform pixel-by-pixel segmentation.\nAdditionally, the U-Net model utilizes residual connections between the layers of the encoder and decoder, allowing for direct connections between low-level and high-level information. This aids in preserving fine details of the image during segmentation.\n1.3 Access to data 1.3.1 Database presentation We had access to a database comprising 57 patients, each with hundreds of CT scans (ranging from 300 to 600 scans). Each slice, meaning each CT scan, consists of the scan image and its associated mask where the lungs have been segmented. These images have a size of 512 x 512 pixels.\nMoreover, to train, validate, and test the model on different data, the database was divided into three separate sets: the portion used for model training corresponds to 70% of the database, totaling 40 patients, the model validation corresponds to 20%, totaling 11 patients, and the remainder is used for testing, totaling 6 patients.\n1.3.2 Data generator As we are working with relatively large datasets and have limited memory available, loading all the data at once is not feasible. Therefore, we utilized a data generator, a technique commonly used to efficiently load data into memory in small batches.\nA data generator is a mechanism that generates and loads data iteratively, as needed, rather than loading it all at once. This significantly reduces memory consumption, as only the data needed at a specific time is loaded into memory.\nThe operation of a data generator typically relies on iterative loops. It divides the dataset into small parts called batches, and each batch is loaded into memory successively for processing. Once a batch is used, it is removed from memory to free up space.\nThus, the data generator allows us to efficiently load large amounts of data into our environment despite the limited memory available.\n1.4 Training and validation of the network 1.4.1 Training process and validation Once the model is built and the data is loaded, the training and validation of the model start. The training process starts by a loop on the specified number of epochs. One epoch corresponds to a complete iteration over the entire training dataset, meaning that each training sample has been presented to the model once. During each epoch, we loop through the training batches. The data and labels are loaded into GPU memory, and then the model is used to make predictions on the input data. We then compute the model\u0026rsquo;s loss at each epoch using various metrics (see next section). The loss is backpropagated through the model, and the weights are updated using the optimizer. Initially, we opt for the Adam optimizer.\nAfter each training epoch, we evaluate the model on the validation set. The validation data is loaded into GPU memory, and we make predictions using the model. We compute the total validation loss. Periodically, every few epochs, we save the model using checkpoints so that we can reload and reuse it later if needed. Once all epochs are completed, we save the final model to a .pth file. This iterative training and validation process optimizes the model weights to improve lung segmentation performance.\n1.4.2 Metrics used In order to evaluate the performance of the model and measure its ability to accurately and consistently segment the lungs, we use different metrics:\nThe dice loss, calculated at each epoch in training and validation, calculating the similarity between two sets (in our case the mask and the predicted image). This metric helps evaluate how well the predicted segmentation matches the actual areas of the lungs. A Dice coefficient close to 1 indicates accurate segmentation similar to real labels, while a value close to 0 indicates inaccurate segmentation. Its formula is, with 𝑦 the mask and 𝑝 the prediction: Dice Score: This metric, complementary to the Dice loss, is calculated as 1 minus the Dice loss. Dice Score (y, p) = 1 - Dice Loss (y, p).\nCross Entropy Loss: Widely used in classification tasks, it measures the accuracy of classification by assigning higher penalties to incorrect predictions. In the context of lung segmentation, it evaluates the model\u0026rsquo;s ability to accurately predict the different classes of pixels.\nTotal Loss: Calculated both during training and validation, the total loss is obtained by summing the Dice loss and the Cross Entropy loss for each batch, and accumulating it batch by batch. It is normalized by dividing by the total number of batches in each epoch. This metric allows us to evaluate the model\u0026rsquo;s performance over epochs, and thus, we focus on it to evaluate and optimize our model.\n1.4.3 First results With this initial model, we choose the following hyperparameters:\nBatch size of x (number of training samples used in one iteration during model training); Learning rate of x (determines the speed at which the model adjusts its weights based on the calculated error during training); Number of epochs of x. As a result, we obtain training and validation loss curves:\nTotal training loss for the first model\nTotal validation loss for the first model\nThus, we observe that the training loss stabilizes after 15 epochs and tends towards a loss of 2%. However, we notice instability in the validation loss, which may be due to overfitting (a situation where the model fits too closely to the training data and loses its ability to generalize to new data). For this reason, we will attempt to improve and optimize the model further.\n1.5 Test We carry out two types of test: on the one hand, in the same way as for validation, we study the performance of the model on data that it does not know, and on the other hand, we visualize and record the predictions on a few patients that the model did not encounter, neither in training nor in validation. As the curves of the first method are generally similar to the validation curves, we focus in the test part on the visualization. For example, on section 100 of patient 47KD:\nNative Image Mask Prediction We observe that the prediction was generally good, but with some inaccuracies at the borders of the lungs.\n1.6 Model optimisation 1.6.1 AdamW vs Adam The first modification made is the use of the AdamW optimizer, a variant of the Adam optimizer previously used. AdamW has the advantage of applying weight decay regularization during the model\u0026rsquo;s weight updates. This regularization helps reduce the risk of overfitting by penalizing high values of weights, thus promoting better model generalization. By incorporating this regularization directly into the optimizer, AdamW simplifies the training process by eliminating the need to manually adjust regularization hyperparameters.\n1.6.2 Accumulated gradient To achieve better performance, we aim to increase the batch size. However, due to limited available memory, we cannot exceed a batch size of 4. Therefore, we use accumulated gradient. Instead of updating the model\u0026rsquo;s weights after each mini-batch, gradient accumulation allows us to collect gradients over multiple mini-batches before updating the weights (the frequency of this update is defined by the hyperparameter grad_accum_step).\nFurthermore, accumulated gradient can also improve training stability by reducing fluctuations in weight updates. By averaging gradients over several mini-batches, weight updates become smoother and less likely to fluctuate significantly from one iteration to another.\nHere are the results obtained for the total training and validation losses:\nTotal training loss with and without accumulated gradient for 40 epochs\nTotal validation loss with and without accumulated gradient for 40 epochs\nThus, we observe that the total training loss converges to a similar value in the cases without and with accumulated gradient, around 0.016. For validation, the difference is more pronounced. Indeed, without accumulated gradient, the total loss converges to a value of 0.15, whereas with accumulated gradient, the convergence value is better at 0.1.\n1.6.3 Choice of Hyperparameters Number of epochs: We observed that in the case of the model with accumulated gradient, the total training loss reaches a plateau before stabilizing after 35 epochs. Therefore, we chose to train the model for 40 epochs subsequently.\nBatch size: Similar to previous settings, we maintained a batch size of 4.\nLearning rate: We opted for a learning rate of 0.01 as it fell within the recommended magnitude for our case.\ngrad_accum_step: As mentioned earlier to mitigate overfitting, we artificially increase the gradient to 32 by setting grad_accum_step to 8 in addition to the batch size of 4. This allows us to train our model on a more diverse dataset, limiting overfitting.\n1.7 Results Using the accumulated gradient model, we obtain the following prediction :\n2. Lung segmentation with pathology 2.1 Objective After segmenting healthy lungs, our goal is to segment lungs with pathologies also using a CNN. This network must be able to identify and extract each lung despite the present diseases:\nExamples of CT scans with real diseases 2.2 Generation of Pathologies As we do not have a database of diseased lungs, we simulated diseases from scans of healthy lungs. For this, we created a function that takes an image and a mask of healthy lungs as parameters. Then, using the regionprops function, we detect the two largest boxes containing the lungs from the mask. After extracting the coordinates, we choose between 7 and 9 random positions in the two detected boxes to add circles at these locations on the original lung image. These added circles have random radius and transparency to simulate the diversity of plausible diseases as accurately as possible.\nA limitation of our simulation is the circular shape of the diseases we insert, which may not necessarily be representative of the pathologies patients may encounter. Another limitation is the detection of bounding boxes for lungs on certain slices that do not show both lungs. In this case, we apply the addition of pathologies only when two lungs are detected.\nSimulation of lung disease 2.2.2 Implementation during training Instead of saving all images with simulated pathology, we chose to implement them directly during training using an approach known as \u0026ldquo;on-the-fly\u0026rdquo; generation. This method involves applying random transformations to the data samples (CT lung scans without pathologies) during training. Thus, the pathology simulations described above are applied to lung images without pathologies during training. To ensure that the model is trained not only on lungs with pathologies but also on those without, pathology generation does not occur on every training image, but only on some, with a variable probability p.\n2.3 Model Optimization 2.3.1 Choice of Hyperparameters All hyperparameters of the model remain unchanged compared to lung segmentation without pathologies.\n2.3.2 Adjustment of Pathology Appearance Frequency in Training We run the models with various frequencies of pathology appearance:\nTotal loss for training and validation of the model with pathologies of frequency 0.6, 0.8 and 0.9\nWhether during training or validation, we notice that the model with a frequency of appearance of pathologies at 0.8 has better performance.\n2.4 Results 2.4.1 Results on images with simulated pathologies Here are the results obtained on a patient data unknown to the model :\nThus we notice that the model trained with pathologies works just as well with as without pathologies, including when the diseases are present at the edge of the lung.\n2.4.1 Results on images with real pathologies For this project, we simulated the presence of pathologies in the lungs due to a lack of real data. However, these simulations are not always very representative. This is why we also did tests on some real scan images with pathology:\nWe observe that the model has difficulties with cases of real pathologies. For it to have better performance, it would be interesting to train it on a large number of data with either a high simulated disease density, or directly with real scans of affected lungs.\nConclusion Results This project implements a method for segmenting healthy lungs and those affected by pathologies in CT-scan images. To achieve this, we deployed a U-Net model and evaluated its performance using various metrics, which characterize our results as satisfactory. Model optimization involved the addition of a data generator, without which the executed programs did not yield results. We also optimized the selection of our hyperparameters to achieve a satisfactory outcome.\nLimitations It is important to note some limitations of this project. Firstly, the execution times are long. For a run of 40 epochs, it takes approximately 29 hours, making it less feasible due to the significant computational time required. Additionally, although our model yields acceptable performance, there is room for improvement.\nMoreover, as previously discussed, the results obtained on real pathology images are less satisfactory. Therefore, it is necessary to consider potential avenues for improvement to achieve better performance, such as using scans with real pathologies during training or increasing the density of simulated pathologies.\nAreas for Improvement We have considered several avenues for improvement, such as better simulating diseases to closely approximate the real aspect of pathologies. Indeed, we observed poorer estimations on real images of diseased lungs since the network is trained only on synthesized pathologies with a superimposition of circles. To address this, we could add more circles on the lungs, with the densities adding up to create obstructed areas. Additionally, we could manipulate the texture or add other shapes besides circles. Moreover, to obtain more training data for better performance, we could consider implementing additional augmentations and/or obtaining more scans with real pathologies.\nFurthermore, we could use models like ResNet in the downward part to improve performance. Finally, the project could be extended to achieve a 3D rendering of the lungs for better visualization.\n","date":"July 19, 2023","hero":"/posts/dl/lung-segmentation/front.png","permalink":"http://localhost:1313/posts/dl/lung-segmentation/","summary":"\u003cp\u003eFor this research project, I collaborated with Clarisse Nouet and Clara Stavun under the supervision of Catalin Fetita, Noureddine Khiati and Antoine Didier from ARTEMIS departement.\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003ch3 id=\"context-and-problem-statement\"\u003eContext and Problem Statement\u003c/h3\u003e\n\u003ch4 id=\"infiltrative-lung-pathologies\"\u003eInfiltrative Lung Pathologies\u003c/h4\u003e\n\u003cp\u003eInfiltrative lung diseases manifest as a progressive disintegration of lung tissues, resulting in impaired functioning. They can obstruct up to 40%, as depicted in the following images, representing lungs affected by IIP (fibrosis), Covid-19, and ARDS (consolidations).\u003c/p\u003e","tags":null,"title":"Achieving robust segmentation of lung regions in CT images irrespective to pathology"}]