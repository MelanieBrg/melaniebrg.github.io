[{"categories":null,"contents":"","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://melaniebrg.github.io/notes/go/basic/_index.bn/","summary":"","tags":null,"title":"Go বেসিক"},{"categories":null,"contents":" Hello World A sample go program is show here.\npackage main import \u0026#34;fmt\u0026#34; func main() { message := greetMe(\u0026#34;world\u0026#34;) fmt.Println(message) } func greetMe(name string) string { return \u0026#34;Hello, \u0026#34; + name + \u0026#34;!\u0026#34; } Run the program as below:\n$ go run hello.go Variables Normal Declaration:\nvar msg string msg = \u0026#34;Hello\u0026#34; Shortcut:\nmsg := \u0026#34;Hello\u0026#34; Constants const Phi = 1.618 ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://melaniebrg.github.io/notes/go/basic/introduction/","summary":" Hello World A sample go program is show here.\npackage main import \u0026#34;fmt\u0026#34; func main() { message := greetMe(\u0026#34;world\u0026#34;) fmt.Println(message) } func greetMe(name string) string { return \u0026#34;Hello, \u0026#34; + name + \u0026#34;!\u0026#34; } Run the program as below:\n$ go run hello.go Variables Normal Declaration:\nvar msg string msg = \u0026#34;Hello\u0026#34; Shortcut:\nmsg := \u0026#34;Hello\u0026#34; Constants const Phi = 1.618 ","tags":null,"title":"Introduction"},{"categories":null,"contents":" Strings str := \u0026#34;Hello\u0026#34; Multiline string\nstr := `Multiline string` Numbers Typical types\nnum := 3 // int num := 3. // float64 num := 3 + 4i // complex128 num := byte(\u0026#39;a\u0026#39;) // byte (alias for uint8) Other Types\nvar u uint = 7 // uint (unsigned) var p float32 = 22.7 // 32-bit float Arrays // var numbers [5]int numbers := [...]int{0, 0, 0, 0, 0} Pointers func main () { b := *getPointer() fmt.Println(\u0026#34;Value is\u0026#34;, b) func getPointer () (myPointer *int) { a := 234 return \u0026amp;a a := new(int) *a = 234 Pointers point to a memory location of a variable. Go is fully garbage-collected.\nType Conversion i := 2 f := float64(i) u := uint(i) Slice slice := []int{2, 3, 4} slice := []byte(\u0026#34;Hello\u0026#34;) ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://melaniebrg.github.io/notes/go/basic/types/","summary":"Strings str := \u0026#34;Hello\u0026#34; Multiline string\nstr := `Multiline string` Numbers Typical types\nnum := 3 // int num := 3. // float64 num := 3 + 4i // complex128 num := byte(\u0026#39;a\u0026#39;) // byte (alias for uint8) Other Types\nvar u uint = 7 // uint (unsigned) var p float32 = 22.7 // 32-bit float Arrays // var numbers [5]int numbers := [...]int{0, 0, 0, 0, 0} Pointers func main () { b := *getPointer() fmt.","tags":null,"title":"Basic Types"},{"categories":null,"contents":"","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://melaniebrg.github.io/notes/go/advanced/_index.bn/","summary":"","tags":null,"title":"অ্যাডভান্সড"},{"categories":null,"contents":" Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) Switch switch day { case \u0026#34;sunday\u0026#34;: // cases don\u0026#39;t \u0026#34;fall through\u0026#34; by default! fallthrough case \u0026#34;saturday\u0026#34;: rest() default: work() } Loop for count := 0; count \u0026lt;= 10; count++ { fmt.Println(\u0026#34;My counter is at\u0026#34;, count) } entry := []string{\u0026#34;Jack\u0026#34;,\u0026#34;John\u0026#34;,\u0026#34;Jones\u0026#34;} for i, val := range entry { fmt.Printf(\u0026#34;At position %d, the character %s is present\\n\u0026#34;, i, val) n := 0 x := 42 for n != x { n := guess() } ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://melaniebrg.github.io/notes/go/basic/flow-control/","summary":"Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) Switch switch day { case \u0026#34;sunday\u0026#34;: // cases don\u0026#39;t \u0026#34;fall through\u0026#34; by default! fallthrough case \u0026#34;saturday\u0026#34;: rest() default: work() } Loop for count := 0; count \u0026lt;= 10; count++ { fmt.Println(\u0026#34;My counter is at\u0026#34;, count) } entry := []string{\u0026#34;Jack\u0026#34;,\u0026#34;John\u0026#34;,\u0026#34;Jones\u0026#34;} for i, val := range entry { fmt.","tags":null,"title":"Flow Control"},{"categories":null,"contents":" Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://melaniebrg.github.io/notes/go/advanced/files/","summary":" Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) ","tags":null,"title":"File Manipulation"},{"categories":null,"contents":" Variable NAME=\u0026#34;John\u0026#34; echo $NAME echo \u0026#34;$NAME\u0026#34; echo \u0026#34;${NAME} Condition if [[ -z \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is empty\u0026#34; elif [[ -n \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is not empty\u0026#34; fi ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://melaniebrg.github.io/notes/bash/basic/","summary":" Variable NAME=\u0026#34;John\u0026#34; echo $NAME echo \u0026#34;$NAME\u0026#34; echo \u0026#34;${NAME} Condition if [[ -z \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is empty\u0026#34; elif [[ -n \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is not empty\u0026#34; fi ","tags":null,"title":"Bash Variables"},{"categories":["vr","metaquest","game","unity","escape"],"contents":"Discover our virtual reality (VR) escape game, crafted by Mélanie Brégou and Pauline Spinga. Inspired by fantastic universes and Unity Store\u0026rsquo;s Escape Room assets, we have developed original enigmas: from recreating melodies to assembling puzzles, all to unravel clues for escaping our magical-themed house.\nSet Up To begin our VR game development journey, we meticulously set up our project environment in Unity. Here\u0026rsquo;s a step-by-step guide on how we initiated the project:\nCreating the Project: We started by creating a new 3D project in Unity using the following specifications:\nUnity version: 2022.3.2f1 Template: 3D Configuring XR Plugin Management:\nNavigate to Edit \u0026gt; Project Settings \u0026gt; XR Plugin Management. Install XR Plugin Management if not already installed. Open XR Plugin Management and confirm to restart Unity. Upon restart, address any issues by: Editing the Interaction Profiles for the Oculus Touch Controller Profile. Clicking on \u0026ldquo;Fix\u0026rdquo; to resolve any Input System issues. Installing XR Interaction Toolkit:\nOpen the Window menu and select Package Manager. Choose the Unity Registry and locate XR Interaction Toolkit. Install XR Interaction Toolkit. Import the \u0026ldquo;Starter Assets\u0026rdquo; from the samples provided. Setting Up XR Origin:\nAdd the XR Origin asset to the project\u0026rsquo;s assets. Removing Default Camera:\nDelete the default Main Camera as it will be replaced with VR camera setups. Integrating Escape Room Assets:\nWe incorporated the \u0026ldquo;Escape Room\u0026rdquo; free asset from the Unity Asset Store. Customized the environment by removing unnecessary elements and puzzles to craft our unique gaming experience. Enigmas Piano Players start in a room with a wall. They must reproduce a specific melody of the radio on the piano to make the wall disappear. Puzzle Upon solving the piano enigma, players gain access to another room where puzzle pieces are hidden. These pieces, designed using Blender, provide a hint for the next action. Assembling the puzzle also reveals a key.\nIncendio The obtained key opens a drawer containing a wand. Utilizing a script from the escape room asset, players can interact with the drawer. With the wand and an image of the puzzle, players can deduce its fire-making ability. By approaching the fireplace with the wand, players trigger an animation of a parchment with the \u0026ldquo;Alohomora\u0026rdquo; inscription.\nAlohomora Referencing the Harry Potter universe, \u0026ldquo;Alohomora\u0026rdquo; is a spell that opens doors. Similar to \u0026ldquo;Incendio\u0026rdquo; players must approach the door with the wand to unlock it.\nStart and End Menu We implemented a simple menu to launch the game and included an end/restart button upon completing the game.\nPlayer Movements In our VR game developed in Unity, player movements are facilitated through a combination of continuous locomotion and teleportation mechanics.\nContinuous Movements Continuous movements allow players to navigate the virtual environment seamlessly. To implement this, we utilized the XR Interaction Toolkit along with the Starter Assets. Here\u0026rsquo;s a guide:\nXR Interaction Toolkit Setup: Navigate to Samples \u0026gt; XR Interaction Toolkit \u0026gt; 2.3.2 \u0026gt; Starter Assets. Add the XRI Default Left Controller component to the ActionBasedController object. Repeat the same process for the right hand controller. In our game we chose to have \u0026lsquo;snap turns\u0026rsquo; on the left joystick and continuous movement on the right joystick.\nXR Origin Configuration: In the Project Settings, access the Preset Manager. In the Find section, input \u0026ldquo;Right\u0026rdquo; and \u0026ldquo;Left\u0026rdquo;, then remove and re-add the XR Origin. Ensure all fields are properly filled.\nAnimated Hands: We enhanced the immersive experience by integrating animated hands. We dowloaded them via our Unity teacher.\nTeleportation Mechanism In addition to continuous movements, our game features teleportation for quick navigation and strategic positioning.\nTeleportation Areas: We designated specific areas within the virtual environment where players can teleport. Optimization Our primary goal was to maintain a performance of above 70FPS to provide a smooth experience and mitigate motion sickness. We achieved this by removing unnecessary decoration assets, baking lighting, and implementing occlusion culling.\nConclusion and Further Improvements We conducted several iterations to refine our VR escape game on MetaQuest, ensuring an immersive and enjoyable experience for players. Moving forward, we plan to continue improving the game by refining existing enigmas and adding speech recognition for spells.\n","date":"January 30, 2024","hero":"/posts/ar-vr/escape-vr/front.png","permalink":"https://melaniebrg.github.io/posts/ar-vr/escape-vr/","summary":"Discover our virtual reality (VR) escape game, crafted by Mélanie Brégou and Pauline Spinga. Inspired by fantastic universes and Unity Store\u0026rsquo;s Escape Room assets, we have developed original enigmas: from recreating melodies to assembling puzzles, all to unravel clues for escaping our magical-themed house.\nSet Up To begin our VR game development journey, we meticulously set up our project environment in Unity. Here\u0026rsquo;s a step-by-step guide on how we initiated the project:","tags":null,"title":"Escape Game in VR"},{"categories":["ar","hololens","game","unity","MRTK"],"contents":"Introducing our AR game circuit developed using Unity3D and MRTK3 for HoloLens 2. This project was brought to life through a collaborative effort between myself and Pauline Spinga.\nHere are the key steps of the project:\nDesign In designing the circuit within a ski station environment, we sourced various models from free3D, turbosquid, and sketchfab websites. Additionally, we crafted a custom panel featuring our school logo. All elements were assembled in Blender, with additional snow textures for ambiance.\nHowever, during the application build for the HoloLens, we encountered performance issues due to the weight of the model. Consequently, we had to simplify the design elements.\nGame positionning To optimize user experience, we implemented a feature allowing players to adjust the position and scale of the circuit before gameplay. This adjustment can be made using hand gestures, facilitated by clicking on the \u0026ldquo;Game\u0026rdquo; button. Once satisfied with the positioning and scale, players can confirm their selection by pressing the \u0026ldquo;Done\u0026rdquo; button.\nGame Control Initially, we implemented a joystick Canva element for sleigh control. However, it was discovered that Canva elements are not compatible with the HoloLens headset. As a workaround, we replaced the joystick with four directional buttons (forward, backward, left, and right).\nLevels and Gameplay We created two levels, both measuring player performance by the circuit\u0026rsquo;s chrono time and the collection of flakes along the track. At the end of each race, the score and time are displayed. Upon completing the first level, players have the option to replay it or progress to the second level.\nIn the second level, snow piles are introduced. Colliding with one of these snow piles results in the loss of one collected flake.\nAt each level, players can choose between two different sleigh designs.\nHoloLens Deployment To deploy our application on the HoloLens headset we had the following build settings. In OpenXR Feature Groups we selected Microsoft HoloLens with Hand Tracking and Mixed Reality Features ticked. There were the following interaction profiles:\nMicrosoft Hand Interaction Profile Microsoft Motion Controller Profile Eye Gaze Interaction Profile Once we built in Active(ARM64) platform with Active(Release) configuration, we connect the headset to the computer with a wire. We click on .snl file and open it in visual studio. Then in Project \u0026gt; properties \u0026gt; configuration properties \u0026gt; Debugging change the Machine Name. The machine name is in update and safety \u0026gt; developer mode \u0026gt; activate all togles to make ethernet work. Then enter the IP adress in machine name.\nHoloLens Deployment To deploy our application on the HoloLens headset, we configured the following build settings. In the OpenXR Feature Groups, we selected Microsoft HoloLens with Hand Tracking and Mixed Reality Features enabled. The interaction profiles included:\nMicrosoft Hand Interaction Profile Microsoft Motion Controller Profile Eye Gaze Interaction Profile After building in the Active(ARM64) platform with the Active(Release) configuration, we connected the headset to the computer using a wire. Then, we opened the .snl file in Visual Studio. To ensure ethernet functionality, we activated all toggles in Update and Safety \u0026gt; Developer Mode (hololens). Next, we entered the IP address as the machine name in Project \u0026gt; Properties \u0026gt; Configuration Properties \u0026gt; Debugging.\nConclusion and Further Improvements Creating this game was not only enjoyable but also provided us with a great AR experience. However, there are several areas where we can further enhance the game:\nAutomatic Scene Positioning: Implementing an automatic scene positioning feature would simplify the user experience, eliminating the need for manual adjustments.\nImproved UI: Enhancing the user interface with better button placement and overall design aesthetics would contribute to a more intuitive experience.\nEnhanced Speed and Trajectory Control: Improving speed control and trajectory adjustments would allow for smoother and more responsive gameplay, enhancing immersion and enjoyment for players.\nHere is a short demo :\n","date":"January 30, 2024","hero":"/posts/ar-vr/sleigh-ar/front.png","permalink":"https://melaniebrg.github.io/posts/ar-vr/sleigh-ar/","summary":"Introducing our AR game circuit developed using Unity3D and MRTK3 for HoloLens 2. This project was brought to life through a collaborative effort between myself and Pauline Spinga.\nHere are the key steps of the project:\nDesign In designing the circuit within a ski station environment, we sourced various models from free3D, turbosquid, and sketchfab websites. Additionally, we crafted a custom panel featuring our school logo. All elements were assembled in Blender, with additional snow textures for ambiance.","tags":null,"title":"Snow sleigh game on Hololens2"},{"categories":["content","protection","blockchain"],"contents":"In collaboration with Kanita Loisy and Pauline Spinga, I delved into a project focusing on the application of blockchain technology in royalty management for the course \u0026ldquo;Digital Content Protection\u0026rdquo; . As the digitization of resources continues to proliferate, the need for robust protection mechanisms has become increasingly apparent. Our project aimed to explore how blockchain could revolutionize the management of intellectual property rights, particularly in the realm of visual content.\nUnderstanding Blockchain and Its Application in Royalty Management Definition and Objectives: Blockchain: A decentralized and secure network of nodes (computers) that generates cryptographically linked immutable data blocks, serving as a digital ledger. Smart Contracts: Pieces of computer code, automated and immutable, acting as engines of decentralized applications offering various services, such as decentralized finance and marketplaces. Tokens: Represent value and digital assets on the blockchain, including transactional tokens (e.g., Gas on the Ethereum network) and user-created tokens (e.g., Ethereum, Bitcoin). Solution Offered by Blockchains: Facilitates decentralized ownership and licensing models through tokenization, providing a secure and transparent way to transfer and manage content rights. Implementation Sharing Rights on an Artwork: Overview: Consideration of an asset (painting, picture) that can be created by one or multiple artists. Each artist owns cuts, and tokens are associated to represent the right of access to the asset.\nImplementation Details:\nUtilization of Ethereum blockchain, with NFTs used to represent non-divisible assets like pictures. Deployment of smart contracts in Solidity language, enabling the creation, transfer, and ownership of tokens representing rights to artistic assets. Tools such as Ganache, Truffle, and Web3.js utilized for development and interaction with the smart contract. Ganache: Ethereum blockchain simulator\nSolidity: Programming language for smart contracts\nTruffle: Ethereum development framework\nWeb3.js: Library for interacting with Ethereum node\nIllustration: Implementation Results Our project implementation resulted in significant insights into how blockchain can streamline royalty management. Here\u0026rsquo;s a breakdown of the process illustrated with examples:\nInitial Accounts and Token Distribution The initial setup depicts fictive accounts on Ganache for Kanita and Pauline (artists) as well as Bob and Mélanie (not artists). Two tokens are created and distributed (id : 123 and 456) for Kanita\u0026rsquo;s and Pauline\u0026rsquo;s work of art, with a 50/50 share between two artists.\nMélanie Buying Token 123 Mélanie purchases Token 123, resulting in changes in account balances. The artists receive 5 ETH each, while Mélanie\u0026rsquo;s account decreases by 10 ETH.\nMélanie Selling Token to Bob Mélanie sells Token 123 to Bob, resulting in royalties for the artists. The transaction details indicate amounts exchanged between parties.\nConclusion In conclusion, protecting visual content and managing royalties can be challenging. However, the integration of blockchain technology presents a promising solution to overcome these challenges. Blockchain offers transparent, tamper-proof, and traceable transactions. The introduction of non-fungible tokens (NFTs) enables the representation of unique assets and the management of intellectual property rights. Through our implementation on the Ethereum blockchain, we have explored the creation, transfer, and ownership of tokens representing rights to artistic assets. Additionally, we have addressed the sharing of rights on artwork and the distribution of royalties.\n","date":"November 10, 2023","hero":"/posts/dcp/blockchain-project/front.png","permalink":"https://melaniebrg.github.io/posts/dcp/blockchain-project/","summary":"In collaboration with Kanita Loisy and Pauline Spinga, I delved into a project focusing on the application of blockchain technology in royalty management for the course \u0026ldquo;Digital Content Protection\u0026rdquo; . As the digitization of resources continues to proliferate, the need for robust protection mechanisms has become increasingly apparent. Our project aimed to explore how blockchain could revolutionize the management of intellectual property rights, particularly in the realm of visual content.","tags":null,"title":"Blockchain Application in Royalty Management for Digital Content Protection"},{"categories":["ar","mobile","game","unity","ARFoundation"],"contents":"In the context of our AR course at Institut Polytechnique de Paris, we were asked to develop a mobile application with AR Foundation in Unity to place object on the ground by taping its desired place on the screen.\nYour browser does not support the video tag. Here are the different steps of the project :\nUnity settings In order to make the application work on my Iphone I tried several Unity versions but the one which worked was the 2022.3.10f1. Then create a new project with AR Mobile Core template\nPlace Item script Write and put the following script with a prefab in the XR origin.\nusing System.Collections.Generic; using UnityEngine; using UnityEngine.XR.ARFoundation; using UnityEngine.XR.ARSubsystems; using EnhancedTouch = UnityEngine.InputSystem.EnhancedTouch; [RequireComponent(typeof(ARRaycastManager), typeof(ARPlaneManager))] public class PlaceItem : MonoBehaviour { [SerializeField] private GameObject prefab; private ARRaycastManager arRaycastManager; private ARPlaneManager arPlaneManager; private List\u0026lt;ARRaycastHit\u0026gt; hits = new List\u0026lt;ARRaycastHit\u0026gt;(); private void Awake() { arRaycastManager = GetComponent\u0026lt;ARRaycastManager\u0026gt;(); arPlaneManager = GetComponent\u0026lt;ARPlaneManager\u0026gt;(); } private void OnEnable() { EnhancedTouch.TouchSimulation.Enable(); EnhancedTouch.EnhancedTouchSupport.Enable(); EnhancedTouch.Touch.onFingerDown += FingerDown; } private void OnDisable() { EnhancedTouch.TouchSimulation.Disable(); EnhancedTouch.EnhancedTouchSupport.Disable(); EnhancedTouch.Touch.onFingerDown -= FingerDown; } private void FingerDown(EnhancedTouch.Finger finger) { if (finger.index != 0) return; // Pas de multi-touch if (arRaycastManager.Raycast(finger.currentTouch.screenPosition, hits, TrackableType.PlaneWithinPolygon)) { foreach (ARRaycastHit hit in hits) { Pose pose = hit.pose; Instantiate(prefab, pose.position, pose.rotation); } } } } ","date":"November 10, 2023","hero":"/posts/ar-vr/tap-to-place/front.png","permalink":"https://melaniebrg.github.io/posts/ar-vr/tap-to-place/","summary":"In the context of our AR course at Institut Polytechnique de Paris, we were asked to develop a mobile application with AR Foundation in Unity to place object on the ground by taping its desired place on the screen.\nYour browser does not support the video tag. Here are the different steps of the project :\nUnity settings In order to make the application work on my Iphone I tried several Unity versions but the one which worked was the 2022.","tags":null,"title":"Tap to Place App"},{"categories":["protection"],"contents":" Name Course Date Mélanie Brégou Digital content protection 27/10/2023 Objective : Implement a spread spectrum watermarking method, with a focus on utilizing off-the-shelf random number generators, understanding the principles of uniform and Gaussian distributed generators, exploring correlation functions, and applying these concepts towards CDMA-based watermarking.\nTable of Contents Off-the-Shelf Random Number Generators Principles Uniform Distributed Generator Gaussian Distributed Generator Correlation Functions Graphical Representation and Properties Temporal Autocorrelation Function Cross-Correlation Function Towards CDMA Study the robustness against noise addition CDMA-Based Watermarking Off-the-Shelf Random Number Generators Principles import matplotlib.pyplot as plt import random import numpy as np We define the LCG (linear congruential generator) function : xn = (a * xn-1 + b) mod c,\nwhere a, b, c are positive integers c can be a prime number, a \u0026lt; c, b \u0026lt; c def LCG(x0,a,b,c): list = [] x = x0 for n in range(0,2*c): list.append((a*x + b) % c) x= (a*x + b) % c return list def Xn(x0,a,b,c,T): x1 = (a*x0 + b) % c return Xn(x1,a,b,c,T-1) if T\u0026gt;0 else x1 Let\u0026rsquo;s generate a random sequence following the LCG rule, for the following parameters: a = 3, b = 5, c = 19 and x0 = 3\nplt.plot(LCG(x0=3,a=3,b=5,c=19)) plt.title(\u0026#34;x0=3,a=3,b=5,c=19\u0026#34;) plt.show() This parameter combination leads to repetitive sequences, resulting in patterns in the plot rather than true randomness.\nfig, axes = plt.subplots(1, 4, figsize=(10, 5)) axes[0].plot(LCG(x0=7,a=3,b=5,c=19)) axes[0].set_title(f\u0026#34;x0=7,a=3,b=5,c=19\u0026#34;) axes[1].plot(LCG(x0=3,a=3,b=7,c=19)) axes[1].set_title(f\u0026#34;x0=3,a=3,b=7,c=19\u0026#34;) axes[2].plot(LCG(x0=3,a=7,b=5,c=19)) axes[2].set_title(f\u0026#34;x0=3,a=7,b=5,c=19\u0026#34;) axes[3].plot(LCG(x0=3,a=3,b=5,c=15)) axes[3].set_title(f\u0026#34;x0=3,a=3,b=5,c=15\u0026#34;) Text(0.5, 1.0, 'x0=3,a=3,b=5,c=15') Repeating the example for:\nc = 19, a = 3, b = 5, and x0 = 7 c = 19, a = 3, b = 7, and x0 = 3 c = 19, a = 7, b = 5, and x0 = 3 c = 15, a = 3, b = 5, and x0 = 3 The generated plots are still repetitive sequences.\nNow let\u0026rsquo;s repeat the example for the MLCG (Multiple LCG), described by xn = (a * xn-1 + b * xn-2 + c) mod d, where a, b, c, d are positive integers and d can be a prime number, a \u0026lt; d, b \u0026lt; d, c \u0026lt; d\ndef MLCG(x0,x1, a,b,c,d): list = [] xn = x0 xn1 = x1 for n in range(0,2*d): list.append((a*xn1 + b*xn + c) % d) xn_ = xn xn = xn1 xn1= (a*xn1 + b*xn_ + c) % d return list fig, axes = plt.subplots(1, 3, figsize=(10, 5)) axes[0].plot(MLCG(x0=7,x1=0,a=3,b=5,c=6,d=31)) axes[0].set_title(f\u0026#34;x0=7,x1=0,a=3,b=5,c=6,d=31\u0026#34;) axes[1].plot(MLCG(x0=3,x1=2,a=3,b=7,c=6,d=31)) axes[1].set_title(f\u0026#34;x0=3,x1=2,a=3,b=7,c=6,d=31\u0026#34;) axes[2].plot(MLCG(x0=3,x1=4,a=3,b=7,c=6,d=31)) axes[2].set_title(f\u0026#34;x0=3,x1=4,a=3,b=7,c=6,d=31\u0026#34;) Text(0.5, 1.0, 'x0=3,x1=4,a=3,b=7,c=6,d=31') The generated sequences do not seem repetitive and appear to be random.\nUniform Distributed Generators def random_vector(T): return [random.uniform(0,1) for t in range(T)] T=100 x = random_vector(T) plt.plot(x) plt.title(f\u0026#34;[0, 1] uniform generator with {T} components\u0026#34;) plt.show() print(\u0026#34;mean :\u0026#34;,np.mean(x)) print(\u0026#34;variance :\u0026#34;,np.var(x)) plt.hist(x, 15, density=True) plt.title(f\u0026#34;[0, 1] uniform generator with {T} components\u0026#34;) plt.show() mean : 0.523604935218904 variance : 0.08380462274270617 The mean is approximately the theoretical mean of 0.5, which aligns with the uniform distribution in the range [0, 1]. The variance is also approximately as expected for a uniform distribution in [0, 1]. The more T is large the more uniform the sequence is.\nprint(\u0026#34;x:\u0026#34;, x) a= 1 b = 6 y1 = [a*xi + b for xi in x] print(\u0026#34;y1:\u0026#34;,y1) a= 6 b = 0 y2 = [a*xi + b for xi in x] print(\u0026#34;y2:\u0026#34;,y2) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].plot(y1) axes[0].set_title(f\u0026#34;y = x + 6\u0026#34;) axes[1].plot(y2) axes[1].set_title(f\u0026#34;y = 6x\u0026#34;) print(\u0026#34;\\nmean y = x + 6:\u0026#34;,np.mean(y1)) print(\u0026#34;variance y = x + 6:\u0026#34;,np.var(y1)) print(\u0026#34;\\nmean y = 6x :\u0026#34;,np.mean(y2)) print(\u0026#34;variance y = 6x:\u0026#34;,np.var(y2)) x: [0.25837191659828673, 0.4470563921290477, 0.21382730728002808, 0.7086462073203836, 0.3705133398244411, 0.776530587035182, 0.6245249968295411, 0.6055792797378172, 0.20047566143197193, 0.7476917028832212, 0.007605557962887088, 0.6600414938252693, 0.7897935065863732, 0.8451555781774278, 0.46916100754285683, 0.29834856967908496, 0.15299829505332485, 0.7990704904514152, 0.7052788414120111, 0.43455085071476685, 0.41080882660183615, 0.8469856705920843, 0.17925364856258053, 0.597235407898144, 0.8894834971435803, 0.17677942597730867, 0.9017874629765549, 0.1391642452258447, 0.16908696917443422, 0.9687377617065794, 0.8558605812694794, 0.9672336228942946, 0.025345710253115716, 0.9368928428842539, 0.2841382877833384, 0.2676295593554707, 0.4483339344855558, 0.3112961811035514, 0.9842870866904126, 0.2429504222912039, 0.9534669885193416, 0.856102428315788, 0.11246021882927382, 0.6255693356389754, 0.0092892352608408, 0.3484323300025869, 0.5204459560633431, 0.4491732384806463, 0.7939177716387159, 0.6778219507997898, 0.25035108261802685, 0.6757293094523129, 0.3729853645172859, 0.17497848177504938, 0.7141314038388153, 0.9839656409272567, 0.6531845148593777, 0.4053919177857205, 0.9676795868041943, 0.3719787397880119, 0.3610795330975649, 0.4260508767361372, 0.27951603091395316, 0.04360423771126598, 0.785288125768064, 0.5039429430086181, 0.1382793843880049, 0.8461760019908358, 0.9147695729971356, 0.21423640538998656, 0.10539646167702355, 0.7953651908588297, 0.3264988721469524, 0.31319950539263697, 0.1657945694437255, 0.210725128993006, 0.6459855824939482, 0.610809529651901, 0.34482684476226844, 0.003863402465228094, 0.7105935752171066, 0.9348469122045779, 0.3340640165132659, 0.19298940842885515, 0.7449744847248124, 0.7317522404803241, 0.8992896601967665, 0.5586670660533961, 0.9212492669920598, 0.5922609334191495, 0.6794701791984561, 0.8624487424953742, 0.7555926452368393, 0.37518956356210553, 0.35120430225899457, 0.7273158434032417, 0.8574333485301392, 0.2783898505862623, 0.24347399882053722, 0.9383110604207417] y1: [6.2583719165982865, 6.447056392129047, 6.213827307280028, 6.708646207320384, 6.370513339824441, 6.776530587035182, 6.624524996829541, 6.6055792797378174, 6.200475661431972, 6.7476917028832215, 6.007605557962887, 6.660041493825269, 6.789793506586373, 6.845155578177428, 6.469161007542857, 6.298348569679085, 6.152998295053325, 6.799070490451415, 6.705278841412011, 6.434550850714767, 6.410808826601836, 6.846985670592084, 6.179253648562581, 6.597235407898144, 6.88948349714358, 6.176779425977308, 6.9017874629765545, 6.139164245225845, 6.169086969174434, 6.9687377617065795, 6.8558605812694795, 6.967233622894295, 6.025345710253116, 6.936892842884254, 6.284138287783338, 6.267629559355471, 6.4483339344855555, 6.311296181103551, 6.984287086690412, 6.242950422291204, 6.953466988519342, 6.856102428315788, 6.112460218829273, 6.625569335638976, 6.009289235260841, 6.348432330002587, 6.520445956063343, 6.449173238480646, 6.793917771638716, 6.67782195079979, 6.250351082618026, 6.675729309452313, 6.372985364517286, 6.1749784817750495, 6.714131403838815, 6.983965640927257, 6.653184514859378, 6.40539191778572, 6.967679586804194, 6.371978739788012, 6.3610795330975645, 6.426050876736137, 6.279516030913953, 6.043604237711266, 6.785288125768064, 6.503942943008618, 6.138279384388005, 6.846176001990836, 6.914769572997136, 6.214236405389986, 6.1053964616770235, 6.7953651908588295, 6.326498872146953, 6.313199505392637, 6.165794569443726, 6.210725128993006, 6.645985582493948, 6.610809529651901, 6.344826844762268, 6.003863402465228, 6.710593575217107, 6.934846912204578, 6.334064016513266, 6.192989408428855, 6.744974484724812, 6.731752240480324, 6.899289660196766, 6.558667066053396, 6.92124926699206, 6.5922609334191495, 6.679470179198456, 6.862448742495374, 6.755592645236839, 6.375189563562105, 6.351204302258995, 6.727315843403241, 6.857433348530139, 6.2783898505862625, 6.243473998820537, 6.938311060420742] y2: [1.5502314995897204, 2.682338352774286, 1.2829638436801685, 4.251877243922301, 2.2230800389466467, 4.659183522211092, 3.7471499809772464, 3.6334756784269033, 1.2028539685918316, 4.486150217299327, 0.04563334777732253, 3.960248962951616, 4.738761039518239, 5.070933469064567, 2.8149660452571412, 1.7900914180745098, 0.9179897703199491, 4.794422942708492, 4.231673048472067, 2.607305104288601, 2.464852959611017, 5.081914023552506, 1.0755218913754832, 3.5834124473888638, 5.336900982861482, 1.060676555863852, 5.410724777859329, 0.8349854713550682, 1.0145218150466053, 5.812426570239476, 5.135163487616876, 5.803401737365768, 0.1520742615186943, 5.621357057305524, 1.7048297267000303, 1.6057773561328244, 2.6900036069133346, 1.8677770866213084, 5.905722520142476, 1.4577025337472234, 5.72080193111605, 5.136614569894728, 0.6747613129756429, 3.753416013833853, 0.0557354115650448, 2.0905939800155213, 3.1226757363800584, 2.6950394308838774, 4.763506629832295, 4.066931704798739, 1.5021064957081611, 4.054375856713877, 2.2379121871037153, 1.0498708906502963, 4.284788423032892, 5.90379384556354, 3.919107089156266, 2.4323515067143227, 5.806077520825166, 2.2318724387280713, 2.1664771985853895, 2.5563052604168233, 1.677096185483719, 0.2616254262675959, 4.711728754608384, 3.0236576580517083, 0.8296763063280295, 5.0770560119450145, 5.488617437982813, 1.2854184323399194, 0.6323787700621413, 4.772191145152979, 1.9589932328817143, 1.8791970323558218, 0.994767416662353, 1.264350773958036, 3.8759134949636893, 3.664857177911406, 2.0689610685736106, 0.023180414791368564, 4.26356145130264, 5.609081473227468, 2.004384099079595, 1.1579364505731309, 4.469846908348875, 4.390513442881945, 5.395737961180599, 3.3520023963203767, 5.527495601952358, 3.5535656005148972, 4.076821075190736, 5.174692454972245, 4.533555871421036, 2.251137381372633, 2.1072258135539674, 4.36389506041945, 5.144600091180835, 1.6703391035175739, 1.4608439929232233, 5.62986636252445] mean y = x + 6: 6.523604935218902 variance y = x + 6: 0.08380462274270613 mean y = 6x : 3.141629611313425 variance y = 6x: 3.016966418737421 On the left :\nWe apply a linear transformation with \u0026lsquo;a = 1\u0026rsquo; and \u0026lsquo;b = 6\u0026rsquo; to the vector x. This transformation shifts all values in x by adding 6.\nThe plot shows that each value in x has been shifted.\nMean is the expected mean of x + 6. Variance remains approximately the same as the variance of x. On the right :\nWe set \u0026lsquo;a = 6\u0026rsquo; and \u0026lsquo;b = 0\u0026rsquo;, which effectively scale all values in x by a factor of 6.\nThe plot demonstrates that all values in x have been multiplied by 6, causing a uniform stretch of the distribution.\nMean is approximately 6 times the mean of x. Variance is 6^2 times the variance of x. Gaussian Distributed Generators T= 100 gaussian = np.random.normal(0,0.1,T) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].plot(gaussian) axes[0].set_title(\u0026#34;Gaussian Distribution Plot\u0026#34;) axes[1].hist(gaussian, 10, density=True) axes[1].set_title(\u0026#34;Histogram of Gaussian Distribution\u0026#34;) print(\u0026#34;mean :\u0026#34;,np.mean(gaussian)) print(\u0026#34;variance :\u0026#34;,np.var(gaussian)) mean : 0.010568025096744142 variance : 0.012131041665992786 In this Gaussian distribution, the mean is close to zero, and the variance reflects the spread of the data around this central point both indicative of the characteristics of a typical Gaussian distribution. With a larger T, the mean would likely converge even closer to zero.\nCorrelation Functions In the domain of watermarking, autocorrelation functions are essential tools for quantifying the inherent randomness within a watermark and its correlation with the host signal. These functions are crucial for the security and resilience of embedded watermarks.\nGraphical Representation and Properties Temporal Autocorrelation Function correlation = np.correlate(x,x,mode=\u0026#39;full\u0026#39;) plt.plot(correlation) plt.title(\u0026#34;Correlation\u0026#34;) plt.show() The central spike corresponds to the peak autocorrelation represents the self-similarity or self-correlation of the signal with itself when no time lag is introduced. This is a fundamental characteristic of any signal\u0026rsquo;s autocorrelation function.\nx_1000 = random_vector(1000) x_10000 = random_vector(10000) correlation_1000 = np.correlate(x_1000,x_1000,mode=\u0026#39;full\u0026#39;) correlation_10000 = np.correlate(x_10000,x_10000,mode=\u0026#39;full\u0026#39;) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].plot(correlation_1000) axes[0].set_title(\u0026#34;Normal autocorrelation (1000)\u0026#34;) axes[1].plot(correlation_10000) axes[1].set_title(\u0026#34;Normal autocorrelation (10000)\u0026#34;) Text(0.5, 1.0, 'Normal autocorrelation (10000)') As the sample size T increase, the central spike becomes narrower and taller, and the random fluctuations become less pronounced. This is because with a larger sample size, the estimate of the autocorrelation becomes more precise and approaches the theoretical expectations.\nThe random fluctuations away from the central spike suggest that the Gaussian signal is essentially uncorrelated with itself at different time lags.\nCross Correlation Function gaussian_1000 = np.random.normal(0,0.1,1000) a= 6 b = 16 z = [a*gauss+ b for gauss in gaussian_1000] t = [a*gauss**2+ b for gauss in gaussian_1000] correlation_y1 = np.correlate(z,z,mode=\u0026#39;full\u0026#39;) correlation_y2 = np.correlate(t,t,mode=\u0026#39;full\u0026#39;) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].plot(correlation_y1) axes[0].set_title(\u0026#34;Autocorrelation z = 6x + 16 \u0026#34;) axes[1].plot(correlation_y2) axes[1].set_title(\u0026#34;Autocorrelation t = 6x**2 + 16 \u0026#34;) Text(0.5, 1.0, 'Autocorrelation t = 6x**2 + 16 ') correlation_gauss_z = np.correlate(gaussian_1000,z,mode=\u0026#39;full\u0026#39;) correlation_gauss_t = np.correlate(gaussian_1000,t,mode=\u0026#39;full\u0026#39;) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].plot(correlation_gauss_z) axes[0].set_title(\u0026#34;Correlation gaussian and z\u0026#34;) axes[1].plot(correlation_gauss_t) axes[1].set_title(\u0026#34;Correlation gaussian and t\u0026#34;) Text(0.5, 1.0, 'Correlation gaussian and t') Towards CDMA Study the robustness against noise addition In this section, we explore the robustness of correlation functions in the context of Code Division Multiple Access (CDMA) as we examine their behavior in the presence of noise.\nT = 1000 alpha = 1 x_gauss_1 = np.random.normal(0,0.1,T) x_gauss_2 = np.random.normal(0,0.1,T) x_gauss_3 = np.random.normal(0,0.1,T) correlation = np.correlate(x_gauss_1,x_gauss_2,mode=\u0026#39;full\u0026#39;) plt.plot(correlation) plt.title(\u0026#34;Correlation\u0026#34;) plt.show() The sets of Gaussian signals are largely uncorrelated.\nr1= [x_gauss_1[i] + alpha* x_gauss_3[i] for i in range(T)] r2 = [x_gauss_1[i] + x_gauss_2[i] + alpha* x_gauss_3[i] for i in range(T)] correlation_gauss_r_x1 = np.correlate(x_gauss_1,r1,mode=\u0026#39;full\u0026#39;) correlation_gauss_r_x2 = np.correlate(x_gauss_2,r1,mode=\u0026#39;full\u0026#39;) correlation_gauss_r2_x1 = np.correlate(x_gauss_1,r2,mode=\u0026#39;full\u0026#39;) correlation_gauss_r2_x2 = np.correlate(x_gauss_2,r2,mode=\u0026#39;full\u0026#39;) fig, axes = plt.subplots(2, 2, figsize=(20, 10)) axes[0][0].plot(correlation_gauss_r_x1) axes[0][0].set_title(\u0026#34;Correlation gaussian1 and r = gaussian1 + alpha * gaussian 3 \u0026#34;) axes[0][1].plot(correlation_gauss_r_x2) axes[0][1].set_title(\u0026#34;Correlation gaussian2 and r\u0026#34;) axes[1][0].plot(correlation_gauss_r2_x1) axes[1][0].set_title(\u0026#34;Correlation gaussian1 and r2 = gaussian1 + gaussian2 + alpha * gaussian 3 \u0026#34;) axes[1][1].plot(correlation_gauss_r2_x2) axes[1][1].set_title(\u0026#34;Correlation gaussian2 and r2 \u0026#34;) fig.suptitle(\u0026#34;Correlation with alpha = 1 and T = 1000\u0026#34;, fontsize=30) Text(0.5, 0.98, 'Correlation with alpha = 1 and T = 1000') On the up right there is no correlation.\nT = 1000 alpha = 1 x_gauss_1 = np.random.normal(0,0.1,T) x_gauss_2 = np.random.normal(0,0.1,T) r= [x_gauss_1[i] + alpha* x_gauss_2[i] for i in range(T)] correlation_gauss_r = np.correlate(x_gauss_1,r,mode=\u0026#39;full\u0026#39;) plt.plot(correlation_gauss_r) plt.title(\u0026#34;Correlation gauss (1000) and r = gauss1 + gauss2 \u0026#34;) plt.show() fig, axes = plt.subplots(1, 3, figsize=(10, 5)) fig.suptitle(\u0026#34;r = gauss1 + alpha * gauss2 with 1000 components\u0026#34;, fontsize=14) alphas = [2,8,64] for i, alpha in enumerate(alphas): r= [x_gauss_1[i] + alpha* x_gauss_2[i] for i in range(T)] correlation_gauss_r = np.correlate(x_gauss_1,r,mode=\u0026#39;full\u0026#39;) axes[i].plot(correlation_gauss_r) axes[i].set_title(f\u0026#34;alpha = {alpha}\u0026#34;) x_gauss_1_100000 = np.random.normal(0, 0.1, 100000) x_gauss_2_100000 = np.random.normal(0, 0.1, 100000) T = 100000 r_gauss_100000 = [x_gauss_1_100000[j] + alpha * x_gauss_2_100000[j] for j in range(T)] fig2, axes2 = plt.subplots(1, 3, figsize=(10, 5)) fig2.suptitle(\u0026#34;r = gauss1 + alpha * gauss2 with 100000 components\u0026#34;, fontsize=14) for i, alpha in enumerate(alphas): correlation_gauss_r_100000 = np.correlate(x_gauss_1, r_gauss_100000, mode=\u0026#39;full\u0026#39;) axes2[i].plot(correlation_gauss_r_100000) axes2[i].set_title(f\u0026#34;{alpha}\u0026#34;) The higher the alpha and the number of samples (T), the noisier the autocorrelation.\nCDMA based watermarking import cv2 Firstly let\u0026rsquo;s compute the DCT transform.\nlena = cv2.imread(\u0026#34;../TP-HTI/lena.jpg\u0026#34;, 0) lena_f = np.float32(lena) dct_lena = cv2.dct(lena_f) idct_lena = cv2.idct(dct_lena) idct_lena = np.uint8(idct_lena) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(\u0026#34;Lena image\u0026#34;) axes[1].imshow(idct_lena, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(\u0026#34;Lena IDCT image\u0026#34;) for ax in axes: ax.axis(\u0026#39;off\u0026#39;) The image obtained after IDCT is identical.\nNow before applying IDCT, we generate a mark: a standard Gaussian noise sequence of 32x32 valus with m(1,1) = 0 and insert the mark according to the formula: niu_w = niu*(1+ alpha*m) where alpha is a scalar.\nalpha = 1 niu = dct_lena[:32,:32] m = np.random.normal(0, 1, (32, 32)) niu_w = niu*(1+ alpha*m) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].imshow(niu, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(\u0026#34;niu\u0026#34;) axes[1].imshow(niu_w, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(\u0026#34;niu_w\u0026#34;) Text(0.5, 1.0, 'niu_w') Then, we perform the IDCT transform on the watermarked image.\ndct_lena[:32,:32] = niu_w dct_lena[0,0] = niu[0,0] idct_lena = cv2.idct(dct_lena) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(\u0026#34;Lena image\u0026#34;) axes[1].imshow(idct_lena, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(\u0026#34;Lena IDCT watermarked image\u0026#34;) for ax in axes: ax.axis(\u0026#39;off\u0026#39;) With a value alpha=0.1 the PSNR is small. In order to have high image quality, let\u0026rsquo;s write a function so that PSNR(lena, lena_mark) \u0026gt; 30 dB.\ndef watermark_lena(alpha): lena = cv2.imread(\u0026#34;../TP-HTI/lena.jpg\u0026#34;, 0) lena_f = np.float32(lena) dct_lena = cv2.dct(lena_f) niu = dct_lena[:32,:32] m = np.random.normal(0, 1, (32, 32)) niu_w = niu*(1+ alpha*m) dct_lena[:32,:32] = niu_w dct_lena[0,0] = niu[0,0] idct_lena = cv2.idct(dct_lena) return idct_lena, m alpha = 1 idct_lena_w, m = watermark_lena(alpha) lena = np.float32(lena) psnr = cv2.PSNR(idct_lena, lena) while psnr \u0026lt; 30 : alpha = alpha/10 idct_lena_w,m = watermark_lena(alpha) psnr = cv2.PSNR(idct_lena_w, lena) print(\u0026#34;PSNR final\u0026#34;,psnr) print(\u0026#34;Alpha final\u0026#34;,alpha) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(\u0026#34;Lena image\u0026#34;) axes[1].imshow(idct_lena_w, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(\u0026#34;Lena IDCT watermarked image\u0026#34;) for ax in axes: ax.axis(\u0026#39;off\u0026#39;) PSNR final 46.59884513165156 Alpha final 0.01 The watermark is now imperceptible.\nDetection step def watermark_detection(lena,idct_lena_w, m, threshold=0.5): idct_lena_w_f = np.float32(idct_lena_w) dct_lena = cv2.dct(idct_lena_w_f) niu_w = dct_lena[:32,:32] niu = niu_w/(1+ alpha*m) dct_lena[:32,:32] = niu dct_lena[0,0] = niu[0,0] idct_lena = cv2.idct(dct_lena) h, w = lena.shape diff = cv2.subtract(lena, idct_lena) err = np.sum(diff**2) mse = err/(float(h*w)) if mse \u0026lt; threshold: return \u0026#34;Detected\u0026#34;, mse else: return \u0026#34;Not Detected\u0026#34;, mse print(watermark_detection(lena,idct_lena_w, m )) ('Detected', 1.0668644856437481e-09) The mean squared error between Lena and the image, from which we removed the watermark \u0026rsquo;m\u0026rsquo; from its DCT, is almost 0, which is as expected, since this image is the one to which we added the watermark.\nRobusteness to jpeg compression from PIL import Image from matplotlib import image Firstly, let\u0026rsquo;s save the watermarked in jpg with different quality factors (100, 99 and 75).\nidct_lena_w = Image.fromarray(idct_lena_w) idct_lena_w = idct_lena_w.convert(\u0026#34;L\u0026#34;) idct_lena_w.save(f\u0026#39;./results/lena_w{alpha}_100.jpg\u0026#39;, optimize=True, quality=100) idct_lena_w.save(f\u0026#39;./results/lena_w{alpha}_99.jpg\u0026#39;, optimize=True, quality=99) idct_lena_w.save(f\u0026#39;./results/lena_w{alpha}_75.jpg\u0026#39;, optimize=True, quality=75) lena_w_100 = image.imread(f\u0026#34;./results/lena_w{alpha}_100.jpg\u0026#34;) lena_w_99 = image.imread(f\u0026#34;./results/lena_w{alpha}_99.jpg\u0026#34;) lena_w_75 = image.imread(f\u0026#34;./results/lena_w{alpha}_75.jpg\u0026#34;) fig, axes = plt.subplots(1, 3, figsize=(10, 5)) axes[0].imshow(lena_w_100, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(f\u0026#34;Lena jpg image Q=100\u0026#34;) axes[1].imshow(lena_w_99, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(f\u0026#34;Lena jpg image Q=99\u0026#34;) axes[2].imshow(lena_w_75, cmap=\u0026#39;gray\u0026#39;) axes[2].set_title(f\u0026#34;Lena jpg image Q=75\u0026#34;) print(watermark_detection(lena,lena_w_100, m )) print(watermark_detection(lena,lena_w_99, m )) print(watermark_detection(lena,lena_w_75, m )) for ax in axes: ax.axis(\u0026#39;off\u0026#39;) ('Detected', 0.3582899868488312) ('Detected', 0.37200385332107544) ('Not Detected', 10.26805305480957) Even if the visual difference is imperceptible, the watermark may not be detected based on the threshold we select, particularly for lower quality images.\n","date":"October 27, 2023","hero":"/posts/dcp/watermarking-lab/front.png","permalink":"https://melaniebrg.github.io/posts/dcp/watermarking-lab/","summary":"Name Course Date Mélanie Brégou Digital content protection 27/10/2023 Objective : Implement a spread spectrum watermarking method, with a focus on utilizing off-the-shelf random number generators, understanding the principles of uniform and Gaussian distributed generators, exploring correlation functions, and applying these concepts towards CDMA-based watermarking.\nTable of Contents Off-the-Shelf Random Number Generators Principles Uniform Distributed Generator Gaussian Distributed Generator Correlation Functions Graphical Representation and Properties Temporal Autocorrelation Function Cross-Correlation Function Towards CDMA Study the robustness against noise addition CDMA-Based Watermarking Off-the-Shelf Random Number Generators Principles import matplotlib.","tags":null,"title":"Spread Spectrum Watermarking Lab"},{"categories":["protection"],"contents":" Name Course Date Mélanie Brégou Digital content protection 13/10/2023 Objective : relate the transform representation and its visual impact\nTable of Contents Compute the 2D-DCT for the images Low pass filtering High pass filtering Illustrating how the spatial DCT frequencies are related the visual content Compute the 2D-DCT for the images The Two-Dimensional Discrete Cosine Transform is a mathematical operation applied in image processing to represent images in a frequency domain. This transformation is a pivotal tool for analyzing the spatial characteristics of images, allowing the separation of high-frequency components (edges or fine details) from low-frequency components (smooth transitions and gradual changes).\nimport cv2 import numpy as np import matplotlib.pyplot as plt import os Let\u0026rsquo;s calculate and save to results folder the DCT of Lena and baboon images.\nresults_dir = \u0026#34;./results\u0026#34; if not os.path.exists(results_dir): os.makedirs(results_dir) lena = cv2.imread(\u0026#34;../TP-HTI/lena.jpg\u0026#34;, 0) lena_f = np.float32(lena) dct_lena = cv2.dct(lena_f) cv2.imwrite(\u0026#34;./results/lena_dct.jpg\u0026#34;, dct_lena) baboon = cv2.imread(\u0026#34;../TP-HTI/baboon.jpg\u0026#34;, 0) baboon_f = np.float32(baboon) dct_baboon = cv2.dct(baboon_f) cv2.imwrite(\u0026#34;./results/baboon_dct.jpg\u0026#34;, dct_baboon) True Now let\u0026rsquo;s apply the Inverse Discrete Cosine Transform (IDCT) which is the reverse process of the DCT : It converts frequency domain representations back into the original spatial domain.\nidct_lena = cv2.idct(dct_lena) idct_lena = np.uint8(idct_lena) cv2.imwrite(\u0026#34;./results/lena_idct.jpg\u0026#34;, idct_lena) idct_baboon = cv2.idct(dct_baboon) idct_baboon = np.uint8(idct_baboon ) cv2.imwrite(\u0026#34;./results/baboon_idct.jpg\u0026#34;, idct_baboon ) True diff_lena = lena - dct_lena diff_baboon = baboon -dct_baboon print(f\u0026#34;The difference between lena and IDCT lena is {np.max(diff_lena)} for the maximal absolute and {np.min(diff_lena)} for the minimal absolute value. \u0026#34;) print(f\u0026#34;The difference between baboon and IDCT baboon is {np.max(diff_baboon)} for the maximal absolute and {np.min(diff_baboon)} for the minimal absolute value. \u0026#34;) The difference between lena and IDCT lena is 5380.279296875 for the maximal absolute and -63353.06640625 for the minimal absolute value. The difference between baboon and IDCT baboon is 4685.52001953125 for the maximal absolute and -65976.203125 for the minimal absolute value. image_dct_lena = cv2.imread(\u0026#34;./results/baboon_dct.jpg\u0026#34;,0) print(np.max(image_dct_lena)) plt.imshow(image_dct_lena, cmap=\u0026#39;gray\u0026#39;) 255 \u0026lt;matplotlib.image.AxesImage at 0x1792de1d0\u0026gt; fig, axes = plt.subplots(2, 3, figsize=(10, 5)) image_dct_lena = cv2.imread(\u0026#34;./results/lena_dct.jpg\u0026#34;,0) image_dct_baboon = cv2.imread(\u0026#34;./results/baboon_dct.jpg\u0026#34;,0) axes[0][0].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[0][0].set_title(\u0026#34;Lena image\u0026#34;) axes[0][1].imshow(image_dct_lena, cmap=\u0026#39;gray\u0026#39;) axes[0][1].set_title(\u0026#34;Lena DCT image\u0026#34;) axes[0][2].imshow(idct_lena, cmap=\u0026#39;gray\u0026#39;) axes[0][2].set_title(\u0026#34;Lena IDCT image\u0026#34;) axes[1][0].imshow(baboon, cmap=\u0026#39;gray\u0026#39;) axes[1][0].set_title(\u0026#34;Baboon image\u0026#34;) axes[1][1].imshow(image_dct_baboon, cmap=\u0026#39;gray\u0026#39;) axes[1][1].set_title(\u0026#34;Baboon DCT image\u0026#34;) axes[1][2].imshow(idct_baboon, cmap=\u0026#39;gray\u0026#39;) axes[1][2].set_title(\u0026#34;Baboon IDCT image\u0026#34;) for ax in axes: for row in ax : row.axis(\u0026#39;off\u0026#39;) The upper-left corner of the DCT images contains lower-frequency components, representing smoother variations in the original images, such as gradual changes in intensity. Conversely, the lower-right corner contains higher-frequency components, which capture fine details, edges, and rapid transitions. As the Baboon image has more texture, its DCT representation contains more white pixels outside the top left-hand corner.\nAfter applying IDCT, the images are reconstructed from their frequency components. However, the reconstructed images exhibit differences due to the inevitable loss of information during the DCT transformation and subsequent IDCT reconstruction. Notably, the maximal and minimal absolute differences with the original images reveal imperceptible deteriorations, highlighting the subtle information loss in the transformation process.\nLow pass filtering In this section, we examine the impact of low-pass filtering on our image reconstruction using a low pass filter before performing the IDCT. By varying the cutoff frequency of the filter, we can observe how different levels of filtering influence the reconstructed images.\ncf_list = [2**i for i in range (2, 9)] def low_pass_filter(img,fc): img_f = np.float32(img) img_dct = cv2.dct(img_f) img_dct_lp = np.zeros(img.shape) img_dct_lp[:fc,:fc] = img_dct[:fc,:fc] img_idct_lp = cv2.idct(img_dct_lp) img_idct_lp = np.uint8(img_idct_lp) return img_idct_lp fig, axes = plt.subplots(2, 4, figsize=(10, 5)) fig2,axes2 = plt.subplots(2, 4, figsize=(10, 5)) for i, fc in enumerate(cf_list) : lena_idct_lp = low_pass_filter(lena,fc) cv2.imwrite(f\u0026#34;./results/lena_idct_lp_fc_{fc}.jpg\u0026#34;, lena_idct_lp) axes[i // 4, (i % 4) ].imshow(lena_idct_lp, cmap=\u0026#39;gray\u0026#39;) axes[i // 4, (i % 4)].set_title(f\u0026#34;Low pass {fc}\u0026#34;) baboon_idct_lp = low_pass_filter(baboon,fc) cv2.imwrite(f\u0026#34;./results/baboon_idct_lp_fc_{fc}.jpg\u0026#34;, baboon_idct_lp) axes2[i // 4, (i % 4) ].imshow(baboon_idct_lp, cmap=\u0026#39;gray\u0026#39;) axes2[i // 4, (i % 4)].set_title(f\u0026#34;Low pass {fc}\u0026#34;) axes[1][3].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[1][3].set_title(f\u0026#34;Original\u0026#34;) axes2[1][3].imshow(baboon, cmap=\u0026#39;gray\u0026#39;) axes2[1][3].set_title(f\u0026#34;Original\u0026#34;) for row in axes: for ax in row: ax.axis(\u0026#39;off\u0026#39;) for row in axes2: for ax in row: ax.axis(\u0026#39;off\u0026#39;) A low-pass filter allows only lower-frequency components to pass through, smoothing the details of the image. Baboon image has more texture than Lena image and its general shape is less detectable than lena at really low fc.\nHigh pass filtering def high_pass_filter(img,fc): img_f = np.float32(img) img_dct = cv2.dct(img_f) img_dct_hp = np.zeros(img.shape) high_freq = img_dct[fc:,fc:] constant = img_dct[0][0] img_dct_hp[fc:,fc:] = high_freq img_dct_hp[0][0]= constant img_idct_hp = cv2.idct(img_dct_hp) img_idct_hp = np.uint8(img_idct_hp) return img_idct_hp fig, axes = plt.subplots(2, 4, figsize=(10, 5)) fig2,axes2 = plt.subplots(2, 4, figsize=(10, 5)) for i, fc in enumerate(cf_list) : lena_idct_hp = high_pass_filter(lena,fc) cv2.imwrite(f\u0026#34;./results/lena_idct_hp_fc_{fc}.jpg\u0026#34;, lena_idct_hp) axes[i // 4, (i % 4) ].imshow(lena_idct_hp, cmap=\u0026#39;gray\u0026#39;) axes[i // 4, (i % 4)].set_title(f\u0026#34;High pass {fc}\u0026#34;) baboon_idct_hp = high_pass_filter(baboon,fc) cv2.imwrite(f\u0026#34;./results/baboon_idct_hp_fc_{fc}.jpg\u0026#34;, baboon_idct_hp) axes2[i // 4, (i % 4) ].imshow(baboon_idct_hp, cmap=\u0026#39;gray\u0026#39;) axes2[i // 4, (i % 4)].set_title(f\u0026#34;High pass {fc}\u0026#34;) axes[1][3].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[1][3].set_title(f\u0026#34;Original\u0026#34;) axes2[1][3].imshow(baboon, cmap=\u0026#39;gray\u0026#39;) axes2[1][3].set_title(f\u0026#34;Original\u0026#34;) for row in axes: for ax in row: ax.axis(\u0026#39;off\u0026#39;) for row in axes2: for ax in row: ax.axis(\u0026#39;off\u0026#39;) The high pass filtering emphasizes the finer details and edge of the images such as the fur of the baboon. However, when the cutoff frequency (fc) is too high, it leads to significant information loss in the images and the filtered images may appear mostly gray.\nIllustrating how the spatial DCT frequencies are related the visual content def combined_image(image_lf, image_hf, fc): image_lf = np.float32(image_lf) image_hf = np.float32(image_hf) dct_lf = cv2.dct(image_lf) dct_hf = cv2.dct(image_hf) dct_lf[fc:, fc:] = dct_hf[fc:, fc:] return cv2.idct(dct_lf) lena_baboon_32 = combined_image(lena,baboon,32) plt.imshow(lena_baboon_32, cmap=\u0026#39;gray\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.show() fig, axes = plt.subplots(2, 4, figsize=(10, 5)) fig2,axes2 = plt.subplots(2, 4, figsize=(10, 5)) for i, fc in enumerate(cf_list) : lena_baboon = combined_image(lena,baboon,fc) cv2.imwrite(f\u0026#34;./results/lena_baboon_{fc}.jpg\u0026#34;, lena_baboon) axes[i // 4, (i % 4) ].imshow(lena_baboon, cmap=\u0026#39;gray\u0026#39;) axes[i // 4, (i % 4)].set_title(f\u0026#34;Lena Baboon {fc}\u0026#34;) baboon_lena = combined_image(baboon,lena,fc) cv2.imwrite(f\u0026#34;./results/baboon_lena_{fc}.jpg\u0026#34;, baboon_lena) axes2[i // 4, (i % 4) ].imshow(baboon_lena, cmap=\u0026#39;gray\u0026#39;) axes2[i // 4, (i % 4)].set_title(f\u0026#34;Baboon Lena {fc}\u0026#34;) axes[1][3].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[1][3].set_title(f\u0026#34;Lena\u0026#34;) axes2[1][3].imshow(baboon, cmap=\u0026#39;gray\u0026#39;) axes2[1][3].set_title(f\u0026#34;Baboon\u0026#34;) for row in axes: for ax in row: ax.axis(\u0026#39;off\u0026#39;) for row in axes2: for ax in row: ax.axis(\u0026#39;off\u0026#39;) Lena_baboon – obtained by considering the lower frequencies from Lena and the higher frequencies from Baboon\nBaboon_lena – obtained by considering the lower frequencies from Baboon and the higher frequencies from Lena\nTherefore images can be decomposed into various frequency components, with low-frequency components capturing large-scale features and high-frequency components representing fine details. Manipulating the DCT coefficients have a direct impact on the visual content of images, allowing us to create new visual compositions by combining frequency characteristics from the different images.\n","date":"October 13, 2023","hero":"/posts/dcp/filtering-lab/front.png","permalink":"https://melaniebrg.github.io/posts/dcp/filtering-lab/","summary":"Name Course Date Mélanie Brégou Digital content protection 13/10/2023 Objective : relate the transform representation and its visual impact\nTable of Contents Compute the 2D-DCT for the images Low pass filtering High pass filtering Illustrating how the spatial DCT frequencies are related the visual content Compute the 2D-DCT for the images The Two-Dimensional Discrete Cosine Transform is a mathematical operation applied in image processing to represent images in a frequency domain.","tags":null,"title":"2D Image filtering Lab"},{"categories":["unity"],"contents":"Introduction After a 12-hour class at the Institut Polytechnique de Paris, where we explored the fundamentals of Unity, Pauline Spinga and I embarked on a creative project to model and develop a music-themed platform game named \u0026ldquo;Virtuoso\u0026rdquo;. In this game, players step into the shoes of Vivaldi, a character who has lost his piano and guitar and must embark on a journey through the changing seasons to recover his instruments while collecting musical notes along the way.\n3D Modeling Our journey began with 3D modeling using Blender. We crafted various elements such as the ground, bridges, trees, houses, mushrooms, and even a snowman. For the musical notes, we sourced a model from an external website. And we included our Blender modelisation projects : my guitar and Pauline\u0026rsquo;s piano !\nPlayer Animation and Controls To bring our character to life, we incorporated animations and controls. We obtained these assets from Mixamo, which provided both the character\u0026rsquo;s appearance and animations.\nHealth system In \u0026ldquo;Virtuoso,\u0026rdquo; players confront a formidable adversary – the snowman, who relentlessly hurls snow bullets at them. To intensify the gaming experience, we introduced a health system that equips the player with three lives. Each time a player is hit by a snow bullet, they lose one of their lives. Moreover, if the player inadvertently falls into the water, it results in an immediate game over.\nMusic, Level Progression and User Interface When you first launch the game, you will find yourself on the starting page, where you can choose to begin or quit the game. The game rules are clearly displayed for your reference.\nVirtuoso is accompanied by Vivaldi\u0026rsquo;s music, and the user interface provides essential information, displaying the player\u0026rsquo;s remaining lives, the status of the piano and guitar (whether found or not), and the collected notes. Furthermore an exit button is available.\nThe game offers two distinct outcomes:\nWhen the player loses (due to falling into the water or running out of lives), a \u0026ldquo;Game Over\u0026rdquo; screen is displayed. On the other hand, when the player successfully recovers both instruments, they are greeted with a \u0026ldquo;Win\u0026rdquo; screen, and their level progresses, ranging from a novice musician to a virtuoso, based on the number of notes collected. In either case, you have the option to return to the starting page or restart the game.\n","date":"October 10, 2023","hero":"/posts/cg/unity-project/front.png","permalink":"https://melaniebrg.github.io/posts/cg/unity-project/","summary":"Introduction After a 12-hour class at the Institut Polytechnique de Paris, where we explored the fundamentals of Unity, Pauline Spinga and I embarked on a creative project to model and develop a music-themed platform game named \u0026ldquo;Virtuoso\u0026rdquo;. In this game, players step into the shoes of Vivaldi, a character who has lost his piano and guitar and must embark on a journey through the changing seasons to recover his instruments while collecting musical notes along the way.","tags":null,"title":"3D platform game Unity"},{"categories":["unity"],"contents":" Name Course Date Mélanie Brégou Digital content protection 06/10/2023 Objective : delve into image encryption and steganography, exploring classical ciphers and digital techniques.\nTable of Contents CAESAR cipher Simple substitution cipher Simple transposition cipher LSB (lowest significant bit) steganography Image compression in JPG Image compression in PNG LSB with 2 significant bit from matplotlib import image import matplotlib.pyplot as plt import numpy as np import random from PIL import Image Firstly, let\u0026rsquo;s display the images used for this lab.\nbaboon = image.imread(\u0026#34;../TP-HTI/baboon.jpg\u0026#34;) lena = image.imread(\u0026#34;../TP-HTI/lena.jpg\u0026#34;) fig, axes = plt.subplots(1, 2, figsize=(8, 4)) axes[0].imshow(baboon, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(\u0026#34;Baboon image\u0026#34;) axes[1].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(\u0026#34;Lena image\u0026#34;) for ax in axes: ax.axis(\u0026#39;off\u0026#39;) plt.show() CAESAR cypher applied to images CAESAR encryption applied to images involves shifting each pixel\u0026rsquo;s intensity by a fixed key value, with encryption adding the key and decryption subtracting it.\ndef encrypt(image,key): encrypted = np.zeros(image.shape) for i in range(image.shape[0]): for j in range(image.shape[1]): pixel = image[i][j] encrypted[i][j] = (pixel + key) % 256 return encrypted def decrypt(image,key): decrypted = np.zeros(image.shape) for i in range(image.shape[0]): for j in range(image.shape[1]): pixel = image[i][j] decrypted[i][j] = (pixel - key) % 256 return decrypted keys = [50,100,150,200] for key in keys : encrypt_baboon = encrypt(baboon, key) encrypt_lena = encrypt(lena,key) decrypt_baboon = decrypt(encrypt_baboon, key) decrypt_lena = decrypt(encrypt_lena,key) fig, axes = plt.subplots(2, 3, figsize=(10, 5)) axes[0][0].imshow(baboon, cmap=\u0026#39;gray\u0026#39;) axes[0][0].set_title(f\u0026#34;Baboon image\u0026#34;) axes[0][1].imshow(encrypt_baboon, cmap=\u0026#39;gray\u0026#39;) axes[0][1].set_title(f\u0026#34;Encrypted image with key ={key}\u0026#34;) axes[0][2].imshow(decrypt_baboon, cmap=\u0026#39;gray\u0026#39;) axes[0][2].set_title(f\u0026#34;Decrypted image with key ={key}\u0026#34;) axes[1][0].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[1][0].set_title(f\u0026#34;Lena image\u0026#34;) axes[1][1].imshow(encrypt_lena, cmap=\u0026#39;gray\u0026#39;) axes[1][1].set_title(f\u0026#34;Encrypted image with key ={key}\u0026#34;) axes[1][2].imshow(decrypt_lena, cmap=\u0026#39;gray\u0026#39;) axes[1][2].set_title(f\u0026#34;Decrypted image with key ={key}\u0026#34;) for row in axes: for ax in row: ax.axis(\u0026#39;off\u0026#39;) plt.show() The encrypted image remains recognizable, regardless of the key value. This suggests a significant limitation of the Caesar cipher for image encryption as it lacks the ability to effectively obscure the content and is highly vulnerable to attacks.\nSimple substitution cipher applied to images Substitution cipher involves changing each pixel\u0026rsquo;s value in an image by mapping it to a predefined shuffled list during encryption and back to the original value during decryption\nvalues = np.arange(255) random.shuffle(values) shuffled_values_list = values.tolist() def encrypt_substitution(image,shuffled_values_list): encrypted = np.zeros(image.shape) for i in range(image.shape[0]): for j in range(image.shape[1]): pixel = image[i][j] encrypted[i][j] = shuffled_values_list[pixel] return encrypted def decrypt_substitution(image,shuffled_values_list): decrypted = np.zeros(image.shape) for i in range(image.shape[0]): for j in range(image.shape[1]): pixel = image[i][j] decrypted[i][j] = shuffled_values_list.index(pixel) return decrypted encrypt_substitution_lena = encrypt_substitution(lena,shuffled_values_list) decrypt_substitution_lena = decrypt_substitution(encrypt_substitution_lena,shuffled_values_list) fig, axes = plt.subplots(1, 3, figsize=(10, 5)) axes[0].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(f\u0026#34;Lena image\u0026#34;) axes[1].imshow(encrypt_substitution_lena, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(f\u0026#34;Encrypted image\u0026#34;) axes[2].imshow(decrypt_substitution_lena, cmap=\u0026#39;gray\u0026#39;) axes[2].set_title(f\u0026#34;Decrypted image\u0026#34;) for ax in axes: ax.axis(\u0026#39;off\u0026#39;) The encrypted image is less visible than Caesar encryption but the general shape is still recognizable.\nSimple transposition cipher applied to images In simple transposition cipher, pixel positions are shuffled using random row and column permutations. This process includes encrypting the original image by rearranging pixel positions and then decrypting it to recover the original image.\nrows = np.arange(512) columns = np.arange(512) random.shuffle(rows) random.shuffle(columns) shuffled_rows_list = rows.tolist() shuffled_columns_list = columns.tolist() def encrypt_transposition(image,shuffled_rows_list,shuffled_columns_list): encrypted = np.zeros(image.shape) for i in range(image.shape[0]): for j in range(image.shape[1]): key_i = shuffled_rows_list[i] key_j = shuffled_columns_list[j] encrypted[i][j] = image[key_i][key_j] return encrypted def decrypt_transposition(image,shuffled_rows_list,shuffled_columns_list): decrypted = np.zeros(image.shape) for i in range(image.shape[0]): for j in range(image.shape[1]): original_i = shuffled_rows_list.index(i) original_j = shuffled_columns_list.index(j) decrypted[i][j] = image[original_i][original_j] return decrypted encrypt_transposition_lena = encrypt_transposition(lena,shuffled_rows_list,shuffled_columns_list) decrypt_transposition_lena = decrypt_transposition(encrypt_transposition_lena,shuffled_rows_list,shuffled_columns_list) fig, axes = plt.subplots(1, 3, figsize=(10, 5)) axes[0].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(f\u0026#34;Lena image\u0026#34;) axes[1].imshow(encrypt_transposition_lena, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(f\u0026#34;Encrypted image\u0026#34;) axes[2].imshow(decrypt_transposition_lena, cmap=\u0026#39;gray\u0026#39;) axes[2].set_title(f\u0026#34;Decrypted image\u0026#34;) for ax in axes: ax.axis(\u0026#39;off\u0026#39;) Lena is now well hidden in the encrypted image !\nLSB (lowest significant bit) steganography In LSB (Least Significant Bit) steganography, the process involves hiding information in an image by manipulating the least significant bits of the pixel values.\nlena_lsb_00 = lena[:256,:256] - lena[:256,:256]%2 lena_lsb_01 = lena[:256,256:] - lena[:256,256:]%2 +1 lena_lsb_10 = lena[256:,:256] - lena[256:,:256]%2 +1 lena_lsb_11 = lena[256:,256:] - lena[256:,256:]%2 fig, axes = plt.subplots(2, 2, figsize=(5, 5)) axes[0][0].imshow(lena_lsb_00, cmap=\u0026#39;gray\u0026#39;) axes[0][1].imshow(lena_lsb_01, cmap=\u0026#39;gray\u0026#39;) axes[1][0].imshow(lena_lsb_10, cmap=\u0026#39;gray\u0026#39;) axes[1][1].imshow(lena_lsb_11, cmap=\u0026#39;gray\u0026#39;) for row in axes: for ax in row: ax.axis(\u0026#39;off\u0026#39;) lena_lsb = np.zeros((512,512)) lena_lsb[:256,:256]= lena_lsb_00 lena_lsb[:256,256:] = lena_lsb_01 lena_lsb[256:,:256] = lena_lsb_10 lena_lsb[256:,256:] = lena_lsb_11 fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(f\u0026#34;Lena image\u0026#34;) axes[1].imshow(lena_lsb, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(f\u0026#34;Lena LSB image\u0026#34;) for ax in axes : ax.axis(\u0026#39;off\u0026#39;) lena_decoded = lena_lsb %2 plt.imshow(lena_decoded, cmap=\u0026#39;gray\u0026#39;) \u0026lt;matplotlib.image.AxesImage at 0x12ace1a50\u0026gt; By creating four versions of the image \u0026ldquo;lena\u0026rdquo; (lena_lsb_00, lena_lsb_01, lena_lsb_10, lena_lsb_11) the least significant bit in different regions is altered. These regions hide binary information without significantly altering the global visual appearance and the hidden information can be decoded.\nLet\u0026rsquo;s examine image compression in JPG and PNG and its consequences on the quality of the transmitted information.\nImage compression in JPG Image compression\nimg_lena_lsb = Image.fromarray(lena_lsb) img_lena_lsb = img_lena_lsb.convert(\u0026#34;L\u0026#34;) img_lena_lsb.save(\u0026#39;./lena_lsb_100.jpg\u0026#39;, optimize=True, quality=100) img_lena_lsb.save(\u0026#39;./lena_lsb_99.jpg\u0026#39;, optimize=True, quality=99) img_lena_lsb.save(\u0026#39;./lena_lsb_75.jpg\u0026#39;, optimize=True, quality=75) Compressed image and decoded information display\nlena_lsb_100 = image.imread(\u0026#34;./lena_lsb_100.jpg\u0026#34;) lena_lsb_99 = image.imread(\u0026#34;./lena_lsb_99.jpg\u0026#34;) lena_lsb_75 = image.imread(\u0026#34;./lena_lsb_75.jpg\u0026#34;) fig, axes = plt.subplots(2, 3, figsize=(10, 5)) axes[0][0].imshow(lena_lsb_100, cmap=\u0026#39;gray\u0026#39;) axes[0][0].set_title(f\u0026#34;Lena jpg image Q=100\u0026#34;) axes[0][1].imshow(lena_lsb_99, cmap=\u0026#39;gray\u0026#39;) axes[0][1].set_title(f\u0026#34;Lena jpg image Q=99\u0026#34;) axes[0][2].imshow(lena_lsb_75, cmap=\u0026#39;gray\u0026#39;) axes[0][2].set_title(f\u0026#34;Lena jpg image Q=75\u0026#34;) lena_decoded_100 = lena_lsb_100 %2 lena_decoded_99 = lena_lsb_99%2 lena_decoded_75 = lena_lsb_75%2 axes[1][0].imshow(lena_decoded_100, cmap=\u0026#39;gray\u0026#39;) axes[1][1].imshow(lena_decoded_99, cmap=\u0026#39;gray\u0026#39;) axes[1][2].imshow(lena_decoded_75, cmap=\u0026#39;gray\u0026#39;) for row in axes: for ax in row: ax.axis(\u0026#39;off\u0026#39;) The compression has no visual effet on the Lena image but deteriorates the transmitted information.\nImage compression in PNG Image compression\nimg_lena_lsb = Image.fromarray(lena_lsb) img_lena_lsb = img_lena_lsb.convert(\u0026#34;L\u0026#34;) img_lena_lsb.save(\u0026#39;./lena_lsb_9.png\u0026#39;, optimize=True, quality= 9) img_lena_lsb.save(\u0026#39;./lena_lsb_7.png\u0026#39;, optimize=True, quality=7) img_lena_lsb.save(\u0026#39;./lena_lsb_5.png\u0026#39;, optimize=True, quality=5) Compressed image and decoded information display\nlena_lsb_100_png = image.imread(\u0026#34;./lena_lsb_9.png\u0026#34;) lena_lsb_99_png = image.imread(\u0026#34;./lena_lsb_7.png\u0026#34;) lena_lsb_75_png = image.imread(\u0026#34;./lena_lsb_5.png\u0026#34;) fig, axes = plt.subplots(2, 3, figsize=(10, 5)) axes[0][0].imshow(lena_lsb_100_png, cmap=\u0026#39;gray\u0026#39;) axes[0][0].set_title(f\u0026#34;Lena png image Q=9\u0026#34;) axes[0][1].imshow(lena_lsb_99_png, cmap=\u0026#39;gray\u0026#39;) axes[0][1].set_title(f\u0026#34;Lena png image Q=7\u0026#34;) axes[0][2].imshow(lena_lsb_75_png, cmap=\u0026#39;gray\u0026#39;) axes[0][2].set_title(f\u0026#34;Lena png image Q=5\u0026#34;) lena_decoded_100_png = (lena_lsb_100_png * 255).astype(np.uint8) % 2 lena_decoded_99_png = (lena_lsb_99_png * 255).astype(np.uint8) % 2 lena_decoded_75_png = (lena_lsb_75_png * 255).astype(np.uint8) % 2 axes[1][0].imshow(lena_decoded_100_png, cmap=\u0026#39;gray\u0026#39;) axes[1][1].imshow(lena_decoded_99_png, cmap=\u0026#39;gray\u0026#39;) axes[1][2].imshow(lena_decoded_75_png, cmap=\u0026#39;gray\u0026#39;) for row in axes: for ax in row: ax.axis(\u0026#39;off\u0026#39;) The PNG compression has no effect on the Lena image and does not impact the transmitted information !\nLSB with 2 significant bit In LSB with 2 significant bits steganography, information is hidden within the image by altering the two least significant bits of pixel values.\nlena_lsb2_00 = lena[:256,:256] - lena[:256,:256]%4 lena_lsb2_01 = lena[:256,256:] - lena[:256,256:]%4 +1 lena_lsb2_10 = lena[256:,:256] - lena[256:,:256]%4 +2 lena_lsb2_11 = lena[256:,256:] - lena[256:,256:]%4 +3 fig, axes = plt.subplots(2, 2, figsize=(5, 5)) axes[0][0].imshow(lena_lsb2_00, cmap=\u0026#39;gray\u0026#39;) axes[0][1].imshow(lena_lsb2_01, cmap=\u0026#39;gray\u0026#39;) axes[1][0].imshow(lena_lsb2_10, cmap=\u0026#39;gray\u0026#39;) axes[1][1].imshow(lena_lsb2_11, cmap=\u0026#39;gray\u0026#39;) for row in axes: for ax in row: ax.axis(\u0026#39;off\u0026#39;) lena_lsb2 = np.zeros((512,512)) lena_lsb2[:256,:256]= lena_lsb2_00 lena_lsb2[:256,256:] = lena_lsb2_01 lena_lsb2[256:,:256] = lena_lsb2_10 lena_lsb2[256:,256:] = lena_lsb2_11 fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].imshow(lena, cmap=\u0026#39;gray\u0026#39;) axes[0].set_title(f\u0026#34;Lena image\u0026#34;) axes[1].imshow(lena_lsb2, cmap=\u0026#39;gray\u0026#39;) axes[1].set_title(f\u0026#34;Lena 2bit-LSB image\u0026#34;) for ax in axes: ax.axis(\u0026#39;off\u0026#39;) The information added has no visual effect on Lena \u0026hellip;\nlena2_decoded = lena_lsb2 %4 plt.imshow(lena2_decoded, cmap=\u0026#39;gray\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) (-0.5, 511.5, 511.5, -0.5) \u0026hellip; but can be effectively decrypted !\nConclusion CAESAR cipher doesn\u0026rsquo;t provide privacy of the image content as well as simple substitution. However, simple transposition ensure privacy for the image content. Furthermore, information can be concealed in images with LSB technique (single or two significant bit). But the use of JPG compression may impact the quality of the decoded information.\n","date":"October 6, 2023","hero":"/posts/dcp/encryption-lab/front.png","permalink":"https://melaniebrg.github.io/posts/dcp/encryption-lab/","summary":"Name Course Date Mélanie Brégou Digital content protection 06/10/2023 Objective : delve into image encryption and steganography, exploring classical ciphers and digital techniques.\nTable of Contents CAESAR cipher Simple substitution cipher Simple transposition cipher LSB (lowest significant bit) steganography Image compression in JPG Image compression in PNG LSB with 2 significant bit from matplotlib import image import matplotlib.pyplot as plt import numpy as np import random from PIL import Image Firstly, let\u0026rsquo;s display the images used for this lab.","tags":null,"title":"Illustrations on image encryption and steganography Lab"},{"categories":null,"contents":"Introduction In this article, I will walk you through the process of developing a rollerball game in Unity, where the player controls a ball on a plane, collecting stars while avoiding red enemies. My game includes both static and shooter enemies, a health system with three lives, and a Game Over screen that allows players to restart or exit the game. Let\u0026rsquo;s dive into the development process step by step.\nGame development Create the Ground, Walls, and Ball The initial step in crafting my rollerball game was setting up the game environment. I meticulously created a ground plane, enclosed it with walls to establish the play area, and strategically positioned the controllable ball at the center.\nAnimate the Ball with a Script Then I created a script to handle the ball\u0026rsquo;s animation, allowing it to roll smoothly on the ground plane and respond to user input.\nCollectibles Counter The goal of the game is to collect as many stars as possible. To do so a counter is displayed in the user interface\nHealth system I implemented a three-lives health system, adding depth to the game\u0026rsquo;s challenge. When a player lost all three lives, a \u0026ldquo;Game Over\u0026rdquo; screen would appear, offering the choice to either restart or exit the game. Unity\u0026rsquo;s user-friendly UI system streamlined the process of creating this pivotal feature.\nShooting ennemy I designed two distinct types of enemies: static and shooter. The static enemy remained stationary, and contact with it led to a life loss. The shooter enemy, on the other hand, utilized Unity\u0026rsquo;s AI Navigation system to pursue the player and shoot projectiles, adding an extra layer of challenge to the game.\nDemonstration on Computer Here is a little demo of the game :\nExtension to mobile game The game currently works on a computer, but the next step is to make it mobile-friendly. I tested it on an iPhone 14 Pro Max Emulator in Xcode. Here are the necessary steps:\nAdd the necessary modules: Unity Hub \u0026gt; Installs \u0026gt; Add Modules and add iOS build support.\nAdjust the game for the emulator by changing the setting to Player Settings \u0026gt; Target SDK: Simulator SDK.\nOpen Xcode and launch the RollerBall app on the emulator.\nExtension to VR game I also built a version for a VR Hololens headset, following my professor\u0026rsquo;s tutorial:\nInstall the OpenXR Plugin and XR Interaction Toolkit from the Package Manager (Unity Registry).\nGo to Edit \u0026gt; Project Settings \u0026gt; XR Plug-in Management and enable OpenXR.\nIn the hierarchy, create an XR Origin by right-clicking: XR \u0026gt; XR Origin (VR).\nAdd Character Controller and Character Controller Driver to the XR Origin. Ensure that the Origin Base GO is set to XR Origin.\nDelete the default main camera from the Hierarchy.\nRight-click in the Hierarchy, go to XR \u0026gt; Locomotion System (Action based), and untick Teleportation and Snap Turn Provider. In the Add component, add Continuous Move and Continuous Turn Provider (action-based). Drag and drop the XR rig and Locomotion System. Make sure Input Action Manager is set in the XR Origin.\nSet the hand XR controller parameters correctly.\nCreate an empty Prefab (HandController) in the Prefabs folder, creating a square and a cylinder to form a remote controller shape. Add it to the model prefab of the hand controller. Add a sphere collider to be able to grab objects. Add XR Direct Interactor and XR Grab Interactable to the PlayBoard.\nCreate an empty game object called AttachPoint and set it as the AttachTransform parameter.\nNow you can immerse yourself in the game and use your hands as controllers.\nThe next step, if time allows, is to create a room and make the playboard smaller to fit inside the room.\n","date":"September 30, 2023","hero":"/posts/hci/simple-unity-project/rollerball.png","permalink":"https://melaniebrg.github.io/posts/hci/simple-unity-project/","summary":"Introduction In this article, I will walk you through the process of developing a rollerball game in Unity, where the player controls a ball on a plane, collecting stars while avoiding red enemies. My game includes both static and shooter enemies, a health system with three lives, and a Game Over screen that allows players to restart or exit the game. Let\u0026rsquo;s dive into the development process step by step.","tags":null,"title":"Developing a Rollerball Game in Unity"},{"categories":null,"contents":"I installed Unity on Mac M1 sucessfully and in this article I\u0026rsquo;ll provide a brief step-by-step guide on how to install Unity based on Léa Saunier\u0026rsquo;s tutorial.\nStep 1: Download Unity Hub Begin by downloading Unity Hub from https://unity.com/download. Unity Hub acts as a central management tool for your Unity projects and installations.\nStep 2: Connect Your Account/License Open Unity Hub and log in with your Unity account or license to access Unity\u0026rsquo;s features and manage your licenses.\nStep 3: Install Unity 2022.3.2f1 Install the recommended Unity version, 2022.3.2f1, by clicking this link: unityhub://2022.3.2f1/d74737c6db50. Unity Hub will handle the installation process for you.\nStep 4: Install Required Modules Customize your Unity installation by installing modules for specific platforms like iOS, Windows, or Android. Unity Hub simplifies the process by allowing you to select the modules you need.\nStep 5: Install a Code Editor It\u0026rsquo;s highly recommended to install Visual Studio alongside Unity. This option will provide a robust code editor and seamlessly integrate with Unity. Unity will prompt you to install the Unity plugin for Visual Studio to enhance your workflow.\nStep 6 : Enjoy ! ","date":"September 30, 2023","hero":"/posts/hci/unity-install/unityinstall.png","permalink":"https://melaniebrg.github.io/posts/hci/unity-install/","summary":"I installed Unity on Mac M1 sucessfully and in this article I\u0026rsquo;ll provide a brief step-by-step guide on how to install Unity based on Léa Saunier\u0026rsquo;s tutorial.\nStep 1: Download Unity Hub Begin by downloading Unity Hub from https://unity.com/download. Unity Hub acts as a central management tool for your Unity projects and installations.\nStep 2: Connect Your Account/License Open Unity Hub and log in with your Unity account or license to access Unity\u0026rsquo;s features and manage your licenses.","tags":null,"title":"Unity Installation"},{"categories":["blender"],"contents":"After a 9-hour introductory class to Blender, I took on the challenge of creating a 3D model of my Taylor 510ce guitar. In this article, I\u0026rsquo;ll walk you through the steps I followed to bring this project to life.\nGathering Reference image To start, I began by capturing reference photos of my guitar from both side and front angles. I then removed the background of these images using Photoshop and put them as background images on x and y axis in Blender.\nSetting Up Background Images With my reference images prepared, I imported them into Blender and positioned them as background images on both the X and Y axes. These images served as blueprints, helping me maintain the accurate proportions and details of the guitar throughout the modeling process.\nModeling the Guitar The core of this project was modeling the guitar\u0026rsquo;s intricate shape. I began by using Blender\u0026rsquo;s Knife tool to outline the guitar\u0026rsquo;s body on a plane. Afterward, I extruded and adjusted vertices to match the contours and dimensions of the reference images. Moving on to the neck and headstock, I created these components using cube primitives. I then proceeded to model other components such as the head, bridge, strings, and smaller details like the rosette, pickguard, and tuners.\nApplying Textures Once satisfied with the 3D structure, I shifted my attention to applying textures to the model. I used the reference pictures to ensure the textures were authentically replicated.\nCreating the Scene To showcase the 3D guitar effectively, I created a virtual room environment within Blender. Adjusting the lighting and spotlights helped illuminate the guitar in a way that highlighted its details.\nCrafting the Animation Finally, to provide a multifaceted view of the guitar, I created a simple camera animation.\n","date":"September 28, 2023","hero":"/posts/cg/blender-project/taylor_guitar.jpg","permalink":"https://melaniebrg.github.io/posts/cg/blender-project/","summary":"After a 9-hour introductory class to Blender, I took on the challenge of creating a 3D model of my Taylor 510ce guitar. In this article, I\u0026rsquo;ll walk you through the steps I followed to bring this project to life.\nGathering Reference image To start, I began by capturing reference photos of my guitar from both side and front angles. I then removed the background of these images using Photoshop and put them as background images on x and y axis in Blender.","tags":["blender"],"title":"Blender project : My guitar in 3D !"},{"categories":null,"contents":"As part of my Human-Computer Interface class, I was tasked with creating a blog to showcase my work during my master\u0026rsquo;s program. I opted to build my website using Hugo, a fast and modern static site generator. In this guide, I will walk you through the steps to set up your own portfolio website using Hugo and deploy it on GitHub Pages.\nHere are the different steps to set up this portfolio :\nInstallation on MacOs Open a terminal and execute the following command to install Hugo using Homebrew: brew install hugo Visit the Hugo Themes website and choose the Toha theme. Then, access the Github repository. Fork the repository and rename it to [your GitHub username].github.io Clone the repository to your local machine: git clone https://github.com/[your GitHub username]/[your GitHub username].github.io.git Website personalization Once you have cloned the repository, open the project in Visual Studio Code or your preferred text editor. Modify the config.yaml file and set baseURL = https://[your github username].github.io. Choose the languages you want to use for the website Customize the template to suit your needs. You can use YAML files in the /data directory for the portfolio page and create blog articles in the /content/posts . Visualize your website locally with the following command hugo server -D Deployment in Github Pages Ensure that your repository name is [your GitHub username].github.io Create a gh-pages branch typing the following command : git checkout -b gh-pages Push the gh-pages branch to Github : git push origin gh-pages Check if the template provides a workflow in .github/workflows/deploy-site.yaml for automated deployments using GitHub Actions. Go back to the main branch and push all your changes Your portfolio website is now accessible at https://[your github username].github.io ","date":"September 23, 2023","hero":"/posts/hci/blog-creation/blog.png","permalink":"https://melaniebrg.github.io/posts/hci/blog-creation/","summary":"As part of my Human-Computer Interface class, I was tasked with creating a blog to showcase my work during my master\u0026rsquo;s program. I opted to build my website using Hugo, a fast and modern static site generator. In this guide, I will walk you through the steps to set up your own portfolio website using Hugo and deploy it on GitHub Pages.\nHere are the different steps to set up this portfolio :","tags":null,"title":"Creating a Portfolio Website with Hugo and Deploying on GitHub Pages"},{"categories":["blog"],"contents":"Welcome to the blog section of my portfolio website, where I present some of my work, projects, and experiments in:\nDeep Leaning AR/VR Computer Graphics Digital Content Protection ","date":"September 22, 2023","hero":"/posts/introduction/space.jpg","permalink":"https://melaniebrg.github.io/posts/introduction/","summary":"Welcome to the blog section of my portfolio website, where I present some of my work, projects, and experiments in:\nDeep Leaning AR/VR Computer Graphics Digital Content Protection ","tags":["Introduction","Blog"],"title":"Introduction"},{"categories":["project","deep learning","segmentation"],"contents":"For this research project, I collaborated with Clarisse Nouet and Clara Stavun under the supervision of Catalin Fetita, Noureddine Khiati and Antoine Didier from ARTEMIS departement.\nIntroduction Context and Problem Statement Infiltrative Lung Pathologies Infiltrative lung diseases manifest as a progressive disintegration of lung tissues, resulting in impaired functioning. They can obstruct up to 40%, as depicted in the following images, representing lungs affected by IIP (fibrosis), Covid-19, and ARDS (consolidations).\nPatient Monitoring and Segmentation These patients require regular monitoring to track the progression of the disease and assess the effectiveness of treatments. The first step in this monitoring is lung segmentation. This method involves identifying and precisely delineating the lungs from medical scans, thereby obtaining quantitative measurements and specifying areas for study. This initial step will facilitate a comprehensive study of the disease, such as the proportion of diseased lung tissues or a study of tissue texture.\nProblem Statement Automated segmentation is relatively straightforward for healthy lungs. However, when a patient has infiltrative lung diseases, certain areas of the lungs appear clearer on CT scans, making differentiation between the lung and its contour difficult, especially when lung peripheries are affected. Therefore, our project aims to address the following problem: How to automate the segmentation of lungs affected by pathologies?\nState of the Art Segmentation on Healthy Lungs Today, there are numerous models of deep neural networks capable of segmenting lungs in CT scans. The most commonly used neural network architectures for lung segmentation are U-Net, cascaded convolutional networks, and residual networks. These models are trained on large annotated CT scan databases, enabling them to learn to identify lung contours and structures with increased accuracy.\nImage preprocessing techniques such as intensity normalization, noise removal, and histogram matching are often used to improve the performance of lung segmentation models. Additionally, data augmentation techniques such as rotation, cropping, and scaling are used to increase the variability of training data and enhance model robustness.\nPerformance evaluation of lung segmentation models is typically done using measures such as Dice coefficient, Jaccard index, and sensitivity. Recent work has also focused on segmenting sub-structures within the lungs, such as blood vessels and nodules, to improve the accuracy and clinical relevance of results.\nSegmentation on Diseased Lungs There are few studies that use deep learning to address the segmentation problem of lungs with pathology. Other methods are used, such as in studies [1] and [2], which deal with automated segmentation on lungs with pathologies using graph search algorithms. Pu et al [1] developed an algorithm to correct borders to include parts of the lungs that were not detected. Hua et al [2] use a graph search algorithm to segment lung tissues. The algorithm utilizes image intensity and image gradient to define a cost function. This cost function is then used to guide graph search to achieve accurate segmentation of lung tissues. However, these methods are limited, and an approach with deep learning could make automatic segmentation more effective.\nGoals The project aims to develop a robust lung segmentation method using convolutional neural networks (CNN), initially for healthy lungs then for those with infiltrative pulmonary pathologies.\nTools We carry out our project on the GPU of the ARTEMIS department with the framework Python\u0026rsquo;s PyTorch. We also use Wandb which allows us to easily visualize online the curves of the evolution of the different parameters of the model.\n1. Lung segmentation without pathology 1.1 Objective Firstly our goal was to segment healthy lungs with a convolutional neural network (CNN). This network should be able to segment the right and left lung from the CT scan of a healthy patient :\nOnce a first CNN has been built and the first results have been obtained, we optimize the model in order to be able to make a good segmentation of the lungs presenting pathologies.\n1.2 U-Net Model 1.2.1 U-Net architecture For our lung segmentation, we chose to use a U-Net architecture due to its common usage in medical segmentation problems. The U-Net model architecture is specifically designed to address the issue of semantic segmentation, where the objective is to classify each pixel of the image into different classes. The model gets its name from its \u0026ldquo;U\u0026rdquo; shape, which consists of a combination of convolutional layers and deconvolutional layers, also known as transpose layers.\nThe uniqueness of the U-Net model lies in its encoder-decoder architecture, where information is progressively downscaled by convolutional layers in the encoder to extract features, and then restored to their original size by deconvolutional layers in the decoder to perform pixel-by-pixel segmentation.\nAdditionally, the U-Net model utilizes residual connections between the layers of the encoder and decoder, allowing for direct connections between low-level and high-level information. This aids in preserving fine details of the image during segmentation.\n1.3 Access to data 1.3.1 Database presentation We had access to a database comprising 57 patients, each with hundreds of CT scans (ranging from 300 to 600 scans). Each slice, meaning each CT scan, consists of the scan image and its associated mask where the lungs have been segmented. These images have a size of 512 x 512 pixels.\nMoreover, to train, validate, and test the model on different data, the database was divided into three separate sets: the portion used for model training corresponds to 70% of the database, totaling 40 patients, the model validation corresponds to 20%, totaling 11 patients, and the remainder is used for testing, totaling 6 patients.\n1.3.2 Data generator As we are working with relatively large datasets and have limited memory available, loading all the data at once is not feasible. Therefore, we utilized a data generator, a technique commonly used to efficiently load data into memory in small batches.\nA data generator is a mechanism that generates and loads data iteratively, as needed, rather than loading it all at once. This significantly reduces memory consumption, as only the data needed at a specific time is loaded into memory.\nThe operation of a data generator typically relies on iterative loops. It divides the dataset into small parts called batches, and each batch is loaded into memory successively for processing. Once a batch is used, it is removed from memory to free up space.\nThus, the data generator allows us to efficiently load large amounts of data into our environment despite the limited memory available.\n1.4 Training and validation of the network 1.4.1 Training process and validation Once the model is built and the data is loaded, the training and validation of the model start. The training process starts by a loop on the specified number of epochs. One epoch corresponds to a complete iteration over the entire training dataset, meaning that each training sample has been presented to the model once. During each epoch, we loop through the training batches. The data and labels are loaded into GPU memory, and then the model is used to make predictions on the input data. We then compute the model\u0026rsquo;s loss at each epoch using various metrics (see next section). The loss is backpropagated through the model, and the weights are updated using the optimizer. Initially, we opt for the Adam optimizer.\nAfter each training epoch, we evaluate the model on the validation set. The validation data is loaded into GPU memory, and we make predictions using the model. We compute the total validation loss. Periodically, every few epochs, we save the model using checkpoints so that we can reload and reuse it later if needed. Once all epochs are completed, we save the final model to a .pth file. This iterative training and validation process optimizes the model weights to improve lung segmentation performance.\n1.4.2 Metrics used In order to evaluate the performance of the model and measure its ability to accurately and consistently segment the lungs, we use different metrics:\nThe dice loss, calculated at each epoch in training and validation, calculating the similarity between two sets (in our case the mask and the predicted image). This metric helps evaluate how well the predicted segmentation matches the actual areas of the lungs. A Dice coefficient close to 1 indicates accurate segmentation similar to real labels, while a value close to 0 indicates inaccurate segmentation. Its formula is, with 𝑦 the mask and 𝑝 the prediction: Dice Score: This metric, complementary to the Dice loss, is calculated as 1 minus the Dice loss. Dice Score (y, p) = 1 - Dice Loss (y, p).\nCross Entropy Loss: Widely used in classification tasks, it measures the accuracy of classification by assigning higher penalties to incorrect predictions. In the context of lung segmentation, it evaluates the model\u0026rsquo;s ability to accurately predict the different classes of pixels.\nTotal Loss: Calculated both during training and validation, the total loss is obtained by summing the Dice loss and the Cross Entropy loss for each batch, and accumulating it batch by batch. It is normalized by dividing by the total number of batches in each epoch. This metric allows us to evaluate the model\u0026rsquo;s performance over epochs, and thus, we focus on it to evaluate and optimize our model.\n1.4.3 First results With this initial model, we choose the following hyperparameters:\nBatch size of x (number of training samples used in one iteration during model training); Learning rate of x (determines the speed at which the model adjusts its weights based on the calculated error during training); Number of epochs of x. As a result, we obtain training and validation loss curves:\nTotal training loss for the first model\nTotal validation loss for the first model\nThus, we observe that the training loss stabilizes after 15 epochs and tends towards a loss of 2%. However, we notice instability in the validation loss, which may be due to overfitting (a situation where the model fits too closely to the training data and loses its ability to generalize to new data). For this reason, we will attempt to improve and optimize the model further.\n1.5 Test We carry out two types of test: on the one hand, in the same way as for validation, we study the performance of the model on data that it does not know, and on the other hand, we visualize and record the predictions on a few patients that the model did not encounter, neither in training nor in validation. As the curves of the first method are generally similar to the validation curves, we focus in the test part on the visualization. For example, on section 100 of patient 47KD:\nNative Image Mask Prediction We observe that the prediction was generally good, but with some inaccuracies at the borders of the lungs.\n1.6 Model optimisation 1.6.1 AdamW vs Adam The first modification made is the use of the AdamW optimizer, a variant of the Adam optimizer previously used. AdamW has the advantage of applying weight decay regularization during the model\u0026rsquo;s weight updates. This regularization helps reduce the risk of overfitting by penalizing high values of weights, thus promoting better model generalization. By incorporating this regularization directly into the optimizer, AdamW simplifies the training process by eliminating the need to manually adjust regularization hyperparameters.\n1.6.2 Accumulated gradient To achieve better performance, we aim to increase the batch size. However, due to limited available memory, we cannot exceed a batch size of 4. Therefore, we use accumulated gradient. Instead of updating the model\u0026rsquo;s weights after each mini-batch, gradient accumulation allows us to collect gradients over multiple mini-batches before updating the weights (the frequency of this update is defined by the hyperparameter grad_accum_step).\nFurthermore, accumulated gradient can also improve training stability by reducing fluctuations in weight updates. By averaging gradients over several mini-batches, weight updates become smoother and less likely to fluctuate significantly from one iteration to another.\nHere are the results obtained for the total training and validation losses:\nTotal training loss with and without accumulated gradient for 40 epochs\nTotal validation loss with and without accumulated gradient for 40 epochs\nThus, we observe that the total training loss converges to a similar value in the cases without and with accumulated gradient, around 0.016. For validation, the difference is more pronounced. Indeed, without accumulated gradient, the total loss converges to a value of 0.15, whereas with accumulated gradient, the convergence value is better at 0.1.\n1.6.3 Choice of Hyperparameters Number of epochs: We observed that in the case of the model with accumulated gradient, the total training loss reaches a plateau before stabilizing after 35 epochs. Therefore, we chose to train the model for 40 epochs subsequently.\nBatch size: Similar to previous settings, we maintained a batch size of 4.\nLearning rate: We opted for a learning rate of 0.01 as it fell within the recommended magnitude for our case.\ngrad_accum_step: As mentioned earlier to mitigate overfitting, we artificially increase the gradient to 32 by setting grad_accum_step to 8 in addition to the batch size of 4. This allows us to train our model on a more diverse dataset, limiting overfitting.\n1.7 Results Using the accumulated gradient model, we obtain the following prediction :\n2. Lung segmentation with pathology 2.1 Objective After segmenting healthy lungs, our goal is to segment lungs with pathologies also using a CNN. This network must be able to identify and extract each lung despite the present diseases:\nExamples of CT scans with real diseases 2.2 Generation of Pathologies As we do not have a database of diseased lungs, we simulated diseases from scans of healthy lungs. For this, we created a function that takes an image and a mask of healthy lungs as parameters. Then, using the regionprops function, we detect the two largest boxes containing the lungs from the mask. After extracting the coordinates, we choose between 7 and 9 random positions in the two detected boxes to add circles at these locations on the original lung image. These added circles have random radius and transparency to simulate the diversity of plausible diseases as accurately as possible.\nA limitation of our simulation is the circular shape of the diseases we insert, which may not necessarily be representative of the pathologies patients may encounter. Another limitation is the detection of bounding boxes for lungs on certain slices that do not show both lungs. In this case, we apply the addition of pathologies only when two lungs are detected.\nSimulation of lung disease 2.2.2 Implementation during training Instead of saving all images with simulated pathology, we chose to implement them directly during training using an approach known as \u0026ldquo;on-the-fly\u0026rdquo; generation. This method involves applying random transformations to the data samples (CT lung scans without pathologies) during training. Thus, the pathology simulations described above are applied to lung images without pathologies during training. To ensure that the model is trained not only on lungs with pathologies but also on those without, pathology generation does not occur on every training image, but only on some, with a variable probability p.\n2.3 Model Optimization 2.3.1 Choice of Hyperparameters All hyperparameters of the model remain unchanged compared to lung segmentation without pathologies.\n2.3.2 Adjustment of Pathology Appearance Frequency in Training We run the models with various frequencies of pathology appearance:\nTotal loss for training and validation of the model with pathologies of frequency 0.6, 0.8 and 0.9\nWhether during training or validation, we notice that the model with a frequency of appearance of pathologies at 0.8 has better performance.\n2.4 Results 2.4.1 Results on images with simulated pathologies Here are the results obtained on a patient data unknown to the model :\nThus we notice that the model trained with pathologies works just as well with as without pathologies, including when the diseases are present at the edge of the lung.\n2.4.1 Results on images with real pathologies For this project, we simulated the presence of pathologies in the lungs due to a lack of real data. However, these simulations are not always very representative. This is why we also did tests on some real scan images with pathology:\nWe observe that the model has difficulties with cases of real pathologies. For it to have better performance, it would be interesting to train it on a large number of data with either a high simulated disease density, or directly with real scans of affected lungs.\nConclusion Results This project implements a method for segmenting healthy lungs and those affected by pathologies in CT-scan images. To achieve this, we deployed a U-Net model and evaluated its performance using various metrics, which characterize our results as satisfactory. Model optimization involved the addition of a data generator, without which the executed programs did not yield results. We also optimized the selection of our hyperparameters to achieve a satisfactory outcome.\nLimitations It is important to note some limitations of this project. Firstly, the execution times are long. For a run of 40 epochs, it takes approximately 29 hours, making it less feasible due to the significant computational time required. Additionally, although our model yields acceptable performance, there is room for improvement.\nMoreover, as previously discussed, the results obtained on real pathology images are less satisfactory. Therefore, it is necessary to consider potential avenues for improvement to achieve better performance, such as using scans with real pathologies during training or increasing the density of simulated pathologies.\nAreas for Improvement We have considered several avenues for improvement, such as better simulating diseases to closely approximate the real aspect of pathologies. Indeed, we observed poorer estimations on real images of diseased lungs since the network is trained only on synthesized pathologies with a superimposition of circles. To address this, we could add more circles on the lungs, with the densities adding up to create obstructed areas. Additionally, we could manipulate the texture or add other shapes besides circles. Moreover, to obtain more training data for better performance, we could consider implementing additional augmentations and/or obtaining more scans with real pathologies.\nFurthermore, we could use models like ResNet in the downward part to improve performance. Finally, the project could be extended to achieve a 3D rendering of the lungs for better visualization.\n","date":"July 19, 2023","hero":"/posts/dl/lung-segmentation/front.png","permalink":"https://melaniebrg.github.io/posts/dl/lung-segmentation/","summary":"For this research project, I collaborated with Clarisse Nouet and Clara Stavun under the supervision of Catalin Fetita, Noureddine Khiati and Antoine Didier from ARTEMIS departement.\nIntroduction Context and Problem Statement Infiltrative Lung Pathologies Infiltrative lung diseases manifest as a progressive disintegration of lung tissues, resulting in impaired functioning. They can obstruct up to 40%, as depicted in the following images, representing lungs affected by IIP (fibrosis), Covid-19, and ARDS (consolidations).","tags":null,"title":"Achieving robust segmentation of lung regions in CT images irrespective to pathology"},{"categories":null,"contents":"Go Notes ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://melaniebrg.github.io/notes/go/_index.bn/","summary":"Go Notes ","tags":null,"title":"Go এর নোট সমূহ"}]